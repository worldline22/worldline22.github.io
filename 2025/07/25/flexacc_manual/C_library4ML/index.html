<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>C Library for ML in FlexACC | Yuchao's Homepage</title><meta name="author" content="Yuchao Qin"><meta name="copyright" content="Yuchao Qin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="A guide to FlexAcc c function kernels">
<meta property="og:type" content="article">
<meta property="og:title" content="C Library for ML in FlexACC">
<meta property="og:url" content="http://example.com/2025/07/25/flexacc_manual/C_library4ML/index.html">
<meta property="og:site_name" content="Yuchao&#39;s Homepage">
<meta property="og:description" content="A guide to FlexAcc c function kernels">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png">
<meta property="article:published_time" content="2025-07-24T19:00:00.000Z">
<meta property="article:modified_time" content="2025-07-29T18:10:39.248Z">
<meta property="article:author" content="Yuchao Qin">
<meta property="article:tag" content="FlexAcc Kernel">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/07/25/flexacc_manual/C_library4ML/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'C Library for ML in FlexACC',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-30 02:10:39'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transpancy.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/profile.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Menu</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/life/"><i class="fa-fw fas fa-coffee"></i><span> Life</span></a></li><li><a class="site-page child" href="/science/"><i class="fa-fw fas fa-atom"></i><span> Research</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Photo</span></a></li><li><a class="site-page child" href="/pku/"><i class="fa-fw fas fa-book-open"></i><span> 学在北大</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Yuchao's Homepage"><img class="site-icon" src="/image/test.gif"/><span class="site-name">Yuchao's Homepage</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Menu</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/life/"><i class="fa-fw fas fa-coffee"></i><span> Life</span></a></li><li><a class="site-page child" href="/science/"><i class="fa-fw fas fa-atom"></i><span> Research</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Photo</span></a></li><li><a class="site-page child" href="/pku/"><i class="fa-fw fas fa-book-open"></i><span> 学在北大</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">C Library for ML in FlexACC</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-24T19:00:00.000Z" title="发表于 2025-07-25 03:00:00">2025-07-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-29T18:10:39.248Z" title="更新于 2025-07-30 02:10:39">2025-07-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Architecture/">Computer Architecture</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="C Library for ML in FlexACC"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><div style="border: 2px solid #ff9999; padding: 10px; margin-top: 20px; background-color: #ffe6e6; border-radius: 10px; box-shadow: 2px 2px 12px #aaaaaa;">
    <p style="font-size: 18px; color: #ab47bc;"><strong>📌README</strong></p>
    <p style="font-size: 16px; color: #000000;">This document details a set of core neural network operations implemented in C, specifically optimized for a Single Instruction, Multiple Data (SIMD) architecture. To achieve high computational throughput, all data structures such as inputs, outputs, and weights are stored in memory using vector types (e.g., actvec_t, vec_t, mat_t). These functions form the building blocks for constructing efficient convolutional neural networks and transformer architecture.
    </p>
</div>


<h1 id="Convolutional-Operations"><a href="#Convolutional-Operations" class="headerlink" title="Convolutional Operations"></a>Convolutional Operations</h1><h2 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a><code>conv2d</code></h2><p>The 2D convolution is the cornerstone of modern computer vision models. This function performs a 2D convolution on an input tensor, applying a set of learnable filters (weights) to produce an output feature map. It also includes fused operations for adding a bias, applying quantization, and executing a ReLU activation function, which minimizes memory access and improves performance.</p>
<h3 id="Interface-Definition"><a href="#Interface-Definition" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">conv2d_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base,</span></span><br><span class="line"><span class="params">                              <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec, <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> IN_C,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> OUT_C, <span class="type">int</span> K, <span class="type">int</span> STRIDE, <span class="type">int</span> PAD,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> mul_o, <span class="type">int</span> shf_o, <span class="type">int</span> apply_relu)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_base</code>: A pointer to the base address in vector memory (<code>VM</code>) where the output tensor will be stored.</li>
<li><code>in_base</code>: A pointer to the base address in vector memory (<code>VM</code>) of the input tensor.</li>
<li><code>weight_base</code>: A pointer to the base address in weight memory (<code>WM</code>) of the convolution kernels (filters).</li>
<li><code>bias_vec</code>: A pointer to the base address in weight memory (<code>WM</code>) for the bias vector. Each element is added to a corresponding output channel.</li>
<li><code>IN_H</code>, <code>IN_W</code>, <code>IN_C</code>: The height, width, and number of channels of the input tensor. <code>IN_C</code> is assumed to be a multiple of the vector size <code>VSIZE</code>.</li>
<li><code>OUT_C</code>: The number of output channels, which corresponds to the number of convolutional filters. <code>OUT_C</code> is also assumed to be a multiple of <code>VSIZE</code>.</li>
<li><code>K</code>: The size (height and width) of the square convolutional kernel.</li>
<li><code>STRIDE</code>: The step size, or stride, for moving the convolutional window across the input tensor.</li>
<li><code>PAD</code>: The number of zero-value pixels to pad around the border of the input tensor. This allows control over the output spatial dimensions.</li>
<li><code>mul_o</code>, <code>shf_o</code>: Scaling factor and right-shift amount for output quantization. This is a common technique in integer-only inference to approximate floating-point division.</li>
<li><code>apply_relu</code>: A flag (1 or 0) that determines whether to apply a Rectified Linear Unit (ReLU) activation function to the output.</li>
</ul>
<h3 id="Core-Logic"><a href="#Core-Logic" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function calculates the output dimensions and then iterates through each spatial location <code>(oh, ow)</code> of the output tensor. For each location, it computes the value for all output channels in a tiled manner.</p>
<p>$$<br>\text{OUT_H} &#x3D; \frac{(\text{IN_H} - K + 2 \times \text{PAD})}{\text{STRIDE}} + 1<br>$$<br>$$<br>\text{OUT_W} &#x3D; \frac{(\text{IN_W} - K + 2 \times \text{PAD})}{\text{STRIDE}} + 1<br>$$</p>
<p>The core computation is a multiply-accumulate operation performed on vector data types. The loops are structured to maximize data locality and enable efficient SIMD processing.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- 2D convolution with Fused ReLU ---</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">conv2d_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base,</span></span><br><span class="line"><span class="params">                              <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec, <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> IN_C,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> OUT_C, <span class="type">int</span> K, <span class="type">int</span> STRIDE, <span class="type">int</span> PAD,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> mul_o, <span class="type">int</span> shf_o, <span class="type">int</span> apply_relu)</span> &#123;</span><br><span class="line">    <span class="comment">// Calculate output tensor dimensions</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_H = (IN_H - K + <span class="number">2</span> * PAD) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_W = (IN_W - K + <span class="number">2</span> * PAD) / STRIDE + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate the number of vector tiles for input and output channels</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> IN_C_TILES = IN_C / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_C_TILES = OUT_C / VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over each spatial location in the output tensor</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> oh = <span class="number">0</span>; oh &lt; OUT_H; oh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ow = <span class="number">0</span>; ow &lt; OUT_W; ow++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Iterate over the output channels in chunks of VSIZE</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> oc_tile = <span class="number">0</span>; oc_tile &lt; OUT_C_TILES; oc_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="comment">// Initialize accumulator with the bias for the current output channel tile</span></span><br><span class="line">                <span class="type">actvec_t</span> act = bias_vec[oc_tile];</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Calculate the corresponding input window, considering padding</span></span><br><span class="line">                <span class="type">int</span> ih_start = -PAD + oh * STRIDE;</span><br><span class="line">                <span class="type">int</span> iw_start = -PAD + ow * STRIDE;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Determine the valid kernel region to apply, clamping to input boundaries</span></span><br><span class="line">                <span class="type">int</span> wh_start = max_(<span class="number">0</span>, PAD - oh * STRIDE);</span><br><span class="line">                <span class="type">int</span> ww_start = max_(<span class="number">0</span>, PAD - ow * STRIDE);</span><br><span class="line">                <span class="type">int</span> wh_end   = min_(K, IN_H - ih_start);</span><br><span class="line">                <span class="type">int</span> ww_end   = min_(K, IN_W - iw_start);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Slide the kernel over the input window</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> wh = wh_start; wh &lt; wh_end; wh++) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> ww = ww_start; ww &lt; ww_end; ww++) &#123;</span><br><span class="line">                        <span class="comment">// Pointer to weights for this kernel position and output channel tile</span></span><br><span class="line">                        <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* wx_ptr = (<span class="type">mat_t</span> chess_storage(WM)*)weight_base + oc_tile * (K * K * IN_C_TILES) + wh * (K * IN_C_TILES) + ww * (IN_C_TILES);</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Pointer to the corresponding input data</span></span><br><span class="line">                        <span class="type">int</span> ih = ih_start + wh;</span><br><span class="line">                        <span class="type">int</span> iw = iw_start + ww;</span><br><span class="line">                        <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = (<span class="type">vec_t</span> chess_storage(VM)*)in_base + ih * (IN_W * IN_C_TILES) + iw * (IN_C_TILES);</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Perform multiply-accumulate across all input channel tiles</span></span><br><span class="line">                        <span class="keyword">for</span> (<span class="type">int</span> ic_tile = <span class="number">0</span>; ic_tile &lt; IN_C_TILES; ic_tile++) &#123;</span><br><span class="line">                            act = mat_vec_mul_add(act, *wx_ptr, *x_ptr);</span><br><span class="line">                            wx_ptr++;</span><br><span class="line">                            x_ptr++;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Quantization and Activation ---</span></span><br><span class="line">                <span class="comment">// Apply post-convolution quantization</span></span><br><span class="line">                act = (act * mul_o) &gt;&gt; shf_o;</span><br><span class="line">                </span><br><span class="line">                <span class="comment">// Apply ReLU activation if enabled</span></span><br><span class="line">                <span class="keyword">if</span> (apply_relu) &#123;</span><br><span class="line">                    act = max_(act, <span class="type">actvec_t</span>(<span class="number">0</span>));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Store Output ---</span></span><br><span class="line">                <span class="comment">// Calculate the output address and store the result</span></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* o_ptr = (<span class="type">vec_t</span> chess_storage(VM)*)out_base + oh * (OUT_W * OUT_C_TILES) + ow * (OUT_C_TILES) + oc_tile;</span><br><span class="line">                *o_ptr = <span class="type">vec_t</span>(act);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max_pooling"></a><code>max_pooling</code></h2><p>Max pooling is a down-sampling operation that reduces the spatial dimensions (height and width) of a feature map. It works by sliding a window over the input and selecting the maximum value within that window. This helps to make the feature representation more robust to small translations in the input image.</p>
<h3 id="Interface-Definition-1"><a href="#Interface-Definition-1" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">max_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, </span></span><br><span class="line"><span class="params">                                <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C,</span></span><br><span class="line"><span class="params">                                <span class="type">int</span> K, <span class="type">int</span> STRIDE)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters-1"><a href="#Parameters-1" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_base</code>: A pointer to the base address for the down-sampled output tensor.</li>
<li><code>in_base</code>: A pointer to the base address of the input tensor.</li>
<li><code>IN_H</code>, <code>IN_W</code>, <code>C</code>: The height, width, and number of channels of the input tensor. <code>C</code> is assumed to be a multiple of <code>VSIZE</code>.</li>
<li><code>K</code>: The size (height and width) of the square pooling window.</li>
<li><code>STRIDE</code>: The step size for moving the pooling window across the input.</li>
</ul>
<h3 id="Core-Logic-1"><a href="#Core-Logic-1" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function computes the output dimensions based on the input size, kernel size, and stride. It then iterates over each position of the output grid and each channel tile. For each output value, it scans the corresponding K×KK \times K window in the input tensor and finds the maximum value vector-wise.</p>
<p>$$<br>\text{OUT_H} &#x3D; \frac{(\text{IN_H} - K)}{\text{STRIDE}} + 1<br>$$<br>$$<br>\text{OUT_W} &#x3D; \frac{(\text{IN_W} - K)}{\text{STRIDE}} + 1<br>$$</p>
<p>The accumulator vector <code>max_accumulator_act</code> is initialized with the minimum possible value to ensure that the first element from the input window is always selected as the initial maximum.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">max_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, </span></span><br><span class="line"><span class="params">                                <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C,</span></span><br><span class="line"><span class="params">                                <span class="type">int</span> K, <span class="type">int</span> STRIDE)</span> &#123;</span><br><span class="line">    <span class="comment">// Calculate output dimensions</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_H = (IN_H - K) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_W = (IN_W - K) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Pre-calculate channel tile count</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> C_TILES = C / VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over the output spatial grid</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> oh = <span class="number">0</span>; oh &lt; OUT_H; oh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ow = <span class="number">0</span>; ow &lt; OUT_W; ow++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Iterate over channel tiles</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> c_tile = <span class="number">0</span>; c_tile &lt; C_TILES; c_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="comment">// Initialize a vector with the minimum possible scalar value (-128 for int8).</span></span><br><span class="line">                <span class="comment">// This ensures any real value from the input will be greater.</span></span><br><span class="line">                <span class="type">actvec_t</span> max_accumulator_act = <span class="number">-128</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Iterate over the pooling window (kernel)</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> kh = <span class="number">0</span>; kh &lt; K; kh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> kw = <span class="number">0</span>; kw &lt; K; kw++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                        <span class="comment">// Calculate input coordinates</span></span><br><span class="line">                        <span class="type">int</span> ih = oh * STRIDE + kh;</span><br><span class="line">                        <span class="type">int</span> iw = ow * STRIDE + kw;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// Pointer to Input[ih][iw][c_tile]</span></span><br><span class="line">                        <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_base +</span><br><span class="line">                                                 ih * (IN_W * C_TILES) +</span><br><span class="line">                                                        iw * (C_TILES) +</span><br><span class="line">                                                                 c_tile;</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Load the input vector and promote it to a wide accumulator type</span></span><br><span class="line">                        <span class="type">actvec_t</span> current_val_act = (<span class="type">actvec_t</span>)* x_ptr;</span><br><span class="line">                        <span class="comment">// Perform element-wise vector maximum</span></span><br><span class="line">                        max_accumulator_act = max_(max_accumulator_act, current_val_act);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Store Output ---</span></span><br><span class="line">                <span class="comment">// Calculate output address: &amp;Output[oh][ow][c_tile]</span></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* o_ptr = out_base +</span><br><span class="line">                                   oh * (OUT_W * C_TILES) +</span><br><span class="line">                                           ow * (C_TILES) +</span><br><span class="line">                                                    c_tile;</span><br><span class="line">                <span class="comment">// Convert the final wide max vector back to a narrow storage vector and store</span></span><br><span class="line">                *o_ptr = (<span class="type">vec_t</span>)max_accumulator_act;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="add-2-relu"><a href="#add-2-relu" class="headerlink" title="add_2_relu"></a><code>add_2_relu</code></h2><p>This operation is fundamental to residual network architectures (e.g., ResNet). It implements an element-wise addition of two tensors (a “main path” and a “shortcut path”) followed by a ReLU activation. This “shortcut connection” allows the gradient to flow more directly through the network, enabling the training of much deeper models.</p>
<h3 id="Interface-Definition-2"><a href="#Interface-Definition-2" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">add_and_relu_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_data, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* main_path,</span></span><br><span class="line"><span class="params">                                    <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* shortcut_path, <span class="type">int</span> num_vectors)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters-2"><a href="#Parameters-2" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_data</code>: A pointer to the memory where the output vector will be stored.</li>
<li><code>main_path</code>: A pointer to the input tensor from the main computational path (e.g., the output of a convolutional block).</li>
<li><code>shortcut_path</code>: A pointer to the input tensor from the shortcut or identity path.</li>
<li><code>num_vectors</code>: The total number of vectors in the input tensors. This is equivalent to <code>(H * W * C) / VSIZE</code>.</li>
</ul>
<h3 id="Core-Logic-2"><a href="#Core-Logic-2" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function iterates through the input tensors one vector at a time. In each iteration, it loads one vector from the <code>main_path</code> and one from the <code>shortcut_path</code>, adds them together, and then applies the ReLU activation <code>max(0, sum)</code>. The result is stored in the output buffer.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Add and ReLU ---</span></span><br><span class="line"><span class="comment">// This function performs element-wise addition of two tensors and applies a ReLU activation.</span></span><br><span class="line"><span class="comment">// It is a key component of residual blocks in architectures like ResNet.</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">add_and_relu_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_data, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* main_path,</span></span><br><span class="line"><span class="params">                                    <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* shortcut_path, <span class="type">int</span> num_vectors)</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; ++i) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Load both vectors and promote to wide accumulator type for addition</span></span><br><span class="line">        <span class="type">actvec_t</span> main_act = <span class="type">actvec_t</span>(main_path[i]);</span><br><span class="line">        <span class="type">actvec_t</span> shortcut_act = <span class="type">actvec_t</span>(shortcut_path[i]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Perform the element-wise addition</span></span><br><span class="line">        <span class="type">actvec_t</span> sum = main_act + shortcut_act;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Apply ReLU activation: result = max(0, sum)</span></span><br><span class="line">        <span class="type">actvec_t</span> result = max_(sum, <span class="type">actvec_t</span>(<span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Store the final result</span></span><br><span class="line">        out_data[i] = <span class="type">vec_t</span>(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="average-pooling"><a href="#average-pooling" class="headerlink" title="average_pooling"></a><code>average_pooling</code></h2><p>Global Average Pooling (GAP) is another down-sampling technique that reduces the entire spatial dimensions of each feature map into a single value, calculated as the average of all values in that map. It is often used in modern CNNs just before the final fully connected layer to reduce the number of parameters and prevent overfitting.</p>
<h3 id="Interface-Definition-3"><a href="#Interface-Definition-3" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">global_average_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_vector, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* in_base,</span></span><br><span class="line"><span class="params">                                           <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters-3"><a href="#Parameters-3" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_vector</code>: A pointer to the output vector of size <code>C</code>.</li>
<li><code>in_base</code>: A pointer to the input tensor.</li>
<li><code>IN_H</code>, <code>IN_W</code>, <code>C</code>: The height, width, and number of channels of the input tensor. <code>C</code> is assumed to be a multiple of <code>VSIZE</code>.</li>
</ul>
<h3 id="Core-Logic-3"><a href="#Core-Logic-3" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function first calculates the total number of spatial elements, <code>num_pixels = IN_H * IN_W</code>. It then iterates through each channel tile. For each tile, it accumulates the sum of all vectors across the spatial dimensions. Finally, it performs a per-lane division of the accumulated sum by <code>num_pixels</code> to compute the average. This lane-wise division is necessary because each lane in the vector corresponds to a different channel.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Global Average Pooling ---</span></span><br><span class="line"><span class="comment">// This function computes the average of each channel across all spatial dimensions (H x W).</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">global_average_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_vector, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* in_base,</span></span><br><span class="line"><span class="params">                                           <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C)</span> &#123;</span><br><span class="line">    <span class="comment">// Pre-calculate tile count and the total number of pixels per feature map</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> C_TILES = C / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_pixels = IN_H * IN_W;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over channel tiles</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> c_tile = <span class="number">0</span>; c_tile &lt; C_TILES; c_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Initialize a wide vector accumulator to zeros</span></span><br><span class="line">        <span class="type">actvec_t</span> sum_accumulator = <span class="type">actvec_t</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Loop over all pixels in the feature maps for the current channel tile.</span></span><br><span class="line">        <span class="comment">// The H and W dimensions are flattened for this operation.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_pixels; ++i) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Calculate pointer to Input[pixel_i][c_tile]</span></span><br><span class="line">            <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_base + i * C_TILES + c_tile;</span><br><span class="line">            <span class="comment">// Accumulate the vector values</span></span><br><span class="line">            sum_accumulator = sum_accumulator + <span class="type">actvec_t</span>(*x_ptr);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- Perform Per-Lane Division to get the Average ---</span></span><br><span class="line">        <span class="type">actvec_t</span> avg_vec = <span class="type">actvec_t</span>(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> lane = <span class="number">0</span>; lane &lt; VSIZE; ++lane) &#123;</span><br><span class="line">            <span class="comment">// Extract the sum for a single channel (lane)</span></span><br><span class="line">            <span class="type">actsca_t</span> lane_sum = act_ext(sum_accumulator, lane);</span><br><span class="line">            <span class="comment">// Perform scalar integer division to find the average</span></span><br><span class="line">            <span class="type">actsca_t</span> lane_avg = (<span class="type">int</span>)lane_sum / num_pixels;</span><br><span class="line">            <span class="comment">// Update the corresponding lane in the average vector with the result</span></span><br><span class="line">            avg_vec = act_upd(avg_vec, (<span class="type">int</span>)lane_avg, lane);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Store the resulting average vector</span></span><br><span class="line">        out_vector[c_tile] = <span class="type">vec_t</span>(avg_vec);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="fully-connected"><a href="#fully-connected" class="headerlink" title="fully_connected"></a><code>fully_connected</code></h2><p>The fully connected (or dense) layer performs a linear transformation on an input vector, typically after the feature extraction stages of a CNN. It maps the learned features to the final output classes. The operation is equivalent to a matrix-vector multiplication, where the matrix is the layer’s weights.</p>
<h3 id="Interface-Definition-4"><a href="#Interface-Definition-4" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">fully_connected_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_results, <span class="type">vec_t</span> chess_storage(VM)* in_vector,</span></span><br><span class="line"><span class="params">                                       <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base, <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec,</span></span><br><span class="line"><span class="params">                                       <span class="type">int</span> IN_FEATURES, <span class="type">int</span> OUT_FEATURES, <span class="type">int</span> mul_o, <span class="type">int</span> shf_o)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters-4"><a href="#Parameters-4" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_results</code>: A pointer to the output vector where the final scores or logits are stored.</li>
<li><code>in_vector</code>: A pointer to the flattened input feature vector.</li>
<li><code>weight_base</code>: A pointer to the weight matrix of shape <code>[OUT_FEATURES, IN_FEATURES]</code>.</li>
<li><code>bias_vec</code>: A pointer to the bias vector of size <code>OUT_FEATURES</code>.</li>
<li><code>IN_FEATURES</code>, <code>OUT_FEATURES</code>: The number of input and output features (neurons). Both are assumed to be multiples of <code>VSIZE</code>.</li>
<li><code>mul_o</code>, <code>shf_o</code>: Scaling factor and right-shift amount for output quantization.</li>
</ul>
<h3 id="Core-Logic-4"><a href="#Core-Logic-4" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function computes the output vector by iterating through the output features in tiles of <code>VSIZE</code>. For each output tile, it initializes an accumulator with the corresponding bias values. It then performs a series of <code>mat_vec_mul_add</code> operations, effectively multiplying a slice of the weight matrix with the entire input vector. This computes one tile of the output. A final quantization step is applied before storing the result.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Fully Connected Layer ---</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">fully_connected_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_results, <span class="type">vec_t</span> chess_storage(VM)* in_vector,</span></span><br><span class="line"><span class="params">                                       <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base, <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec,</span></span><br><span class="line"><span class="params">                                       <span class="type">int</span> IN_FEATURES, <span class="type">int</span> OUT_FEATURES, <span class="type">int</span> mul_o, <span class="type">int</span> shf_o)</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> IN_TILES = IN_FEATURES / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_TILES_VEC = OUT_FEATURES / VSIZE; <span class="comment">// Number of output vector tiles</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Process the output features in chunks of VSIZE</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> out_tile = <span class="number">0</span>; out_tile &lt; OUT_TILES_VEC; ++out_tile) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Initialize accumulator with the bias for this output tile</span></span><br><span class="line">        <span class="type">actvec_t</span> act = bias_vec[out_tile];</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Pointer to the start of the weights for this output tile</span></span><br><span class="line">        <span class="type">const</span> <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* wx_ptr = weight_base + out_tile * IN_TILES;</span><br><span class="line">        <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_vector;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Loop over the input features, performing matrix-vector multiplication in a tiled fashion</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> in_tile = <span class="number">0</span>; in_tile &lt; IN_TILES; ++in_tile) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            act = mat_vec_mul_add(act, *wx_ptr, *x_ptr);</span><br><span class="line">            wx_ptr++;</span><br><span class="line">            x_ptr++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Apply final quantization to the accumulated result</span></span><br><span class="line">        act = (act * mul_o) &gt;&gt; shf_o;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Store the VSIZE scalar results</span></span><br><span class="line">        out_results[out_tile] = <span class="type">vec_t</span>(act);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="attention-head"><a href="#attention-head" class="headerlink" title="attention_head"></a><code>attention_head</code></h2><p>This function implements a complete, quantized single-head attention mechanism, a fundamental component of the Transformer architecture. It executes the core scaled dot-product attention formula:</p>
<p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$</p>
<p>The implementation is highly optimized for a vector processor, employing techniques such as memory layout optimization, tiled computation, in-place operations, and integer quantization to achieve high performance. The function is broken down into four main computational steps.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">compute_attention_head</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Data Pointers ---</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* q_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* k_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* v_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* o_ptr,</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Intermediate Buffer Pointers ---</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* attention_scores_ptr,  <span class="comment">// Buffer for A = QK^T</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* attention_weights_ptr, <span class="comment">// Buffer for B = Softmax(A)</span></span></span><br><span class="line"><span class="params">    <span class="comment">// --- Dimension Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> T_SIZE,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> K_SIZE,</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Quantization/Scaling Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_mul_qk,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_shf_qk,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_mul_softmax,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_shf_softmax,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_mul_out,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_shf_out</span></span><br><span class="line"><span class="params">)</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">byte_off_t</span> mat_offset = VSIZE * VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">byte_off_t</span> actmat_offset = <span class="number">4</span> * VSIZE * VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 1: Attention Score Calculation -&gt; A = matmul(Q, K^T)</span></span><br><span class="line">    <span class="comment">// The code computes matmul(Q, K) assuming K is already laid out as K^T in memory.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; T_SIZE / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; VSIZE; ii++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">int</span> row_offset = ii * VSIZE + i * VSIZE * K_SIZE;</span><br><span class="line">                <span class="type">int</span> col_offset = j * VSIZE * K_SIZE;</span><br><span class="line"></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* q_vec_ptr = (<span class="type">vec_t</span> chess_storage(WM)*)q_ptr + (<span class="type">byte_off_t</span>)row_offset;</span><br><span class="line">                <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* k_mat_ptr = (<span class="type">mat_t</span> chess_storage(VM)*)k_ptr + (<span class="type">byte_off_t</span>)col_offset;</span><br><span class="line">                <span class="type">actvec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* a_actvec_ptr = (<span class="type">actvec_t</span> chess_storage(WM)*)attention_scores_ptr + (<span class="type">byte_off_t</span>)(row_offset * <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">                <span class="type">actvec_t</span> act = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; K_SIZE / VSIZE; k++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                    act = mat_vec_mul_add(act, *k_mat_ptr, *q_vec_ptr);</span><br><span class="line">                    q_vec_ptr = q_vec_ptr + mat_offset;</span><br><span class="line">                    k_mat_ptr = k_mat_ptr + mat_offset;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Scale and quantize the result of the matrix multiplication.</span></span><br><span class="line">                act = (act * scale_mul_qk) &gt;&gt; scale_shf_qk;</span><br><span class="line">                *a_actvec_ptr = act;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 2: B = Softmax(A)</span></span><br><span class="line">    <span class="comment">// This is a three-pass, row-wise softmax for numerical stability and quantization.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; T_SIZE / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; VSIZE; ii++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="type">int</span> row_base_offset = ii * VSIZE + i * VSIZE * K_SIZE;</span><br><span class="line">            <span class="type">int</span> act_row_base_offset = row_base_offset * <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">            <span class="type">actvec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* a_actvec_ptr = (<span class="type">actvec_t</span> chess_storage(WM)*)attention_scores_ptr + (<span class="type">byte_off_t</span>)(act_row_base_offset);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// --- Pass 1: Find the maximum value in the current row of A ---</span></span><br><span class="line">            <span class="type">actvec_t</span> act_max_vec = *a_actvec_ptr;</span><br><span class="line">            a_actvec_ptr = a_actvec_ptr + actmat_offset;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                act_max_vec = max_(*a_actvec_ptr, act_max_vec);</span><br><span class="line">                a_actvec_ptr = a_actvec_ptr + actmat_offset;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">int</span> max_val_scalar = <span class="number">-1000000</span>; <span class="comment">// A very small number</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                max_val_scalar = max_(max_val_scalar, act_ext(act_max_vec, j));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// --- Pass 2: Calculate exponentials and sum ---</span></span><br><span class="line">            <span class="comment">// We compute exp(x_i - max(x)) for numerical stability. pow2 is used as a hardware-friendly approximation of exp.</span></span><br><span class="line">            <span class="type">int</span> sum_exp = <span class="number">0</span>;</span><br><span class="line">            max_val_scalar = -max_val_scalar; <span class="comment">// Prepare for addition (subtraction)</span></span><br><span class="line">            a_actvec_ptr = (<span class="type">actvec_t</span> chess_storage(WM)*)attention_scores_ptr + (<span class="type">byte_off_t</span>)(act_row_base_offset);</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">actvec_t</span> act = *a_actvec_ptr;</span><br><span class="line">                act = act + max_val_scalar;</span><br><span class="line">                act = pow2(act); <span class="comment">// Approximation of exp2</span></span><br><span class="line">                *a_actvec_ptr = act; <span class="comment">// Overwrite A with exponentiated values</span></span><br><span class="line">                sum_exp = sum_exp + vsum(act);</span><br><span class="line">                a_actvec_ptr = a_actvec_ptr + actmat_offset;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// --- Pass 3: Normalize and quantize --- Calculate 1/sum_exp using integer division and scale it.</span></span><br><span class="line">            <span class="type">int</span> inv_sum = <span class="number">65536</span> / sum_exp; <span class="comment">// 65536 represents 1.0 in Q16 format</span></span><br><span class="line">            inv_sum = (inv_sum * scale_mul_softmax); <span class="comment">// Apply user-defined scaling</span></span><br><span class="line"></span><br><span class="line">            a_actvec_ptr = (<span class="type">actvec_t</span> chess_storage(WM)*)attention_scores_ptr + (<span class="type">byte_off_t</span>)(act_row_base_offset);</span><br><span class="line">            <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* b_vec_ptr = (<span class="type">vec_t</span> chess_storage(WM)*)attention_weights_ptr + (<span class="type">byte_off_t</span>)(row_base_offset);</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">actvec_t</span> act = *a_actvec_ptr;</span><br><span class="line">                act = act * inv_sum;</span><br><span class="line">                act = act &gt;&gt; scale_shf_softmax; <span class="comment">// Final shift for quantization</span></span><br><span class="line">                *b_vec_ptr = act; <span class="comment">// Store final 8-bit quantized attention weight</span></span><br><span class="line">                b_vec_ptr = b_vec_ptr + mat_offset;</span><br><span class="line">                a_actvec_ptr = a_actvec_ptr + actmat_offset;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 3: In-place Transpose of the Value (V) matrix. This prepares V for the final matrix multiplication.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; T_SIZE / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = i; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="type">byte_off_t</span> up_offset = (i * (T_SIZE / VSIZE) + j) * VSIZE * VSIZE;</span><br><span class="line">            <span class="type">byte_off_t</span> dw_offset = (j * (T_SIZE / VSIZE) + i) * VSIZE * VSIZE;</span><br><span class="line"></span><br><span class="line">            <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* mat_ptr_up = (<span class="type">mat_t</span> chess_storage(VM)*)v_ptr + up_offset;</span><br><span class="line">            <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* mat_ptr_dw = (<span class="type">mat_t</span> chess_storage(VM)*)v_ptr + dw_offset;</span><br><span class="line"></span><br><span class="line">            <span class="type">mat_t</span> mat_up = *mat_ptr_up;</span><br><span class="line">            <span class="keyword">if</span> (i == j) &#123;</span><br><span class="line">                *mat_ptr_up = transpose(mat_up);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">mat_t</span> mat_dw = *mat_ptr_dw;</span><br><span class="line">                *mat_ptr_up = transpose(mat_dw);</span><br><span class="line">                *mat_ptr_dw = transpose(mat_up);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 4: Final Output -&gt; O = matmul(B, V_transposed)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; T_SIZE / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; VSIZE; ii++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">int</span> row_offset = ii * VSIZE + i * VSIZE * K_SIZE;</span><br><span class="line">                <span class="type">int</span> col_offset = j * VSIZE * K_SIZE;</span><br><span class="line"></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* b_vec_ptr = (<span class="type">vec_t</span> chess_storage(WM)*)attention_weights_ptr + (<span class="type">byte_off_t</span>)row_offset;</span><br><span class="line">                <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* v_mat_ptr = (<span class="type">mat_t</span> chess_storage(VM)*)v_ptr + (<span class="type">byte_off_t</span>)col_offset;</span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* o_vec_ptr = (<span class="type">vec_t</span> chess_storage(VM)*)o_ptr + (<span class="type">byte_off_t</span>)row_offset;</span><br><span class="line"></span><br><span class="line">                <span class="type">actvec_t</span> act = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; K_SIZE / VSIZE; k++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                    act = mat_vec_mul_add(act, *v_mat_ptr, *b_vec_ptr);</span><br><span class="line">                    b_vec_ptr = b_vec_ptr + mat_offset;</span><br><span class="line">                    v_mat_ptr = v_mat_ptr + mat_offset;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// Scale and quantize the final output.</span></span><br><span class="line">                act = (act * scale_mul_out) &gt;&gt; scale_shf_out;</span><br><span class="line">                *o_vec_ptr = act;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Functional-Breakdown"><a href="#Functional-Breakdown" class="headerlink" title="Functional Breakdown"></a>Functional Breakdown</h3><ol>
<li><p><strong>Step 1: Attention Score Calculation (A &#x3D; QK<sup>T</sup>)</strong></p>
<ul>
<li>This step computes the dot product between each query vector and all key vectors. The code assumes the Key matrix (<code>k_ptr</code>) is already stored in a pre-transposed, tiled layout to facilitate efficient memory access patterns for the <code>mat_vec_mul_add</code> intrinsic.</li>
<li>The result, a 32-bit activation matrix <code>A</code>, is immediately scaled and quantized using the provided <code>scale_mul_qk</code> and <code>scale_shf_qk</code> parameters. This corresponds to the <code>1/sqrt(d_k)</code> scaling factor and prepares the data for the subsequent softmax operation. The result is stored in the <code>attention_scores_ptr</code> buffer.</li>
</ul>
</li>
<li><p><strong>Step 2: Quantized Softmax (B &#x3D; Softmax(A))</strong></p>
<ul>
<li>A numerically stable, three-pass softmax is applied row-wise to the attention scores matrix <code>A</code>.</li>
<li><strong>Pass 1:</strong> Finds the maximum value in each row to prevent numerical overflow during exponentiation.</li>
<li><strong>Pass 2:</strong> Subtracts the row maximum, computes an approximation of the exponential function using the hardware-accelerated <code>pow2</code> instruction, and calculates the sum of these exponentiated values for each row.</li>
<li><strong>Pass 3:</strong> Normalizes each element by the sum and performs a final scaling and quantization to produce the 8-bit attention weights matrix <code>B</code>, which is stored in the <code>attention_weights_ptr</code> buffer.</li>
</ul>
</li>
<li><p><strong>Step 3: In-place Transposition of Value Matrix (V)</strong></p>
<ul>
<li>To optimize the final matrix multiplication, the Value matrix (<code>v_ptr</code>) is transposed in-place. This operation rearranges the <code>VSIZE x VSIZE</code> blocks of the matrix, ensuring that the subsequent multiplication step can access data from <code>V</code> with a contiguous memory access pattern, which is critical for performance.</li>
</ul>
</li>
<li><p><strong>Step 4: Output Calculation (O &#x3D; BV)</strong></p>
<ul>
<li>The final output of the attention head is computed by multiplying the 8-bit attention weights matrix <code>B</code> with the now-transposed Value matrix <code>V</code>.</li>
<li>Similar to Step 1, this is performed using the efficient <code>mat_vec_mul_add</code> intrinsic. The 32-bit accumulated result is then scaled and quantized to the final 8-bit output format using <code>scale_mul_out</code> and <code>scale_shf_out</code> before being written to the output buffer <code>o_ptr</code>.</li>
</ul>
</li>
</ol>
<h2 id="Add-Norm"><a href="#Add-Norm" class="headerlink" title="Add&amp;Norm"></a><code>Add&amp;Norm</code></h2><p>This function implements a Post-LayerNorm residual connection, a standard operation found after the self-attention and feed-forward sub-layers in a Transformer block. It first performs an element-wise addition of an input tensor and a residual tensor, then applies Layer Normalization to the result. The implementation is carefully designed to maximize performance by using vector primitives for initial calculations and a register-pressure-aware strategy for the final normalization step.</p>
<p>The Layer Normalization formula is:</p>
<p>$$<br>\text{output} &#x3D; \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta<br>$$</p>
<p>where $x$ is the result of <code>input + residual</code>, $\mu$ and $\sigma^2$ are the mean and variance across the feature dimension, and $\gamma$ and $\beta$ are learnable scale and shift parameters.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">add_and_layer_norm</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Data Pointers (in Vector Memory) ---</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* input_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* residual_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* gamma_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* beta_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* output_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* temp_buffer_ptr,</span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params">    <span class="comment">// --- Dimension &amp; Configuration Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">const</span> <span class="type">int</span> D,</span></span><br><span class="line"><span class="params">    <span class="type">const</span> <span class="type">float</span> epsilon,</span></span><br><span class="line"><span class="params">    <span class="type">const</span> <span class="type">float</span> output_quant_scale</span></span><br><span class="line"><span class="params">)</span> &#123;</span><br><span class="line">    <span class="comment">// Cast void pointers to the correct vector types</span></span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_in   = (<span class="type">vec_t</span> chess_storage(VM)*)input_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_res  = (<span class="type">vec_t</span> chess_storage(VM)*)residual_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_gam  = (<span class="type">vec_t</span> chess_storage(VM)*)gamma_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_bet  = (<span class="type">vec_t</span> chess_storage(VM)*)beta_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_out  = (<span class="type">vec_t</span> chess_storage(VM)*)output_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_tmp  = (<span class="type">vec_t</span> chess_storage(VM)*)temp_buffer_ptr;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_vectors = D / VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 1: Residual Addition (Input + Residual) -&gt; Temp Buffer</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="type">vec_t</span> in_vec = *p_in++;</span><br><span class="line">        <span class="type">vec_t</span> res_vec = *p_res++;</span><br><span class="line">        <span class="type">vec_t</span> sum = in_vec + res_vec; <span class="comment">// Element-wise addition of input and residual</span></span><br><span class="line">        *p_tmp++ = sum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 2: Calculate Mean and Variance using vector primitives</span></span><br><span class="line">    <span class="comment">// We use Var(X) = E[X^2] - (E[X])^2 for efficiency.</span></span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> total_sum = <span class="number">0</span>;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> total_sum_sq = <span class="number">0</span>;</span><br><span class="line">    p_tmp = (<span class="type">vec_t</span> chess_storage(VM)*)temp_buffer_ptr; <span class="comment">// Reset temp pointer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="type">vec_t</span> tmp_vec = *p_tmp++;</span><br><span class="line">        <span class="type">actvec_t</span> tmp_actvec = (<span class="type">actvec_t</span>) tmp_vec; <span class="comment">// Promote 8-bit vector to 32-bit vector</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Accumulate sum for mean calculation</span></span><br><span class="line">        total_sum += (<span class="type">long</span> <span class="type">long</span>)vsum(tmp_actvec);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Accumulate sum of squares for variance calculation</span></span><br><span class="line">        <span class="type">actvec_t</span> tmp_actvec2 = tmp_actvec; <span class="comment">// Create a 32-bit accumulator vector</span></span><br><span class="line">        <span class="type">actvec_t</span> tmp_sq_actvec = tmp_actvec * tmp_actvec2; <span class="comment">// Element-wise square</span></span><br><span class="line">        total_sum_sq += (<span class="type">long</span> <span class="type">long</span>)vsum(tmp_sq_actvec);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// --- Perform float calculations for statistics ---</span></span><br><span class="line">    <span class="comment">// Note: Type conversions use the provided soft-float wrappers</span></span><br><span class="line">    <span class="type">float32_t</span> f_sum = wrap_i64_to_f32(total_sum);</span><br><span class="line">    <span class="type">float32_t</span> f_sum_sq = wrap_i64_to_f32(total_sum_sq);</span><br><span class="line">    <span class="type">float32_t</span> f_dim = wrap_i32_to_f32(D);</span><br><span class="line"></span><br><span class="line">    <span class="type">float32_t</span> mean = f_sum / f_dim;</span><br><span class="line">    <span class="type">float32_t</span> e_x_sq = f_sum_sq / f_dim;</span><br><span class="line">    <span class="type">float32_t</span> variance = e_x_sq - (mean * mean);</span><br><span class="line"></span><br><span class="line">    <span class="type">float32_t</span> inv_std_dev = <span class="number">1.0f</span> / sqrtf(variance + epsilon);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 3: Apply Normalization, Scale (γ), Shift (β), and Quantize</span></span><br><span class="line">    <span class="comment">// This part is done element-wise.</span></span><br><span class="line">    p_tmp = (<span class="type">vec_t</span> chess_storage(VM)*)temp_buffer_ptr; <span class="comment">// Reset pointers again</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="type">vec_t</span> tmp_vec = *p_tmp++;</span><br><span class="line">        <span class="type">vec_t</span> gam_vec = *p_gam++;</span><br><span class="line">        <span class="type">vec_t</span> bet_vec = *p_bet++;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- Create simple, spillable C arrays on the stack ---</span></span><br><span class="line">        <span class="type">int</span> val_scalars[VSIZE];</span><br><span class="line">        <span class="type">int</span> gam_scalars[VSIZE];</span><br><span class="line">        <span class="type">int</span> bet_scalars[VSIZE];</span><br><span class="line">        <span class="type">float32_t</span> norm_f_temp[VSIZE];</span><br><span class="line">        <span class="type">float32_t</span> scaled_f_temp[VSIZE];</span><br><span class="line">        <span class="type">int</span> final_results[VSIZE];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- STAGE 1: DATA EXTRACTION ---</span></span><br><span class="line">        <span class="comment">// This loop only uses vector intrinsics to read data. Low register pressure.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            val_scalars[j] = act_ext((<span class="type">actvec_t</span>)tmp_vec, j);</span><br><span class="line">            gam_scalars[j] = act_ext((<span class="type">actvec_t</span>)gam_vec, j);</span><br><span class="line">            bet_scalars[j] = act_ext((<span class="type">actvec_t</span>)bet_vec, j);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- STAGE 2: FLOATING-POINT COMPUTATION ---</span></span><br><span class="line">        <span class="comment">// This loop only uses standard C types and function calls.</span></span><br><span class="line">        <span class="comment">// The compiler can freely spill any of these variables to memory.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            <span class="comment">// --- Convert to float ---</span></span><br><span class="line">            <span class="type">float32_t</span> val_f = wrap_i32_to_f32(val_scalars[j]);</span><br><span class="line">            <span class="comment">// --- Apply LayerNorm formula in float ---</span></span><br><span class="line">            norm_f_temp[j] = (val_f - mean) * inv_std_dev;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            <span class="comment">// --- Scale and shift ---</span></span><br><span class="line">            <span class="type">float32_t</span> gam_f = wrap_i32_to_f32(gam_scalars[j]);</span><br><span class="line">            <span class="type">float32_t</span> bet_f = wrap_i32_to_f32(bet_scalars[j]);</span><br><span class="line">            scaled_f_temp[j] = norm_f_temp[j] * gam_f + bet_f;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            <span class="comment">// --- Quantize back to integer ---</span></span><br><span class="line">            final_results[j] = wrap_f32_to_i32(scaled_f_temp[j] * output_quant_scale);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- STAGE 3: VECTOR ASSEMBLY ---</span></span><br><span class="line">        <span class="comment">// This loop only uses vector intrinsics to write data. Low register pressure.</span></span><br><span class="line">        <span class="type">vec_t</span> out_actvec; <span class="comment">// Initialize an empty 32-bit accumulator vector</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            out_actvec = (<span class="type">vec_t</span>)act_upd(out_actvec, final_results[j], j);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- Store the final 8-bit vector ---</span></span><br><span class="line">        <span class="comment">// Convert the 32-bit accumulator vector back to an 8-bit vector and store</span></span><br><span class="line">        *p_out++ = (<span class="type">vec_t</span>)out_actvec;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ol>
<li><p><strong>Residual Addition:</strong> The function begins by performing a simple, element-wise vector addition <code>input + residual</code>, storing the intermediate 8-bit result in the provided <code>temp_buffer_ptr</code>.</p>
</li>
<li><p><strong>Statistics Calculation:</strong> In a single pass over the temporary buffer, the function calculates the sum and sum-of-squares of the elements required for normalization.</p>
<ul>
<li><strong>High-Precision Accumulation:</strong> It uses <code>long long</code> accumulators (<code>total_sum</code>, <code>total_sum_sq</code>) to prevent overflow when summing up values over a large feature dimension <code>D</code>.</li>
<li><strong>Efficient Variance:</strong> It computes variance using the stable formula $\text{Var}(X) &#x3D; E[X^2] - (E[X])^2$.</li>
<li><strong>Floating-Point Conversion:</strong> The final statistics (mean, variance, inverse standard deviation) are calculated using 32-bit floating-point arithmetic for higher accuracy.</li>
</ul>
</li>
<li><p><strong>Normalization, Scaling, and Quantization:</strong> This final step applies the normalization formula and is implemented using a three-stage approach to minimize register pressure and allow the compiler to generate efficient code. For each vector of data:</p>
<ul>
<li><strong>Stage 1 (Data Extraction):</strong> Vector data (<code>tmp_vec</code>, <code>gam_vec</code>, <code>bet_vec</code>) is read from memory and its elements are extracted into simple scalar C arrays on the stack. This isolates vector memory access.</li>
<li><strong>Stage 2 (Floating-Point Computation):</strong> All floating-point operations—normalization, scaling ($\gamma$), shifting ($\beta$), and final output scaling—are performed on the scalar arrays. This core computational stage is free of vector intrinsics, making it easier for the compiler to manage registers by spilling variables to the stack if necessary.</li>
<li><strong>Stage 3 (Vector Assembly):</strong> The final integer results are assembled back into a vector register using <code>act_upd</code> intrinsics and then written to the output memory location.</li>
</ul>
</li>
</ol>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a><code>Softmax</code></h2><p>This function implements a row-wise, numerically stable, quantized softmax operation. It is optimized for vector processors and is a key subroutine within the <code>attention_head</code> function, but is also provided here as a standalone kernel. It takes a matrix of 32-bit activation scores as input and produces a matrix of 8-bit quantized weights.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">quantized_softmax_rowwise</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Data Pointers ---</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* in_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* out_ptr,</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Dimension Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> dim_size,</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Quantization/Scaling Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_mul_softmax,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_shf_softmax</span></span><br><span class="line"><span class="params">)</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">byte_off_t</span> act_tile_offset = <span class="number">4</span> * VSIZE * VSIZE; <span class="comment">// Offset for a tile of 32-bit activations</span></span><br><span class="line">    <span class="type">const</span> <span class="type">byte_off_t</span> out_tile_offset = VSIZE * VSIZE;     <span class="comment">// Offset for a tile of 8-bit outputs</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over each row of the input matrix.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; dim_size / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; VSIZE; ii++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Calculate the byte offset to the beginning of the current row.</span></span><br><span class="line">            <span class="type">int</span> row_start_idx = ii * VSIZE + i * VSIZE * dim_size;</span><br><span class="line">            <span class="type">int</span> act_row_base_offset = (<span class="type">int</span>)row_start_idx * <span class="number">4</span>;</span><br><span class="line">            <span class="type">int</span> out_row_base_offset = (<span class="type">int</span>)row_start_idx;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// === Pass 1: Find the maximum value in the current row ===</span></span><br><span class="line">            <span class="type">actvec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* p_act_row = (<span class="type">actvec_t</span> chess_storage(WM)*)in_ptr + (<span class="type">byte_off_t</span>)act_row_base_offset;</span><br><span class="line">            <span class="type">actvec_t</span> act_max_vec = *p_act_row;</span><br><span class="line"></span><br><span class="line">            p_act_row = p_act_row + (<span class="type">byte_off_t</span>)act_tile_offset; <span class="comment">// Move to the next tile in the same row</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt; dim_size / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                act_max_vec = max_(*p_act_row, act_max_vec);</span><br><span class="line">                p_act_row = p_act_row + (<span class="type">byte_off_t</span>)act_tile_offset;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// Reduce the vector max to a scalar max</span></span><br><span class="line">            <span class="type">int</span> max_val_scalar = <span class="number">-1000000</span>; <span class="comment">// Initialize with a very small number</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                max_val_scalar = max_(max_val_scalar, act_ext(act_max_vec, j));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// === Pass 2: Calculate exponentials (using pow2) and sum ===</span></span><br><span class="line">            <span class="comment">// We compute exp(x_i - max(x)) for numerical stability.</span></span><br><span class="line">            <span class="type">int</span> sum_exp = <span class="number">0</span>;</span><br><span class="line">            max_val_scalar = -max_val_scalar; <span class="comment">// Negate for addition (to perform subtraction)</span></span><br><span class="line"></span><br><span class="line">            p_act_row = (<span class="type">actvec_t</span> chess_storage(WM)*)in_ptr + (<span class="type">byte_off_t</span>)act_row_base_offset;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; dim_size / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">actvec_t</span> act = *p_act_row;</span><br><span class="line">                act = act + max_val_scalar;</span><br><span class="line">                act = pow2(act); <span class="comment">// Hardware approximation of exp2</span></span><br><span class="line">                *p_act_row = act; <span class="comment">// Overwrite input buffer with exponentiated values</span></span><br><span class="line">                sum_exp = sum_exp + vsum(act);</span><br><span class="line">                p_act_row = p_act_row + (<span class="type">byte_off_t</span>)act_tile_offset;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// === Pass 3: Normalize and quantize the row ===</span></span><br><span class="line">            <span class="type">int</span> inv_sum = (sum_exp == <span class="number">0</span>) ? <span class="number">0</span> : (<span class="number">65536</span> / sum_exp);</span><br><span class="line">            inv_sum = (inv_sum * scale_mul_softmax); <span class="comment">// Apply user-defined scaling</span></span><br><span class="line"></span><br><span class="line">            p_act_row = (<span class="type">actvec_t</span> chess_storage(WM)*)in_ptr + (<span class="type">byte_off_t</span>)act_row_base_offset;</span><br><span class="line">            <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* p_out_row = (<span class="type">vec_t</span> chess_storage(WM)*)out_ptr + (<span class="type">byte_off_t</span>)out_row_base_offset;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; dim_size / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">actvec_t</span> act = *p_act_row;</span><br><span class="line">                act = act * inv_sum;</span><br><span class="line">                act = act &gt;&gt; scale_shf_softmax; <span class="comment">// Final shift for quantization</span></span><br><span class="line">                *p_out_row = act; <span class="comment">// Store final 8-bit quantized attention weight</span></span><br><span class="line">                p_out_row = p_out_row + (<span class="type">byte_off_t</span>)out_tile_offset;</span><br><span class="line">                p_act_row = p_act_row + (<span class="type">byte_off_t</span>)act_tile_offset;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Algorithmic-Strategy"><a href="#Algorithmic-Strategy" class="headerlink" title="Algorithmic Strategy"></a>Algorithmic Strategy</h3><p>The function employs a three-pass algorithm to compute the softmax for each row of the input matrix.</p>
<ol>
<li><p><strong>Pass 1: Find Row Maximum:</strong> To ensure numerical stability, the first pass iterates through each row to find its maximum value. Subtracting this maximum from every element in the row before exponentiation $(\text{i.e., } e^{x_i - \max(x)})$ recenters the data around zero, preventing potential floating-point overflow when dealing with large activation values. This pass uses vector <code>max_</code> operations for efficiency, followed by a scalar reduction to get the final maximum value.</p>
</li>
<li><p><strong>Pass 2: Exponentiate and Sum:</strong> The second pass revisits each row. It subtracts the previously found maximum value, computes the exponential, and accumulates the sum of these exponentiated values.</p>
<ul>
<li><strong>Hardware Approximation:</strong> The standard <code>exp(x)</code> function is replaced with a hardware-friendly <code>pow2(x)</code> approximation, which is significantly faster on the target processor.</li>
<li><strong>In-Place Operation:</strong> To conserve memory, the calculated exponentiated values overwrite the original input data in <code>in_ptr</code>.</li>
</ul>
</li>
<li><p><strong>Pass 3: Normalize and Quantize:</strong> The final pass normalizes the exponentiated values and quantizes them to 8-bit integers.</p>
<ul>
<li><strong>Multiplicative Inverse:</strong> Instead of performing a costly division by <code>sum_exp</code> for each element, the code calculates the scaled reciprocal (<code>inv_sum</code>) once per row and uses multiplication. The fixed-point value <code>65536</code> represents <code>1.0</code>.</li>
<li><strong>Quantization:</strong> The result is scaled by <code>inv_sum</code> and the user-provided <code>scale_mul_softmax</code>, then right-shifted by <code>scale_shf_softmax</code> to produce the final 8-bit quantized value, which is written to the <code>out_ptr</code> buffer.</li>
</ul>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Yuchao Qin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/07/25/flexacc_manual/C_library4ML/">http://example.com/2025/07/25/flexacc_manual/C_library4ML/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Yuchao's Homepage</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/FlexAcc-Kernel/">FlexAcc Kernel</a></div><div class="post_share"><div class="social-share" data-image="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/08/07/learning_note/SIMD&amp;SIMT/" title="SIMT vs. SIMD - Deconstructing Parallel Architectures"><img class="cover" src="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads#/media/File:Vortex_microarchitecture.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">SIMT vs. SIMD - Deconstructing Parallel Architectures</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/03/asip_design_tool/asip_training_processor_modeling/" title="ASIP Designer Processor Modeling"><img class="cover" src="https://www.synopsys.com/dw/images/ds/asip-designer-flow-diagram.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ASIP Designer Processor Modeling</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/image/profile.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Yuchao Qin</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">8</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">5</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/worldline22"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/worldline22" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:qinyuchao22@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #D44638;"></i></a><a class="social-icon" href="/image/myweixin.png" target="_blank" title="Wechat"><i class="fab fa-weixin" style="color: #7BB32E;"></i></a><a class="social-icon" href="https://x.com/worldline22" target="_blank" title="Twitter"><i class="fab fa-twitter" style="color: #1da1f2;"></i></a><a class="social-icon" href="https://www.facebook.com/profile.php?id=61561687587707" target="_blank" title="Facebook"><i class="fab fa-facebook" style="color: #3b5998;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Happy Chinese new year! 🎉🎉🎉</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Convolutional-Operations"><span class="toc-number">1.</span> <span class="toc-text">Convolutional Operations</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#conv2d"><span class="toc-number">1.1.</span> <span class="toc-text">conv2d</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition"><span class="toc-number">1.1.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters"><span class="toc-number">1.1.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic"><span class="toc-number">1.1.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#max-pooling"><span class="toc-number">1.2.</span> <span class="toc-text">max_pooling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition-1"><span class="toc-number">1.2.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters-1"><span class="toc-number">1.2.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#add-2-relu"><span class="toc-number">1.3.</span> <span class="toc-text">add_2_relu</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition-2"><span class="toc-number">1.3.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters-2"><span class="toc-number">1.3.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic-2"><span class="toc-number">1.3.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#average-pooling"><span class="toc-number">1.4.</span> <span class="toc-text">average_pooling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition-3"><span class="toc-number">1.4.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters-3"><span class="toc-number">1.4.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic-3"><span class="toc-number">1.4.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#fully-connected"><span class="toc-number">1.5.</span> <span class="toc-text">fully_connected</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition-4"><span class="toc-number">1.5.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters-4"><span class="toc-number">1.5.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic-4"><span class="toc-number">1.5.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer"><span class="toc-number">2.</span> <span class="toc-text">Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#attention-head"><span class="toc-number">2.1.</span> <span class="toc-text">attention_head</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Functional-Breakdown"><span class="toc-number">2.1.1.</span> <span class="toc-text">Functional Breakdown</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Add-Norm"><span class="toc-number">2.2.</span> <span class="toc-text">Add&amp;Norm</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Implementation-Details"><span class="toc-number">2.2.1.</span> <span class="toc-text">Implementation Details</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Softmax"><span class="toc-number">2.3.</span> <span class="toc-text">Softmax</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Algorithmic-Strategy"><span class="toc-number">2.3.1.</span> <span class="toc-text">Algorithmic Strategy</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/10/20/ras/ras_readme/" title="RAS - A Bit-Exact rANS Accelerator for High-Performance Neural Lossless Compression"><img src="https://worldline22.github.io/image/ras/ras_bk_overview.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="RAS - A Bit-Exact rANS Accelerator for High-Performance Neural Lossless Compression"/></a><div class="content"><a class="title" href="/2025/10/20/ras/ras_readme/" title="RAS - A Bit-Exact rANS Accelerator for High-Performance Neural Lossless Compression">RAS - A Bit-Exact rANS Accelerator for High-Performance Neural Lossless Compression</a><time datetime="2025-10-20T11:25:00.000Z" title="发表于 2025-10-20 19:25:00">2025-10-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/20/test/" title="无题"><img src="https://th.bing.com/th/id/OIP.dRAwl80sSA-35xbqyTL5XAHaFj?rs=1&amp;pid=ImgDetMain" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="无题"/></a><div class="content"><a class="title" href="/2025/08/20/test/" title="无题">无题</a><time datetime="2025-08-19T21:54:31.125Z" title="发表于 2025-08-20 05:54:31">2025-08-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/08/07/learning_note/SIMD&amp;SIMT/" title="SIMT vs. SIMD - Deconstructing Parallel Architectures"><img src="https://en.wikipedia.org/wiki/Single_instruction,_multiple_threads#/media/File:Vortex_microarchitecture.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SIMT vs. SIMD - Deconstructing Parallel Architectures"/></a><div class="content"><a class="title" href="/2025/08/07/learning_note/SIMD&amp;SIMT/" title="SIMT vs. SIMD - Deconstructing Parallel Architectures">SIMT vs. SIMD - Deconstructing Parallel Architectures</a><time datetime="2025-08-07T06:00:00.000Z" title="发表于 2025-08-07 14:00:00">2025-08-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/25/flexacc_manual/C_library4ML/" title="C Library for ML in FlexACC"><img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C Library for ML in FlexACC"/></a><div class="content"><a class="title" href="/2025/07/25/flexacc_manual/C_library4ML/" title="C Library for ML in FlexACC">C Library for ML in FlexACC</a><time datetime="2025-07-24T19:00:00.000Z" title="发表于 2025-07-25 03:00:00">2025-07-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/03/asip_design_tool/asip_training_processor_modeling/" title="ASIP Designer Processor Modeling"><img src="https://www.synopsys.com/dw/images/ds/asip-designer-flow-diagram.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ASIP Designer Processor Modeling"/></a><div class="content"><a class="title" href="/2025/07/03/asip_design_tool/asip_training_processor_modeling/" title="ASIP Designer Processor Modeling">ASIP Designer Processor Modeling</a><time datetime="2025-07-03T00:00:00.000Z" title="发表于 2025-07-03 08:00:00">2025-07-03</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By Yuchao Qin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://worldline22.github.io">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://example.com/2025/07/25/flexacc_manual/C_library4ML/'
    this.page.identifier = '/2025/07/25/flexacc_manual/C_library4ML/'
    this.page.title = 'C Library for ML in FlexACC'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23liX4NCpV2ZdkGR2S',
      clientSecret: '0704901937de27f01f6057bd2c6c7992f40346a6',
      repo: 'worldline22.github.io.gitalk',
      owner: 'Yuchao Qin',
      admin: ['Yuchao Qin'],
      id: 'a12951a9f34df2ac9fbbb11b90ac5f5b',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Valine' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>