<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>C Library for ML in FlexACC | Yuchao's Homepage</title><meta name="author" content="Yuchao Qin"><meta name="copyright" content="Yuchao Qin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="A guide to FlexAcc c function kernels">
<meta property="og:type" content="article">
<meta property="og:title" content="C Library for ML in FlexACC">
<meta property="og:url" content="http://example.com/2025/07/24/flexacc_manual/C_library4ML/index.html">
<meta property="og:site_name" content="Yuchao&#39;s Homepage">
<meta property="og:description" content="A guide to FlexAcc c function kernels">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png">
<meta property="article:published_time" content="2025-07-24T19:00:00.000Z">
<meta property="article:modified_time" content="2025-07-24T22:03:36.792Z">
<meta property="article:author" content="Yuchao Qin">
<meta property="article:tag" content="FlexAcc Kernel">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2025/07/24/flexacc_manual/C_library4ML/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":false,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'C Library for ML in FlexACC',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-24 15:03:36'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/transpancy.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/image/profile.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Menu</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/life/"><i class="fa-fw fas fa-coffee"></i><span> Life</span></a></li><li><a class="site-page child" href="/science/"><i class="fa-fw fas fa-atom"></i><span> Research</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Photo</span></a></li><li><a class="site-page child" href="/pku/"><i class="fa-fw fas fa-book-open"></i><span> 学在北大</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Yuchao's Homepage"><img class="site-icon" src="/image/test.gif"/><span class="site-name">Yuchao's Homepage</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Menu</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/life/"><i class="fa-fw fas fa-coffee"></i><span> Life</span></a></li><li><a class="site-page child" href="/science/"><i class="fa-fw fas fa-atom"></i><span> Research</span></a></li><li><a class="site-page child" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> Photo</span></a></li><li><a class="site-page child" href="/pku/"><i class="fa-fw fas fa-book-open"></i><span> 学在北大</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">C Library for ML in FlexACC</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-24T19:00:00.000Z" title="发表于 2025-07-24 12:00:00">2025-07-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-07-24T22:03:36.792Z" title="更新于 2025-07-24 15:03:36">2025-07-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Computer-Architecture/">Computer Architecture</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="C Library for ML in FlexACC"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>Of course. Here is a polished and enriched version of your report. I have expanded on the explanations, clarified the purpose of each function and its parameters, and improved the overall structure for better readability, all while adhering to your specified formatting requirements.</p>
<h1 id="Convolutional-Operations"><a href="#Convolutional-Operations" class="headerlink" title="Convolutional Operations"></a>Convolutional Operations</h1><p>This document details a set of core neural network operations implemented in C, specifically optimized for a Single Instruction, Multiple Data (SIMD) architecture. To achieve high computational throughput, all data structures such as inputs, outputs, and weights are stored in memory using vector types (e.g., <code>actvec_t</code>, <code>vec_t</code>, <code>mat_t</code>). These functions form the building blocks for constructing efficient convolutional neural networks (CNNs).</p>
<h2 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a><code>conv2d</code></h2><p>The 2D convolution is the cornerstone of modern computer vision models. This function performs a 2D convolution on an input tensor, applying a set of learnable filters (weights) to produce an output feature map. It also includes fused operations for adding a bias, applying quantization, and executing a ReLU activation function, which minimizes memory access and improves performance.</p>
<h3 id="Interface-Definition"><a href="#Interface-Definition" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">conv2d_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base,</span></span><br><span class="line"><span class="params">                              <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec, <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> IN_C,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> OUT_C, <span class="type">int</span> K, <span class="type">int</span> STRIDE, <span class="type">int</span> PAD,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> mul_o, <span class="type">int</span> shf_o, <span class="type">int</span> apply_relu)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_base</code>: A pointer to the base address in vector memory (<code>VM</code>) where the output tensor will be stored.</li>
<li><code>in_base</code>: A pointer to the base address in vector memory (<code>VM</code>) of the input tensor.</li>
<li><code>weight_base</code>: A pointer to the base address in weight memory (<code>WM</code>) of the convolution kernels (filters).</li>
<li><code>bias_vec</code>: A pointer to the base address in weight memory (<code>WM</code>) for the bias vector. Each element is added to a corresponding output channel.</li>
<li><code>IN_H</code>, <code>IN_W</code>, <code>IN_C</code>: The height, width, and number of channels of the input tensor. <code>IN_C</code> is assumed to be a multiple of the vector size <code>VSIZE</code>.</li>
<li><code>OUT_C</code>: The number of output channels, which corresponds to the number of convolutional filters. <code>OUT_C</code> is also assumed to be a multiple of <code>VSIZE</code>.</li>
<li><code>K</code>: The size (height and width) of the square convolutional kernel.</li>
<li><code>STRIDE</code>: The step size, or stride, for moving the convolutional window across the input tensor.</li>
<li><code>PAD</code>: The number of zero-value pixels to pad around the border of the input tensor. This allows control over the output spatial dimensions.</li>
<li><code>mul_o</code>, <code>shf_o</code>: Scaling factor and right-shift amount for output quantization. This is a common technique in integer-only inference to approximate floating-point division.</li>
<li><code>apply_relu</code>: A flag (1 or 0) that determines whether to apply a Rectified Linear Unit (ReLU) activation function to the output.</li>
</ul>
<h3 id="Core-Logic"><a href="#Core-Logic" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function calculates the output dimensions and then iterates through each spatial location <code>(oh, ow)</code> of the output tensor. For each location, it computes the value for all output channels in a tiled manner.</p>
<p>$$<br>\text{OUT_H} &#x3D; \frac{(\text{IN_H} - K + 2 \times \text{PAD})}{\text{STRIDE}} + 1<br>$$<br>$$<br>\text{OUT_W} &#x3D; \frac{(\text{IN_W} - K + 2 \times \text{PAD})}{\text{STRIDE}} + 1<br>$$</p>
<p>The core computation is a multiply-accumulate operation performed on vector data types. The loops are structured to maximize data locality and enable efficient SIMD processing.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- 2D convolution with Fused ReLU ---</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">conv2d_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base,</span></span><br><span class="line"><span class="params">                              <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec, <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> IN_C,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> OUT_C, <span class="type">int</span> K, <span class="type">int</span> STRIDE, <span class="type">int</span> PAD,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> mul_o, <span class="type">int</span> shf_o, <span class="type">int</span> apply_relu)</span> &#123;</span><br><span class="line">    <span class="comment">// Calculate output tensor dimensions</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_H = (IN_H - K + <span class="number">2</span> * PAD) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_W = (IN_W - K + <span class="number">2</span> * PAD) / STRIDE + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate the number of vector tiles for input and output channels</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> IN_C_TILES = IN_C / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_C_TILES = OUT_C / VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over each spatial location in the output tensor</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> oh = <span class="number">0</span>; oh &lt; OUT_H; oh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ow = <span class="number">0</span>; ow &lt; OUT_W; ow++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Iterate over the output channels in chunks of VSIZE</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> oc_tile = <span class="number">0</span>; oc_tile &lt; OUT_C_TILES; oc_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="comment">// Initialize accumulator with the bias for the current output channel tile</span></span><br><span class="line">                <span class="type">actvec_t</span> act = bias_vec[oc_tile];</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Calculate the corresponding input window, considering padding</span></span><br><span class="line">                <span class="type">int</span> ih_start = -PAD + oh * STRIDE;</span><br><span class="line">                <span class="type">int</span> iw_start = -PAD + ow * STRIDE;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Determine the valid kernel region to apply, clamping to input boundaries</span></span><br><span class="line">                <span class="type">int</span> wh_start = max_(<span class="number">0</span>, PAD - oh * STRIDE);</span><br><span class="line">                <span class="type">int</span> ww_start = max_(<span class="number">0</span>, PAD - ow * STRIDE);</span><br><span class="line">                <span class="type">int</span> wh_end   = min_(K, IN_H - ih_start);</span><br><span class="line">                <span class="type">int</span> ww_end   = min_(K, IN_W - iw_start);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Slide the kernel over the input window</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> wh = wh_start; wh &lt; wh_end; wh++) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> ww = ww_start; ww &lt; ww_end; ww++) &#123;</span><br><span class="line">                        <span class="comment">// Pointer to weights for this kernel position and output channel tile</span></span><br><span class="line">                        <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* wx_ptr = (<span class="type">mat_t</span> chess_storage(WM)*)weight_base + oc_tile * (K * K * IN_C_TILES) + wh * (K * IN_C_TILES) + ww * (IN_C_TILES);</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Pointer to the corresponding input data</span></span><br><span class="line">                        <span class="type">int</span> ih = ih_start + wh;</span><br><span class="line">                        <span class="type">int</span> iw = iw_start + ww;</span><br><span class="line">                        <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = (<span class="type">vec_t</span> chess_storage(VM)*)in_base + ih * (IN_W * IN_C_TILES) + iw * (IN_C_TILES);</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Perform multiply-accumulate across all input channel tiles</span></span><br><span class="line">                        <span class="keyword">for</span> (<span class="type">int</span> ic_tile = <span class="number">0</span>; ic_tile &lt; IN_C_TILES; ic_tile++) &#123;</span><br><span class="line">                            act = mat_vec_mul_add(act, *wx_ptr, *x_ptr);</span><br><span class="line">                            wx_ptr++;</span><br><span class="line">                            x_ptr++;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Quantization and Activation ---</span></span><br><span class="line">                <span class="comment">// Apply post-convolution quantization</span></span><br><span class="line">                act = (act * mul_o) &gt;&gt; shf_o;</span><br><span class="line">                </span><br><span class="line">                <span class="comment">// Apply ReLU activation if enabled</span></span><br><span class="line">                <span class="keyword">if</span> (apply_relu) &#123;</span><br><span class="line">                    act = max_(act, <span class="type">actvec_t</span>(<span class="number">0</span>));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Store Output ---</span></span><br><span class="line">                <span class="comment">// Calculate the output address and store the result</span></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* o_ptr = (<span class="type">vec_t</span> chess_storage(VM)*)out_base + oh * (OUT_W * OUT_C_TILES) + ow * (OUT_C_TILES) + oc_tile;</span><br><span class="line">                *o_ptr = <span class="type">vec_t</span>(act);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max_pooling"></a><code>max_pooling</code></h2><p>Max pooling is a down-sampling operation that reduces the spatial dimensions (height and width) of a feature map. It works by sliding a window over the input and selecting the maximum value within that window. This helps to make the feature representation more robust to small translations in the input image.</p>
<h3 id="Interface-Definition-1"><a href="#Interface-Definition-1" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">max_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, </span></span><br><span class="line"><span class="params">                                <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C,</span></span><br><span class="line"><span class="params">                                <span class="type">int</span> K, <span class="type">int</span> STRIDE)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters-1"><a href="#Parameters-1" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_base</code>: A pointer to the base address for the down-sampled output tensor.</li>
<li><code>in_base</code>: A pointer to the base address of the input tensor.</li>
<li><code>IN_H</code>, <code>IN_W</code>, <code>C</code>: The height, width, and number of channels of the input tensor. <code>C</code> is assumed to be a multiple of <code>VSIZE</code>.</li>
<li><code>K</code>: The size (height and width) of the square pooling window.</li>
<li><code>STRIDE</code>: The step size for moving the pooling window across the input.</li>
</ul>
<h3 id="Core-Logic-1"><a href="#Core-Logic-1" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function computes the output dimensions based on the input size, kernel size, and stride. It then iterates over each position of the output grid and each channel tile. For each output value, it scans the corresponding K×KK \times K window in the input tensor and finds the maximum value vector-wise.</p>
<p>$$<br>\text{OUT_H} &#x3D; \frac{(\text{IN_H} - K)}{\text{STRIDE}} + 1<br>$$<br>$$<br>\text{OUT_W} &#x3D; \frac{(\text{IN_W} - K)}{\text{STRIDE}} + 1<br>$$</p>
<p>The accumulator vector <code>max_accumulator_act</code> is initialized with the minimum possible value to ensure that the first element from the input window is always selected as the initial maximum.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">max_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, </span></span><br><span class="line"><span class="params">                                <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C,</span></span><br><span class="line"><span class="params">                                <span class="type">int</span> K, <span class="type">int</span> STRIDE)</span> &#123;</span><br><span class="line">    <span class="comment">// Calculate output dimensions</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_H = (IN_H - K) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_W = (IN_W - K) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Pre-calculate channel tile count</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> C_TILES = C / VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over the output spatial grid</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> oh = <span class="number">0</span>; oh &lt; OUT_H; oh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ow = <span class="number">0</span>; ow &lt; OUT_W; ow++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Iterate over channel tiles</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> c_tile = <span class="number">0</span>; c_tile &lt; C_TILES; c_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="comment">// Initialize a vector with the minimum possible scalar value (-128 for int8).</span></span><br><span class="line">                <span class="comment">// This ensures any real value from the input will be greater.</span></span><br><span class="line">                <span class="type">actvec_t</span> max_accumulator_act = <span class="number">-128</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Iterate over the pooling window (kernel)</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> kh = <span class="number">0</span>; kh &lt; K; kh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> kw = <span class="number">0</span>; kw &lt; K; kw++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                        <span class="comment">// Calculate input coordinates</span></span><br><span class="line">                        <span class="type">int</span> ih = oh * STRIDE + kh;</span><br><span class="line">                        <span class="type">int</span> iw = ow * STRIDE + kw;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// Pointer to Input[ih][iw][c_tile]</span></span><br><span class="line">                        <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_base +</span><br><span class="line">                                                 ih * (IN_W * C_TILES) +</span><br><span class="line">                                                        iw * (C_TILES) +</span><br><span class="line">                                                                 c_tile;</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Load the input vector and promote it to a wide accumulator type</span></span><br><span class="line">                        <span class="type">actvec_t</span> current_val_act = (<span class="type">actvec_t</span>)* x_ptr;</span><br><span class="line">                        <span class="comment">// Perform element-wise vector maximum</span></span><br><span class="line">                        max_accumulator_act = max_(max_accumulator_act, current_val_act);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Store Output ---</span></span><br><span class="line">                <span class="comment">// Calculate output address: &amp;Output[oh][ow][c_tile]</span></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* o_ptr = out_base +</span><br><span class="line">                                   oh * (OUT_W * C_TILES) +</span><br><span class="line">                                           ow * (C_TILES) +</span><br><span class="line">                                                    c_tile;</span><br><span class="line">                <span class="comment">// Convert the final wide max vector back to a narrow storage vector and store</span></span><br><span class="line">                *o_ptr = (<span class="type">vec_t</span>)max_accumulator_act;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="add-2-relu"><a href="#add-2-relu" class="headerlink" title="add_2_relu"></a><code>add_2_relu</code></h2><p>This operation is fundamental to residual network architectures (e.g., ResNet). It implements an element-wise addition of two tensors (a “main path” and a “shortcut path”) followed by a ReLU activation. This “shortcut connection” allows the gradient to flow more directly through the network, enabling the training of much deeper models.</p>
<h3 id="Interface-Definition-2"><a href="#Interface-Definition-2" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">add_and_relu_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_data, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* main_path,</span></span><br><span class="line"><span class="params">                                    <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* shortcut_path, <span class="type">int</span> num_vectors)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters-2"><a href="#Parameters-2" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_data</code>: A pointer to the memory where the output vector will be stored.</li>
<li><code>main_path</code>: A pointer to the input tensor from the main computational path (e.g., the output of a convolutional block).</li>
<li><code>shortcut_path</code>: A pointer to the input tensor from the shortcut or identity path.</li>
<li><code>num_vectors</code>: The total number of vectors in the input tensors. This is equivalent to <code>(H * W * C) / VSIZE</code>.</li>
</ul>
<h3 id="Core-Logic-2"><a href="#Core-Logic-2" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function iterates through the input tensors one vector at a time. In each iteration, it loads one vector from the <code>main_path</code> and one from the <code>shortcut_path</code>, adds them together, and then applies the ReLU activation <code>max(0, sum)</code>. The result is stored in the output buffer.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Add and ReLU ---</span></span><br><span class="line"><span class="comment">// This function performs element-wise addition of two tensors and applies a ReLU activation.</span></span><br><span class="line"><span class="comment">// It is a key component of residual blocks in architectures like ResNet.</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">add_and_relu_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_data, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* main_path,</span></span><br><span class="line"><span class="params">                                    <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* shortcut_path, <span class="type">int</span> num_vectors)</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; ++i) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Load both vectors and promote to wide accumulator type for addition</span></span><br><span class="line">        <span class="type">actvec_t</span> main_act = <span class="type">actvec_t</span>(main_path[i]);</span><br><span class="line">        <span class="type">actvec_t</span> shortcut_act = <span class="type">actvec_t</span>(shortcut_path[i]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Perform the element-wise addition</span></span><br><span class="line">        <span class="type">actvec_t</span> sum = main_act + shortcut_act;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Apply ReLU activation: result = max(0, sum)</span></span><br><span class="line">        <span class="type">actvec_t</span> result = max_(sum, <span class="type">actvec_t</span>(<span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Store the final result</span></span><br><span class="line">        out_data[i] = <span class="type">vec_t</span>(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="average-pooling"><a href="#average-pooling" class="headerlink" title="average_pooling"></a><code>average_pooling</code></h2><p>Global Average Pooling (GAP) is another down-sampling technique that reduces the entire spatial dimensions of each feature map into a single value, calculated as the average of all values in that map. It is often used in modern CNNs just before the final fully connected layer to reduce the number of parameters and prevent overfitting.</p>
<h3 id="Interface-Definition-3"><a href="#Interface-Definition-3" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">global_average_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_vector, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* in_base,</span></span><br><span class="line"><span class="params">                                           <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters-3"><a href="#Parameters-3" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_vector</code>: A pointer to the output vector of size <code>C</code>.</li>
<li><code>in_base</code>: A pointer to the input tensor.</li>
<li><code>IN_H</code>, <code>IN_W</code>, <code>C</code>: The height, width, and number of channels of the input tensor. <code>C</code> is assumed to be a multiple of <code>VSIZE</code>.</li>
</ul>
<h3 id="Core-Logic-3"><a href="#Core-Logic-3" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function first calculates the total number of spatial elements, <code>num_pixels = IN_H * IN_W</code>. It then iterates through each channel tile. For each tile, it accumulates the sum of all vectors across the spatial dimensions. Finally, it performs a per-lane division of the accumulated sum by <code>num_pixels</code> to compute the average. This lane-wise division is necessary because each lane in the vector corresponds to a different channel.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Global Average Pooling ---</span></span><br><span class="line"><span class="comment">// This function computes the average of each channel across all spatial dimensions (H x W).</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">global_average_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_vector, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* in_base,</span></span><br><span class="line"><span class="params">                                           <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C)</span> &#123;</span><br><span class="line">    <span class="comment">// Pre-calculate tile count and the total number of pixels per feature map</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> C_TILES = C / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_pixels = IN_H * IN_W;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over channel tiles</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> c_tile = <span class="number">0</span>; c_tile &lt; C_TILES; c_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Initialize a wide vector accumulator to zeros</span></span><br><span class="line">        <span class="type">actvec_t</span> sum_accumulator = <span class="type">actvec_t</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Loop over all pixels in the feature maps for the current channel tile.</span></span><br><span class="line">        <span class="comment">// The H and W dimensions are flattened for this operation.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_pixels; ++i) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Calculate pointer to Input[pixel_i][c_tile]</span></span><br><span class="line">            <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_base + i * C_TILES + c_tile;</span><br><span class="line">            <span class="comment">// Accumulate the vector values</span></span><br><span class="line">            sum_accumulator = sum_accumulator + <span class="type">actvec_t</span>(*x_ptr);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- Perform Per-Lane Division to get the Average ---</span></span><br><span class="line">        <span class="type">actvec_t</span> avg_vec = <span class="type">actvec_t</span>(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> lane = <span class="number">0</span>; lane &lt; VSIZE; ++lane) &#123;</span><br><span class="line">            <span class="comment">// Extract the sum for a single channel (lane)</span></span><br><span class="line">            <span class="type">actsca_t</span> lane_sum = act_ext(sum_accumulator, lane);</span><br><span class="line">            <span class="comment">// Perform scalar integer division to find the average</span></span><br><span class="line">            <span class="type">actsca_t</span> lane_avg = (<span class="type">int</span>)lane_sum / num_pixels;</span><br><span class="line">            <span class="comment">// Update the corresponding lane in the average vector with the result</span></span><br><span class="line">            avg_vec = act_upd(avg_vec, (<span class="type">int</span>)lane_avg, lane);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Store the resulting average vector</span></span><br><span class="line">        out_vector[c_tile] = <span class="type">vec_t</span>(avg_vec);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="fully-connected"><a href="#fully-connected" class="headerlink" title="fully_connected"></a><code>fully_connected</code></h2><p>The fully connected (or dense) layer performs a linear transformation on an input vector, typically after the feature extraction stages of a CNN. It maps the learned features to the final output classes. The operation is equivalent to a matrix-vector multiplication, where the matrix is the layer’s weights.</p>
<h3 id="Interface-Definition-4"><a href="#Interface-Definition-4" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">fully_connected_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_results, <span class="type">vec_t</span> chess_storage(VM)* in_vector,</span></span><br><span class="line"><span class="params">                                       <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base, <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec,</span></span><br><span class="line"><span class="params">                                       <span class="type">int</span> IN_FEATURES, <span class="type">int</span> OUT_FEATURES, <span class="type">int</span> mul_o, <span class="type">int</span> shf_o)</span>;</span><br></pre></td></tr></table></figure>

<h3 id="Parameters-4"><a href="#Parameters-4" class="headerlink" title="Parameters"></a>Parameters</h3><ul>
<li><code>out_results</code>: A pointer to the output vector where the final scores or logits are stored.</li>
<li><code>in_vector</code>: A pointer to the flattened input feature vector.</li>
<li><code>weight_base</code>: A pointer to the weight matrix of shape <code>[OUT_FEATURES, IN_FEATURES]</code>.</li>
<li><code>bias_vec</code>: A pointer to the bias vector of size <code>OUT_FEATURES</code>.</li>
<li><code>IN_FEATURES</code>, <code>OUT_FEATURES</code>: The number of input and output features (neurons). Both are assumed to be multiples of <code>VSIZE</code>.</li>
<li><code>mul_o</code>, <code>shf_o</code>: Scaling factor and right-shift amount for output quantization.</li>
</ul>
<h3 id="Core-Logic-4"><a href="#Core-Logic-4" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function computes the output vector by iterating through the output features in tiles of <code>VSIZE</code>. For each output tile, it initializes an accumulator with the corresponding bias values. It then performs a series of <code>mat_vec_mul_add</code> operations, effectively multiplying a slice of the weight matrix with the entire input vector. This computes one tile of the output. A final quantization step is applied before storing the result.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Fully Connected Layer ---</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">fully_connected_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_results, <span class="type">vec_t</span> chess_storage(VM)* in_vector,</span></span><br><span class="line"><span class="params">                                       <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base, <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec,</span></span><br><span class="line"><span class="params">                                       <span class="type">int</span> IN_FEATURES, <span class="type">int</span> OUT_FEATURES, <span class="type">int</span> mul_o, <span class="type">int</span> shf_o)</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> IN_TILES = IN_FEATURES / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_TILES_VEC = OUT_FEATURES / VSIZE; <span class="comment">// Number of output vector tiles</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Process the output features in chunks of VSIZE</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> out_tile = <span class="number">0</span>; out_tile &lt; OUT_TILES_VEC; ++out_tile) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Initialize accumulator with the bias for this output tile</span></span><br><span class="line">        <span class="type">actvec_t</span> act = bias_vec[out_tile];</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Pointer to the start of the weights for this output tile</span></span><br><span class="line">        <span class="type">const</span> <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* wx_ptr = weight_base + out_tile * IN_TILES;</span><br><span class="line">        <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_vector;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Loop over the input features, performing matrix-vector multiplication in a tiled fashion</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> in_tile = <span class="number">0</span>; in_tile &lt; IN_TILES; ++in_tile) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            act = mat_vec_mul_add(act, *wx_ptr, *x_ptr);</span><br><span class="line">            wx_ptr++;</span><br><span class="line">            x_ptr++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Apply final quantization to the accumulated result</span></span><br><span class="line">        act = (act * mul_o) &gt;&gt; shf_o;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Store the VSIZE scalar results</span></span><br><span class="line">        out_results[out_tile] = <span class="type">vec_t</span>(act);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://example.com">Yuchao Qin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2025/07/24/flexacc_manual/C_library4ML/">http://example.com/2025/07/24/flexacc_manual/C_library4ML/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Yuchao's Homepage</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/FlexAcc-Kernel/">FlexAcc Kernel</a></div><div class="post_share"><div class="social-share" data-image="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/07/02/asip_design_tool/asip_training_processor_modeling/" title="ASIP Designer Processor Modeling"><img class="cover" src="https://www.synopsys.com/dw/images/ds/asip-designer-flow-diagram.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">ASIP Designer Processor Modeling</div></div></a></div></nav><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div class="comment-switch"><span class="first-comment">Valine</span><span id="switch-btn"></span><span class="second-comment">Disqus</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div><div><div id="disqus_thread"></div></div><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/image/profile.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Yuchao Qin</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/worldline22"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/worldline22" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:qinyuchao22@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #D44638;"></i></a><a class="social-icon" href="/image/myweixin.png" target="_blank" title="Wechat"><i class="fab fa-weixin" style="color: #7BB32E;"></i></a><a class="social-icon" href="https://x.com/worldline22" target="_blank" title="Twitter"><i class="fab fa-twitter" style="color: #1da1f2;"></i></a><a class="social-icon" href="https://www.facebook.com/profile.php?id=61561687587707" target="_blank" title="Facebook"><i class="fab fa-facebook" style="color: #3b5998;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Happy Chinese new year! 🎉🎉🎉</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Convolutional-Operations"><span class="toc-number">1.</span> <span class="toc-text">Convolutional Operations</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#conv2d"><span class="toc-number">1.1.</span> <span class="toc-text">conv2d</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition"><span class="toc-number">1.1.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters"><span class="toc-number">1.1.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic"><span class="toc-number">1.1.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#max-pooling"><span class="toc-number">1.2.</span> <span class="toc-text">max_pooling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition-1"><span class="toc-number">1.2.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters-1"><span class="toc-number">1.2.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic-1"><span class="toc-number">1.2.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#add-2-relu"><span class="toc-number">1.3.</span> <span class="toc-text">add_2_relu</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition-2"><span class="toc-number">1.3.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters-2"><span class="toc-number">1.3.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic-2"><span class="toc-number">1.3.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#average-pooling"><span class="toc-number">1.4.</span> <span class="toc-text">average_pooling</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition-3"><span class="toc-number">1.4.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters-3"><span class="toc-number">1.4.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic-3"><span class="toc-number">1.4.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#fully-connected"><span class="toc-number">1.5.</span> <span class="toc-text">fully_connected</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Interface-Definition-4"><span class="toc-number">1.5.1.</span> <span class="toc-text">Interface Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Parameters-4"><span class="toc-number">1.5.2.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Core-Logic-4"><span class="toc-number">1.5.3.</span> <span class="toc-text">Core Logic</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2025/07/24/flexacc_manual/C_library4ML/" title="C Library for ML in FlexACC"><img src="https://miro.medium.com/v2/resize:fit:4800/format:webp/1*qpfm51RWgHcNRdOXNonW-w.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C Library for ML in FlexACC"/></a><div class="content"><a class="title" href="/2025/07/24/flexacc_manual/C_library4ML/" title="C Library for ML in FlexACC">C Library for ML in FlexACC</a><time datetime="2025-07-24T19:00:00.000Z" title="发表于 2025-07-24 12:00:00">2025-07-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/02/asip_design_tool/asip_training_processor_modeling/" title="ASIP Designer Processor Modeling"><img src="https://www.synopsys.com/dw/images/ds/asip-designer-flow-diagram.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ASIP Designer Processor Modeling"/></a><div class="content"><a class="title" href="/2025/07/02/asip_design_tool/asip_training_processor_modeling/" title="ASIP Designer Processor Modeling">ASIP Designer Processor Modeling</a><time datetime="2025-07-03T00:00:00.000Z" title="发表于 2025-07-02 17:00:00">2025-07-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/07/01/asip_design_tool/Introduction_to_asip_design/" title="Overview of ASIP design tool"><img src="https://www.synopsys.com/dw/images/ds/asip-designer-flow-diagram.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Overview of ASIP design tool"/></a><div class="content"><a class="title" href="/2025/07/01/asip_design_tool/Introduction_to_asip_design/" title="Overview of ASIP design tool">Overview of ASIP design tool</a><time datetime="2025-07-02T00:20:00.000Z" title="发表于 2025-07-01 17:20:00">2025-07-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/02/10/aboutMe/" title="Biography"><img src="https://worldline22.github.io/image/pinCover.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Biography"/></a><div class="content"><a class="title" href="/2025/02/10/aboutMe/" title="Biography">Biography</a><time datetime="2025-02-10T20:00:00.000Z" title="发表于 2025-02-10 12:00:00">2025-02-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/07/28/preinfo/" title="写在前面（Preface）"><img src="https://th.bing.com/th/id/R.e69674a9db6acac7483d6a69111df212?rik=zYh7aAR6872GFQ&amp;riu=http%3a%2f%2fn.sinaimg.cn%2fsinacn00%2f605%2fw640h765%2f20180731%2fe839-hhacrcc8311426.jpg&amp;ehk=xCz4PpDP2t62S%2bC30z%2fg0fOXYWHne%2bKz7ly6cmS09Kc%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="写在前面（Preface）"/></a><div class="content"><a class="title" href="/2024/07/28/preinfo/" title="写在前面（Preface）">写在前面（Preface）</a><time datetime="2024-07-29T00:28:35.000Z" title="发表于 2024-07-28 17:28:35">2024-07-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2024 - 2025 By Yuchao Qin</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a target="_blank" rel="noopener" href="https://worldline22.github.io">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>(() => {
  const initValine = () => {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  const loadValine = async () => {
    if (typeof Valine === 'function') initValine()
    else {
      await getScript('https://cdn.jsdelivr.net/npm/valine@1.5.1/dist/Valine.min.js')
      initValine()
    }
  }

  if ('Valine' === 'Valine' || !false) {
    if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
    else setTimeout(loadValine, 0)
  } else {
    window.loadOtherComment = loadValine
  }
})()</script><script>(() => {
  const disqus_config = function () {
    this.page.url = 'http://example.com/2025/07/24/flexacc_manual/C_library4ML/'
    this.page.identifier = '/2025/07/24/flexacc_manual/C_library4ML/'
    this.page.title = 'C Library for ML in FlexACC'
  }

  const disqusReset = () => {
    window.DISQUS && window.DISQUS.reset({
      reload: true,
      config: disqus_config
    })
  }

  btf.addGlobalFn('themeChange', disqusReset, 'disqus')

  const loadDisqus = () =>{
    if (window.DISQUS) disqusReset()
    else {
      const script = document.createElement('script')
      script.src = 'https://.disqus.com/embed.js'
      script.setAttribute('data-timestamp', +new Date())
      document.head.appendChild(script)
    }
  }

  const getCount = async() => {
    try {
      const eleGroup = document.querySelector('#post-meta .disqus-comment-count')
      if (!eleGroup) return
      const cleanedLinks = eleGroup.href.replace(/#post-comment$/, '')

      const res = await fetch(`https://disqus.com/api/3.0/threads/set.json?forum=&api_key=&thread:link=${cleanedLinks}`,{
        method: 'GET'
      })
      const result = await res.json()

      const count = result.response.length ? result.response[0].posts : 0
      eleGroup.textContent = count
    } catch (err) {
      console.error(err)
    }
  }

  if ('Valine' === 'Disqus' || !false) {
    if (false) btf.loadComment(document.getElementById('disqus_thread'), loadDisqus)
    else {
      loadDisqus()
      
    }
  } else {
    window.loadOtherComment = loadDisqus
  }
})()</script><script>(() => {
  const initGitalk = () => {
    const gitalk = new Gitalk(Object.assign({
      clientID: 'Ov23liX4NCpV2ZdkGR2S',
      clientSecret: '0704901937de27f01f6057bd2c6c7992f40346a6',
      repo: 'worldline22.github.io.gitalk',
      owner: 'Yuchao Qin',
      admin: ['Yuchao Qin'],
      id: 'c82bbe820f9d33f075ec0deffb220a23',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  const loadGitalk = async() => {
    if (typeof Gitalk === 'function') initGitalk()
    else {
      await getCSS('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.css')
      await getScript('https://cdn.jsdelivr.net/npm/gitalk@1.8.0/dist/gitalk.min.js')
      initGitalk()
    }
  }
  
  const commentCount = n => {
    const isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
    if (isCommentCount) {
      isCommentCount.textContent= n
    }
  }

  if ('Valine' === 'Gitalk' || !false) {
    if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
    else loadGitalk()
  } else {
    window.loadOtherComment = loadGitalk
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/canvas-fluttering-ribbon.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>