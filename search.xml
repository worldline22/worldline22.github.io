<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>RAS - A Bit-Exact rANS Accelerator for High-Performance Neural Lossless Compression</title>
      <link href="/2025/10/20/ras/ras_readme/"/>
      <url>/2025/10/20/ras/ras_readme/</url>
      
        <content type="html"><![CDATA[<h1 id="RAS-A-Bit-Exact-rANS-Accelerator-for-High-Performance-Neural-Lossless-Compression"><a href="#RAS-A-Bit-Exact-rANS-Accelerator-for-High-Performance-Neural-Lossless-Compression" class="headerlink" title="RAS: A Bit-Exact rANS Accelerator for High-Performance Neural Lossless Compression"></a>RAS: A Bit-Exact rANS Accelerator for High-Performance Neural Lossless Compression</h1><p><img src="/../../image/ras/ras_architecture.png" alt="RAS Architecture Overview"></p><p><em>Figure 1: Overall RAS architecture.</em></p><p>Data centers process massive data streams that demand fast, bit-exact lossless compression. <strong>RAS</strong>—the <strong>R</strong>ange Asymmetric Numeral System <strong>A</strong>cceleration <strong>S</strong>ystem—integrates an rANS core with a learned probability generator, storing distributions in <strong>BF16</strong> and converting them once to a shared fixed-point domain. A unified division&#x2F;modulo datapath and a <strong>two-stage rANS update</strong> with <strong>byte-level re-normalization</strong> cut logic and memory traffic, while a <strong>prediction-guided decoding</strong> path speculatively narrows the CDF search window and safely falls back to preserve exactness. A simple multi-lane organization scales throughput and enables fine-grained clock gating. In simulation on image workloads, RAS achieves <strong>121.2×</strong> encode and <strong>70.9×</strong> decode speedups over a Python rANS baseline, and reduces average decoder binary-search steps from <strong>7.00</strong> to <strong>3.15</strong> (~<strong>55%</strong> fewer), matching or exceeding classical codecs when paired with neural probability models. </p><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>The encoder updates a single rANS state per symbol using shared mixed-precision probability tables: BF16 distributions are converted once by the Streaming Prefetch Converter (SPC) into fixed-point CDF&#x2F;frequency tables, then a two-stage update computes quotient and remainder in parallel—$\big\lfloor s_{i-1}&#x2F;f(x_i)\big\rfloor \cdot R$ and $(s_{i-1}\bmod f(x_i)) + C(x_i)$—before summation and byte-level re-normalization. This unified div&#x2F;mod datapath, deterministic rounding, and stationary dataflow preserve bit-exactness while sustaining near one-symbol-per-cycle steady-state throughput and reducing off-chip bandwidth. A canonical algebraic form is:<br>$$<br>s_i ;&#x3D;; \Big\lfloor \tfrac{s_{i-1}}{f(x_i)} \Big\rfloor \cdot R ;+; \big( s_{i-1} \bmod f(x_i) \big) ;+; C(x_i),<br>$$<br>followed by re-normalization to keep (s \in [L,,R!L)). </p><p><strong>Encoder layout</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rANS_encoder/</span><br><span class="line">├─ data/</span><br><span class="line">├─ src/</span><br><span class="line">├─ testbench/</span><br><span class="line">├─ waveform_tb/</span><br><span class="line">├─ makefile</span><br><span class="line">└─ <span class="keyword">debug</span>.<span class="keyword">log</span></span><br></pre></td></tr></table></figure><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>The decoder inverts the rANS state back to symbols using the same fixed-point tables, accelerating the usual CDF binary search with a <strong>speculate-and-verify</strong> path: a lightweight predictor proposes a trial symbol within a window $[\mu-\Delta,\mu+\Delta]$. On a match, it commits immediately; on a mismatch, it restores and falls back to the baseline search—preserving the bitstream while cutting average search depth (from $7.00$ to $3.15$ steps in simulation). This reduces table probes and memory traffic without changing coder invariants. </p><p><strong>Decoder layout</strong></p><figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">rANS_decoder/</span><br><span class="line">├─ data/</span><br><span class="line">├─ <span class="keyword">log</span>/</span><br><span class="line">├─ src/</span><br><span class="line">├─ testbench/</span><br><span class="line">├─ waveform_tb/</span><br><span class="line">├─ makefile</span><br><span class="line">└─ <span class="keyword">debug</span>.<span class="keyword">log</span></span><br></pre></td></tr></table></figure><h2 id="Python-Reference-Implementations"><a href="#Python-Reference-Implementations" class="headerlink" title="Python Reference Implementations"></a>Python Reference Implementations</h2><p>A Python rANS encoder&#x2F;decoder pair validates the hardware path and provides streaming test vectors with mixed-precision tables and two-stage updates:</p><ul><li><code>Python_version/rans_encode_stream.py</code></li><li><code>Python_version/rans_decode_stream.py</code></li></ul><p>Use these scripts to generate golden bitstreams for RTL verification and to benchmark against the software baseline.</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p><img src="/../../image/ras/results.png" alt="RAS Performance Comparison"></p><p><em>Figure 2: Simulated design exploration results.</em></p><p>Figure 2 summarizes the simulated design exploration: <strong>(a)</strong> cycle-normalized compute cost (cycles&#x2F;run; lower is better) for compression and decompression shows RAS outperforming a Python rANS baseline with speedups of $121.2\times$ for encoding and $70.9\times$ for decoding; <strong>(b)</strong> prediction-guided decoding reduces the average CDF binary-search depth from $7.00$ to $3.15$ (approximately $55%$ fewer steps); <strong>(c)</strong> across ImageNet32&#x2F;64 and CIFAR-10, neural rANS-based models (e.g., IDF, PiMC) achieve higher compression ratios than classical codecs (JPEG2000, WebP, PNG, Zstd).</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li>Qin, Y., Fan, A., Yan, B. <strong>“RAS: A Bit-Exact rANS Accelerator for High-Performance Neural Lossless Compression.”</strong> [<a href="https://worldline22.github.io/pdf/RAS.pdf">here</a>.]</li></ul>]]></content>
      
      
      <categories>
          
          <category> Computer Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fix-datapath accelerator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2025/08/20/test/"/>
      <url>/2025/08/20/test/</url>
      
        <content type="html"><![CDATA[<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">opn <span class="title">tnlp</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    sc: bit32_ifmt,     vc: slot_vc,     <span class="comment">// scalar, vector</span></span></span></span><br><span class="line"><span class="params"><span class="function">    vm: slot_vm,        wm: slot_wm)</span>     <span class="comment">// VM ldst, WM ldst</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  action &#123; sc; vc; vm; wm; &#125;</span><br><span class="line">  syntax : arbitrary_order sc <span class="string">&quot; | &quot;</span> vc <span class="string">&quot; | &quot;</span> vm <span class="string">&quot; | &quot;</span> wm;</span><br><span class="line">  image  : sc::vc::vm::wm;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>| Custom Opcode (1011011) | WM_addr (rs2) | DRAM_addr (rs1) | funct3 | xxxxx |</code></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SIMT vs. SIMD - Deconstructing Parallel Architectures</title>
      <link href="/2025/08/07/learning_note/SIMD&amp;SIMT/"/>
      <url>/2025/08/07/learning_note/SIMD&amp;SIMT/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>The relentless pursuit of computational power has led to the dominance of parallel computing architectures. Among these, Single Instruction, Multiple Threads (SIMT) has emerged as the cornerstone of modern General-Purpose Graphics Processing Units (GPGPUs), offering a powerful blend of performance and programmability. This report delves into the SIMT paradigm, contrasting it with the more traditional Single Instruction, Multiple Data (SIMD) model to highlight its architectural nuances and superior flexibility.</p><h1 id="The-Essence-of-SIMT"><a href="#The-Essence-of-SIMT" class="headerlink" title="The Essence of SIMT"></a>The Essence of SIMT</h1><p>The SIMT architecture represents a significant evolution in parallel computing, enabling the massive parallelization that defines modern GPUs. At its core, SIMT allows a single instruction to be executed by a multitude of threads concurrently. While groups of threads (organized into “warps” or “wavefronts”) execute in lockstep, the architecture’s true power lies in its ability to allow individual threads to make independent decisions, a feature that dramatically enhances the programmability of the GPU. We will explore SIMT from both a hardware and a software (CUDA) perspective.</p><h2 id="A-Hardware-Perspective-The-Hierarchy-of-Execution"><a href="#A-Hardware-Perspective-The-Hierarchy-of-Execution" class="headerlink" title="A Hardware Perspective: The Hierarchy of Execution"></a>A Hardware Perspective: The Hierarchy of Execution</h2><p>From a hardware viewpoint, the SIMT model is structured in a clear hierarchy:</p><ul><li><p><strong>CUDA Core (The Thread Processor)</strong>: At the lowest level is the individual processing unit, often called a CUDA core or Streaming Processor (SP). This is the fundamental execution unit that maps to a single thread in the software model. It has its own program counter and registers, enabling a degree of independent operation.</p></li><li><p><strong>Warp (The Core Group)</strong>: A group of threads, typically 32 in NVIDIA’s architecture, is bundled into a “warp.” The threads within a warp are dispatched together and execute the same instruction at any given time. This grouped execution is the “Single Instruction” part of SIMT. If all threads in a warp follow the same execution path, SIMT behaves much like a traditional SIMD system, achieving maximum hardware utilization.</p></li><li><p><strong>Streaming MultStreaming Multiprocessor (SM)</strong>: An SM is a larger unit composed of multiple CUDA cores, shared memory, and a warp scheduler. The scheduler’s role is crucial: it selects which warps are ready to execute and issues instructions to them. By managing and interleaving multiple warps, the SM can hide memory latency and keep the execution cores busy, maximizing overall throughput.</p></li></ul><p><img src="https://stevengong.co/attachments/Pasted-image-20230915100702.png" alt="A diagram illustrating the relationship between software constructs (Grid, Block, Thread) and hardware units (Device, SM, Core)."></p><h2 id="A-CUDA-Perspective-Programmable-Thread-Level-Control"><a href="#A-CUDA-Perspective-Programmable-Thread-Level-Control" class="headerlink" title="A CUDA Perspective: Programmable Thread-Level Control"></a>A CUDA Perspective: Programmable Thread-Level Control</h2><p>The programmability of the SIMT architecture is best illustrated through NVIDIA’s CUDA platform. Let’s examine a simple CUDA kernel that performs a conditional vector addition. This example highlights how individual threads can take different actions based on the data they are processing.</p><p>The task is to add elements of two vectors, A and B, into a vector C, but only if the corresponding element in A exceeds a given threshold.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">conditional_vector_add</span><span class="params">(<span class="type">float</span> *A, <span class="type">float</span> *B, <span class="type">float</span> *C, <span class="type">int</span> n, <span class="type">float</span> threshold)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Each thread calculates its unique global index</span></span><br><span class="line">    <span class="type">int</span> i = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i &lt; n) &#123;</span><br><span class="line">        <span class="comment">// Each thread independently evaluates the condition based on its data A[i]</span></span><br><span class="line">        <span class="keyword">if</span> (A[i] &gt; threshold) &#123;</span><br><span class="line">            <span class="comment">// This path is taken only by threads whose data meets the criterion</span></span><br><span class="line">            C[i] = A[i] + B[i];</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// This alternate path is taken by the remaining threads</span></span><br><span class="line">            C[i] = A[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The critical element here is the if (A[i] &gt; threshold) statement. Each thread in a warp evaluates this condition for its assigned element A[i]. This allows for fine-grained, data-dependent control flow, a hallmark of SIMT’s flexibility.</p><h1 id="SIMT-vs-SIMD-A-Tale-of-Two-Paradigms"><a href="#SIMT-vs-SIMD-A-Tale-of-Two-Paradigms" class="headerlink" title="SIMT vs. SIMD: A Tale of Two Paradigms"></a>SIMT vs. SIMD: A Tale of Two Paradigms</h1><p>The distinction between SIMT and SIMD is fundamental to understanding modern parallel processors.</p><h2 id="SIMD-Rigid-Vectorization"><a href="#SIMD-Rigid-Vectorization" class="headerlink" title="SIMD: Rigid Vectorization"></a>SIMD: Rigid Vectorization</h2><p>In a traditional SIMD architecture, a single instruction operates on a vector of data. This is highly efficient for uniform, predictable workloads like image processing or simple linear algebra. However, handling control flow is cumbersome. To implement our conditional addition, a SIMD system would typically use masking:</p><ul><li><p><strong>Compare</strong>: An instruction compares all elements of vector A against the threshold, generating a bitmask (e.g., 1 for true, 0 for false).</p></li><li><p><strong>Masked Add</strong>: The addition A + B is performed, but the results are only written to memory locations where the corresponding bit in the mask is 1.</p></li><li><p><strong>Masked Move</strong>: A separate instruction might be needed to write A to the remaining locations.</p></li></ul><p>This process is less intuitive and forces the programmer to think in terms of vector-wide operations and masks rather than simple thread-level logic. The hardware itself lacks the inherent flexibility to manage divergent paths automatically.</p><h2 id="SIMT-Flexible-Thread-Level-Logic"><a href="#SIMT-Flexible-Thread-Level-Logic" class="headerlink" title="SIMT: Flexible Thread-Level Logic"></a>SIMT: Flexible Thread-Level Logic</h2><p>The SIMT architecture abstracts away this complexity. While the underlying hardware (the warp) still executes a single instruction at a time, it intelligently handles divergent paths. When the threads in a warp disagree on the outcome of the if statement (an event known as thread divergence), the hardware scheduler handles it gracefully:</p><ul><li><p><strong>DiverDivergence</strong>: The warp temporarily splits.</p></li><li><p><strong>Serialized Execution</strong>: All threads that took the if path execute their instructions while the else threads are temporarily paused (masked off by the hardware).</p></li><li><p><strong>Path Inversion</strong>: The scheduler then pauses the if threads and executes the instructions for the else path.</p></li><li><p><strong>Reconvergence</strong>: Once both paths are complete, all threads in the warp reconverge to the same execution stream.</p></li></ul><p>Although this serialization introduces a performance penalty (as not all cores in the warp are doing useful work simultaneously), it vastly simplifies programming. The developer can write straightforward, intuitive code as if programming for a single thread, and the hardware automatically manages the underlying complexity of parallel execution.</p><p>Key Differences Summarized</p><table><thead><tr><th align="left">Feature</th><th align="left">SIMD</th><th align="left">SIMT</th></tr></thead><tbody><tr><td align="left"><strong>Programming Model</strong></td><td align="left">Explicit vectorization; programmer manages data parallelism directly.</td><td align="left">Scalar programming for a single thread; hardware manages thread parallelism.</td></tr><tr><td align="left"><strong>Control Flow</strong></td><td align="left">Handled via explicit masking; cumbersome for complex, data-driven branches.</td><td align="left">Handled via standard <code>if/else</code>, <code>while</code> loops; hardware manages thread divergence.</td></tr><tr><td align="left"><strong>Flexibility</strong></td><td align="left">Best for highly regular, predictable data. Struggles with irregular workloads.</td><td align="left">Highly flexible; excels at complex algorithms with data-dependent branching.</td></tr><tr><td align="left"><strong>Hardware Abstraction</strong></td><td align="left">Low-level; programmer is aware of vector lanes and masks.</td><td align="left">High-level; hardware abstracts the management of thread groups (warps).</td></tr></tbody></table><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>The SIMT architecture is a masterful compromise, combining the raw throughput of a data-parallel model like SIMD with the programmability and flexibility of a thread-parallel model. By providing an intuitive programming model where developers can write simple scalar code with complex control flow, CUDA and the underlying SIMT hardware unlock the immense power of the GPU for a wide range of general-purpose computing tasks. While SIMD remains effective for specialized, highly regular problems, the ability of SIMT to automatically handle thread divergence and complex logic has cemented its role as the dominant architecture in the world of high-performance parallel computing.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Parallel Computing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C Library for ML in FlexACC</title>
      <link href="/2025/07/25/flexacc_manual/C_library4ML/"/>
      <url>/2025/07/25/flexacc_manual/C_library4ML/</url>
      
        <content type="html"><![CDATA[<div style="border: 2px solid #ff9999; padding: 10px; margin-top: 20px; background-color: #ffe6e6; border-radius: 10px; box-shadow: 2px 2px 12px #aaaaaa;">    <p style="font-size: 18px; color: #ab47bc;"><strong>📌README</strong></p>    <p style="font-size: 16px; color: #000000;">This document details a set of core neural network operations implemented in C, specifically optimized for a Single Instruction, Multiple Data (SIMD) architecture. To achieve high computational throughput, all data structures such as inputs, outputs, and weights are stored in memory using vector types (e.g., actvec_t, vec_t, mat_t). These functions form the building blocks for constructing efficient convolutional neural networks and transformer architecture.    </p></div><h1 id="Convolutional-Operations"><a href="#Convolutional-Operations" class="headerlink" title="Convolutional Operations"></a>Convolutional Operations</h1><h2 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a><code>conv2d</code></h2><p>The 2D convolution is the cornerstone of modern computer vision models. This function performs a 2D convolution on an input tensor, applying a set of learnable filters (weights) to produce an output feature map. It also includes fused operations for adding a bias, applying quantization, and executing a ReLU activation function, which minimizes memory access and improves performance.</p><h3 id="Interface-Definition"><a href="#Interface-Definition" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">conv2d_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base,</span></span><br><span class="line"><span class="params">                              <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec, <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> IN_C,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> OUT_C, <span class="type">int</span> K, <span class="type">int</span> STRIDE, <span class="type">int</span> PAD,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> mul_o, <span class="type">int</span> shf_o, <span class="type">int</span> apply_relu)</span>;</span><br></pre></td></tr></table></figure><h3 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li><code>out_base</code>: A pointer to the base address in vector memory (<code>VM</code>) where the output tensor will be stored.</li><li><code>in_base</code>: A pointer to the base address in vector memory (<code>VM</code>) of the input tensor.</li><li><code>weight_base</code>: A pointer to the base address in weight memory (<code>WM</code>) of the convolution kernels (filters).</li><li><code>bias_vec</code>: A pointer to the base address in weight memory (<code>WM</code>) for the bias vector. Each element is added to a corresponding output channel.</li><li><code>IN_H</code>, <code>IN_W</code>, <code>IN_C</code>: The height, width, and number of channels of the input tensor. <code>IN_C</code> is assumed to be a multiple of the vector size <code>VSIZE</code>.</li><li><code>OUT_C</code>: The number of output channels, which corresponds to the number of convolutional filters. <code>OUT_C</code> is also assumed to be a multiple of <code>VSIZE</code>.</li><li><code>K</code>: The size (height and width) of the square convolutional kernel.</li><li><code>STRIDE</code>: The step size, or stride, for moving the convolutional window across the input tensor.</li><li><code>PAD</code>: The number of zero-value pixels to pad around the border of the input tensor. This allows control over the output spatial dimensions.</li><li><code>mul_o</code>, <code>shf_o</code>: Scaling factor and right-shift amount for output quantization. This is a common technique in integer-only inference to approximate floating-point division.</li><li><code>apply_relu</code>: A flag (1 or 0) that determines whether to apply a Rectified Linear Unit (ReLU) activation function to the output.</li></ul><h3 id="Core-Logic"><a href="#Core-Logic" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function calculates the output dimensions and then iterates through each spatial location <code>(oh, ow)</code> of the output tensor. For each location, it computes the value for all output channels in a tiled manner.</p><p>$$<br>\text{OUT_H} &#x3D; \frac{(\text{IN_H} - K + 2 \times \text{PAD})}{\text{STRIDE}} + 1<br>$$<br>$$<br>\text{OUT_W} &#x3D; \frac{(\text{IN_W} - K + 2 \times \text{PAD})}{\text{STRIDE}} + 1<br>$$</p><p>The core computation is a multiply-accumulate operation performed on vector data types. The loops are structured to maximize data locality and enable efficient SIMD processing.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- 2D convolution with Fused ReLU ---</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">conv2d_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base,</span></span><br><span class="line"><span class="params">                              <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec, <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> IN_C,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> OUT_C, <span class="type">int</span> K, <span class="type">int</span> STRIDE, <span class="type">int</span> PAD,</span></span><br><span class="line"><span class="params">                              <span class="type">int</span> mul_o, <span class="type">int</span> shf_o, <span class="type">int</span> apply_relu)</span> &#123;</span><br><span class="line">    <span class="comment">// Calculate output tensor dimensions</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_H = (IN_H - K + <span class="number">2</span> * PAD) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_W = (IN_W - K + <span class="number">2</span> * PAD) / STRIDE + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Calculate the number of vector tiles for input and output channels</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> IN_C_TILES = IN_C / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_C_TILES = OUT_C / VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over each spatial location in the output tensor</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> oh = <span class="number">0</span>; oh &lt; OUT_H; oh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ow = <span class="number">0</span>; ow &lt; OUT_W; ow++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Iterate over the output channels in chunks of VSIZE</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> oc_tile = <span class="number">0</span>; oc_tile &lt; OUT_C_TILES; oc_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="comment">// Initialize accumulator with the bias for the current output channel tile</span></span><br><span class="line">                <span class="type">actvec_t</span> act = bias_vec[oc_tile];</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Calculate the corresponding input window, considering padding</span></span><br><span class="line">                <span class="type">int</span> ih_start = -PAD + oh * STRIDE;</span><br><span class="line">                <span class="type">int</span> iw_start = -PAD + ow * STRIDE;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Determine the valid kernel region to apply, clamping to input boundaries</span></span><br><span class="line">                <span class="type">int</span> wh_start = max_(<span class="number">0</span>, PAD - oh * STRIDE);</span><br><span class="line">                <span class="type">int</span> ww_start = max_(<span class="number">0</span>, PAD - ow * STRIDE);</span><br><span class="line">                <span class="type">int</span> wh_end   = min_(K, IN_H - ih_start);</span><br><span class="line">                <span class="type">int</span> ww_end   = min_(K, IN_W - iw_start);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Slide the kernel over the input window</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> wh = wh_start; wh &lt; wh_end; wh++) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> ww = ww_start; ww &lt; ww_end; ww++) &#123;</span><br><span class="line">                        <span class="comment">// Pointer to weights for this kernel position and output channel tile</span></span><br><span class="line">                        <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* wx_ptr = (<span class="type">mat_t</span> chess_storage(WM)*)weight_base + oc_tile * (K * K * IN_C_TILES) + wh * (K * IN_C_TILES) + ww * (IN_C_TILES);</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Pointer to the corresponding input data</span></span><br><span class="line">                        <span class="type">int</span> ih = ih_start + wh;</span><br><span class="line">                        <span class="type">int</span> iw = iw_start + ww;</span><br><span class="line">                        <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = (<span class="type">vec_t</span> chess_storage(VM)*)in_base + ih * (IN_W * IN_C_TILES) + iw * (IN_C_TILES);</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Perform multiply-accumulate across all input channel tiles</span></span><br><span class="line">                        <span class="keyword">for</span> (<span class="type">int</span> ic_tile = <span class="number">0</span>; ic_tile &lt; IN_C_TILES; ic_tile++) &#123;</span><br><span class="line">                            act = mat_vec_mul_add(act, *wx_ptr, *x_ptr);</span><br><span class="line">                            wx_ptr++;</span><br><span class="line">                            x_ptr++;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Quantization and Activation ---</span></span><br><span class="line">                <span class="comment">// Apply post-convolution quantization</span></span><br><span class="line">                act = (act * mul_o) &gt;&gt; shf_o;</span><br><span class="line">                </span><br><span class="line">                <span class="comment">// Apply ReLU activation if enabled</span></span><br><span class="line">                <span class="keyword">if</span> (apply_relu) &#123;</span><br><span class="line">                    act = max_(act, <span class="type">actvec_t</span>(<span class="number">0</span>));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Store Output ---</span></span><br><span class="line">                <span class="comment">// Calculate the output address and store the result</span></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* o_ptr = (<span class="type">vec_t</span> chess_storage(VM)*)out_base + oh * (OUT_W * OUT_C_TILES) + ow * (OUT_C_TILES) + oc_tile;</span><br><span class="line">                *o_ptr = <span class="type">vec_t</span>(act);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="max-pooling"><a href="#max-pooling" class="headerlink" title="max_pooling"></a><code>max_pooling</code></h2><p>Max pooling is a down-sampling operation that reduces the spatial dimensions (height and width) of a feature map. It works by sliding a window over the input and selecting the maximum value within that window. This helps to make the feature representation more robust to small translations in the input image.</p><h3 id="Interface-Definition-1"><a href="#Interface-Definition-1" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">max_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, </span></span><br><span class="line"><span class="params">                                <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C,</span></span><br><span class="line"><span class="params">                                <span class="type">int</span> K, <span class="type">int</span> STRIDE)</span>;</span><br></pre></td></tr></table></figure><h3 id="Parameters-1"><a href="#Parameters-1" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li><code>out_base</code>: A pointer to the base address for the down-sampled output tensor.</li><li><code>in_base</code>: A pointer to the base address of the input tensor.</li><li><code>IN_H</code>, <code>IN_W</code>, <code>C</code>: The height, width, and number of channels of the input tensor. <code>C</code> is assumed to be a multiple of <code>VSIZE</code>.</li><li><code>K</code>: The size (height and width) of the square pooling window.</li><li><code>STRIDE</code>: The step size for moving the pooling window across the input.</li></ul><h3 id="Core-Logic-1"><a href="#Core-Logic-1" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function computes the output dimensions based on the input size, kernel size, and stride. It then iterates over each position of the output grid and each channel tile. For each output value, it scans the corresponding K×KK \times K window in the input tensor and finds the maximum value vector-wise.</p><p>$$<br>\text{OUT_H} &#x3D; \frac{(\text{IN_H} - K)}{\text{STRIDE}} + 1<br>$$<br>$$<br>\text{OUT_W} &#x3D; \frac{(\text{IN_W} - K)}{\text{STRIDE}} + 1<br>$$</p><p>The accumulator vector <code>max_accumulator_act</code> is initialized with the minimum possible value to ensure that the first element from the input window is always selected as the initial maximum.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">max_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_base, <span class="type">vec_t</span> chess_storage(VM)* in_base, </span></span><br><span class="line"><span class="params">                                <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C,</span></span><br><span class="line"><span class="params">                                <span class="type">int</span> K, <span class="type">int</span> STRIDE)</span> &#123;</span><br><span class="line">    <span class="comment">// Calculate output dimensions</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_H = (IN_H - K) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_W = (IN_W - K) / STRIDE + <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// Pre-calculate channel tile count</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> C_TILES = C / VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over the output spatial grid</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> oh = <span class="number">0</span>; oh &lt; OUT_H; oh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ow = <span class="number">0</span>; ow &lt; OUT_W; ow++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Iterate over channel tiles</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> c_tile = <span class="number">0</span>; c_tile &lt; C_TILES; c_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="comment">// Initialize a vector with the minimum possible scalar value (-128 for int8).</span></span><br><span class="line">                <span class="comment">// This ensures any real value from the input will be greater.</span></span><br><span class="line">                <span class="type">actvec_t</span> max_accumulator_act = <span class="number">-128</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Iterate over the pooling window (kernel)</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> kh = <span class="number">0</span>; kh &lt; K; kh++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                    <span class="keyword">for</span> (<span class="type">int</span> kw = <span class="number">0</span>; kw &lt; K; kw++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                        <span class="comment">// Calculate input coordinates</span></span><br><span class="line">                        <span class="type">int</span> ih = oh * STRIDE + kh;</span><br><span class="line">                        <span class="type">int</span> iw = ow * STRIDE + kw;</span><br><span class="line"></span><br><span class="line">                        <span class="comment">// Pointer to Input[ih][iw][c_tile]</span></span><br><span class="line">                        <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_base +</span><br><span class="line">                                                 ih * (IN_W * C_TILES) +</span><br><span class="line">                                                        iw * (C_TILES) +</span><br><span class="line">                                                                 c_tile;</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment">// Load the input vector and promote it to a wide accumulator type</span></span><br><span class="line">                        <span class="type">actvec_t</span> current_val_act = (<span class="type">actvec_t</span>)* x_ptr;</span><br><span class="line">                        <span class="comment">// Perform element-wise vector maximum</span></span><br><span class="line">                        max_accumulator_act = max_(max_accumulator_act, current_val_act);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// --- Store Output ---</span></span><br><span class="line">                <span class="comment">// Calculate output address: &amp;Output[oh][ow][c_tile]</span></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* o_ptr = out_base +</span><br><span class="line">                                   oh * (OUT_W * C_TILES) +</span><br><span class="line">                                           ow * (C_TILES) +</span><br><span class="line">                                                    c_tile;</span><br><span class="line">                <span class="comment">// Convert the final wide max vector back to a narrow storage vector and store</span></span><br><span class="line">                *o_ptr = (<span class="type">vec_t</span>)max_accumulator_act;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="add-2-relu"><a href="#add-2-relu" class="headerlink" title="add_2_relu"></a><code>add_2_relu</code></h2><p>This operation is fundamental to residual network architectures (e.g., ResNet). It implements an element-wise addition of two tensors (a “main path” and a “shortcut path”) followed by a ReLU activation. This “shortcut connection” allows the gradient to flow more directly through the network, enabling the training of much deeper models.</p><h3 id="Interface-Definition-2"><a href="#Interface-Definition-2" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">add_and_relu_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_data, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* main_path,</span></span><br><span class="line"><span class="params">                                    <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* shortcut_path, <span class="type">int</span> num_vectors)</span>;</span><br></pre></td></tr></table></figure><h3 id="Parameters-2"><a href="#Parameters-2" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li><code>out_data</code>: A pointer to the memory where the output vector will be stored.</li><li><code>main_path</code>: A pointer to the input tensor from the main computational path (e.g., the output of a convolutional block).</li><li><code>shortcut_path</code>: A pointer to the input tensor from the shortcut or identity path.</li><li><code>num_vectors</code>: The total number of vectors in the input tensors. This is equivalent to <code>(H * W * C) / VSIZE</code>.</li></ul><h3 id="Core-Logic-2"><a href="#Core-Logic-2" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function iterates through the input tensors one vector at a time. In each iteration, it loads one vector from the <code>main_path</code> and one from the <code>shortcut_path</code>, adds them together, and then applies the ReLU activation <code>max(0, sum)</code>. The result is stored in the output buffer.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Add and ReLU ---</span></span><br><span class="line"><span class="comment">// This function performs element-wise addition of two tensors and applies a ReLU activation.</span></span><br><span class="line"><span class="comment">// It is a key component of residual blocks in architectures like ResNet.</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">add_and_relu_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_data, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* main_path,</span></span><br><span class="line"><span class="params">                                    <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* shortcut_path, <span class="type">int</span> num_vectors)</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; ++i) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Load both vectors and promote to wide accumulator type for addition</span></span><br><span class="line">        <span class="type">actvec_t</span> main_act = <span class="type">actvec_t</span>(main_path[i]);</span><br><span class="line">        <span class="type">actvec_t</span> shortcut_act = <span class="type">actvec_t</span>(shortcut_path[i]);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Perform the element-wise addition</span></span><br><span class="line">        <span class="type">actvec_t</span> sum = main_act + shortcut_act;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Apply ReLU activation: result = max(0, sum)</span></span><br><span class="line">        <span class="type">actvec_t</span> result = max_(sum, <span class="type">actvec_t</span>(<span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Store the final result</span></span><br><span class="line">        out_data[i] = <span class="type">vec_t</span>(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="average-pooling"><a href="#average-pooling" class="headerlink" title="average_pooling"></a><code>average_pooling</code></h2><p>Global Average Pooling (GAP) is another down-sampling technique that reduces the entire spatial dimensions of each feature map into a single value, calculated as the average of all values in that map. It is often used in modern CNNs just before the final fully connected layer to reduce the number of parameters and prevent overfitting.</p><h3 id="Interface-Definition-3"><a href="#Interface-Definition-3" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">global_average_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_vector, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* in_base,</span></span><br><span class="line"><span class="params">                                           <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C)</span>;</span><br></pre></td></tr></table></figure><h3 id="Parameters-3"><a href="#Parameters-3" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li><code>out_vector</code>: A pointer to the output vector of size <code>C</code>.</li><li><code>in_base</code>: A pointer to the input tensor.</li><li><code>IN_H</code>, <code>IN_W</code>, <code>C</code>: The height, width, and number of channels of the input tensor. <code>C</code> is assumed to be a multiple of <code>VSIZE</code>.</li></ul><h3 id="Core-Logic-3"><a href="#Core-Logic-3" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function first calculates the total number of spatial elements, <code>num_pixels = IN_H * IN_W</code>. It then iterates through each channel tile. For each tile, it accumulates the sum of all vectors across the spatial dimensions. Finally, it performs a per-lane division of the accumulated sum by <code>num_pixels</code> to compute the average. This lane-wise division is necessary because each lane in the vector corresponds to a different channel.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Global Average Pooling ---</span></span><br><span class="line"><span class="comment">// This function computes the average of each channel across all spatial dimensions (H x W).</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">global_average_pool_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_vector, <span class="type">const</span> <span class="type">vec_t</span> chess_storage(VM)* in_base,</span></span><br><span class="line"><span class="params">                                           <span class="type">int</span> IN_H, <span class="type">int</span> IN_W, <span class="type">int</span> C)</span> &#123;</span><br><span class="line">    <span class="comment">// Pre-calculate tile count and the total number of pixels per feature map</span></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> C_TILES = C / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_pixels = IN_H * IN_W;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over channel tiles</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> c_tile = <span class="number">0</span>; c_tile &lt; C_TILES; c_tile++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Initialize a wide vector accumulator to zeros</span></span><br><span class="line">        <span class="type">actvec_t</span> sum_accumulator = <span class="type">actvec_t</span>(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Loop over all pixels in the feature maps for the current channel tile.</span></span><br><span class="line">        <span class="comment">// The H and W dimensions are flattened for this operation.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_pixels; ++i) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Calculate pointer to Input[pixel_i][c_tile]</span></span><br><span class="line">            <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_base + i * C_TILES + c_tile;</span><br><span class="line">            <span class="comment">// Accumulate the vector values</span></span><br><span class="line">            sum_accumulator = sum_accumulator + <span class="type">actvec_t</span>(*x_ptr);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- Perform Per-Lane Division to get the Average ---</span></span><br><span class="line">        <span class="type">actvec_t</span> avg_vec = <span class="type">actvec_t</span>(<span class="number">0</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> lane = <span class="number">0</span>; lane &lt; VSIZE; ++lane) &#123;</span><br><span class="line">            <span class="comment">// Extract the sum for a single channel (lane)</span></span><br><span class="line">            <span class="type">actsca_t</span> lane_sum = act_ext(sum_accumulator, lane);</span><br><span class="line">            <span class="comment">// Perform scalar integer division to find the average</span></span><br><span class="line">            <span class="type">actsca_t</span> lane_avg = (<span class="type">int</span>)lane_sum / num_pixels;</span><br><span class="line">            <span class="comment">// Update the corresponding lane in the average vector with the result</span></span><br><span class="line">            avg_vec = act_upd(avg_vec, (<span class="type">int</span>)lane_avg, lane);</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Store the resulting average vector</span></span><br><span class="line">        out_vector[c_tile] = <span class="type">vec_t</span>(avg_vec);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="fully-connected"><a href="#fully-connected" class="headerlink" title="fully_connected"></a><code>fully_connected</code></h2><p>The fully connected (or dense) layer performs a linear transformation on an input vector, typically after the feature extraction stages of a CNN. It maps the learned features to the final output classes. The operation is equivalent to a matrix-vector multiplication, where the matrix is the layer’s weights.</p><h3 id="Interface-Definition-4"><a href="#Interface-Definition-4" class="headerlink" title="Interface Definition"></a>Interface Definition</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">fully_connected_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_results, <span class="type">vec_t</span> chess_storage(VM)* in_vector,</span></span><br><span class="line"><span class="params">                                       <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base, <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec,</span></span><br><span class="line"><span class="params">                                       <span class="type">int</span> IN_FEATURES, <span class="type">int</span> OUT_FEATURES, <span class="type">int</span> mul_o, <span class="type">int</span> shf_o)</span>;</span><br></pre></td></tr></table></figure><h3 id="Parameters-4"><a href="#Parameters-4" class="headerlink" title="Parameters"></a>Parameters</h3><ul><li><code>out_results</code>: A pointer to the output vector where the final scores or logits are stored.</li><li><code>in_vector</code>: A pointer to the flattened input feature vector.</li><li><code>weight_base</code>: A pointer to the weight matrix of shape <code>[OUT_FEATURES, IN_FEATURES]</code>.</li><li><code>bias_vec</code>: A pointer to the bias vector of size <code>OUT_FEATURES</code>.</li><li><code>IN_FEATURES</code>, <code>OUT_FEATURES</code>: The number of input and output features (neurons). Both are assumed to be multiples of <code>VSIZE</code>.</li><li><code>mul_o</code>, <code>shf_o</code>: Scaling factor and right-shift amount for output quantization.</li></ul><h3 id="Core-Logic-4"><a href="#Core-Logic-4" class="headerlink" title="Core Logic"></a>Core Logic</h3><p>The function computes the output vector by iterating through the output features in tiles of <code>VSIZE</code>. For each output tile, it initializes an accumulator with the corresponding bias values. It then performs a series of <code>mat_vec_mul_add</code> operations, effectively multiplying a slice of the weight matrix with the entire input vector. This computes one tile of the output. A final quantization step is applied before storing the result.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --- Fully Connected Layer ---</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">fully_connected_vectorized</span><span class="params">(<span class="type">vec_t</span> chess_storage(VM)* out_results, <span class="type">vec_t</span> chess_storage(VM)* in_vector,</span></span><br><span class="line"><span class="params">                                       <span class="type">const</span> <span class="type">mat_t</span> chess_storage(WM)* weight_base, <span class="type">const</span> <span class="type">actvec_t</span> chess_storage(WM)* bias_vec,</span></span><br><span class="line"><span class="params">                                       <span class="type">int</span> IN_FEATURES, <span class="type">int</span> OUT_FEATURES, <span class="type">int</span> mul_o, <span class="type">int</span> shf_o)</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> IN_TILES = IN_FEATURES / VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> OUT_TILES_VEC = OUT_FEATURES / VSIZE; <span class="comment">// Number of output vector tiles</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Process the output features in chunks of VSIZE</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> out_tile = <span class="number">0</span>; out_tile &lt; OUT_TILES_VEC; ++out_tile) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="comment">// Initialize accumulator with the bias for this output tile</span></span><br><span class="line">        <span class="type">actvec_t</span> act = bias_vec[out_tile];</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Pointer to the start of the weights for this output tile</span></span><br><span class="line">        <span class="type">const</span> <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* wx_ptr = weight_base + out_tile * IN_TILES;</span><br><span class="line">        <span class="type">const</span> <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* x_ptr = in_vector;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Loop over the input features, performing matrix-vector multiplication in a tiled fashion</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> in_tile = <span class="number">0</span>; in_tile &lt; IN_TILES; ++in_tile) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            act = mat_vec_mul_add(act, *wx_ptr, *x_ptr);</span><br><span class="line">            wx_ptr++;</span><br><span class="line">            x_ptr++;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Apply final quantization to the accumulated result</span></span><br><span class="line">        act = (act * mul_o) &gt;&gt; shf_o;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// Store the VSIZE scalar results</span></span><br><span class="line">        out_results[out_tile] = <span class="type">vec_t</span>(act);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><h2 id="attention-head"><a href="#attention-head" class="headerlink" title="attention_head"></a><code>attention_head</code></h2><p>This function implements a complete, quantized single-head attention mechanism, a fundamental component of the Transformer architecture. It executes the core scaled dot-product attention formula:</p><p>$$<br>\text{Attention}(Q, K, V) &#x3D; \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$</p><p>The implementation is highly optimized for a vector processor, employing techniques such as memory layout optimization, tiled computation, in-place operations, and integer quantization to achieve high performance. The function is broken down into four main computational steps.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">compute_attention_head</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Data Pointers ---</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* q_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* k_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* v_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* o_ptr,</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Intermediate Buffer Pointers ---</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* attention_scores_ptr,  <span class="comment">// Buffer for A = QK^T</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* attention_weights_ptr, <span class="comment">// Buffer for B = Softmax(A)</span></span></span><br><span class="line"><span class="params">    <span class="comment">// --- Dimension Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> T_SIZE,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> K_SIZE,</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Quantization/Scaling Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_mul_qk,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_shf_qk,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_mul_softmax,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_shf_softmax,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_mul_out,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_shf_out</span></span><br><span class="line"><span class="params">)</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">byte_off_t</span> mat_offset = VSIZE * VSIZE;</span><br><span class="line">    <span class="type">const</span> <span class="type">byte_off_t</span> actmat_offset = <span class="number">4</span> * VSIZE * VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 1: Attention Score Calculation -&gt; A = matmul(Q, K^T)</span></span><br><span class="line">    <span class="comment">// The code computes matmul(Q, K) assuming K is already laid out as K^T in memory.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; T_SIZE / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; VSIZE; ii++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">int</span> row_offset = ii * VSIZE + i * VSIZE * K_SIZE;</span><br><span class="line">                <span class="type">int</span> col_offset = j * VSIZE * K_SIZE;</span><br><span class="line"></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* q_vec_ptr = (<span class="type">vec_t</span> chess_storage(WM)*)q_ptr + (<span class="type">byte_off_t</span>)row_offset;</span><br><span class="line">                <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* k_mat_ptr = (<span class="type">mat_t</span> chess_storage(VM)*)k_ptr + (<span class="type">byte_off_t</span>)col_offset;</span><br><span class="line">                <span class="type">actvec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* a_actvec_ptr = (<span class="type">actvec_t</span> chess_storage(WM)*)attention_scores_ptr + (<span class="type">byte_off_t</span>)(row_offset * <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">                <span class="type">actvec_t</span> act = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; K_SIZE / VSIZE; k++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                    act = mat_vec_mul_add(act, *k_mat_ptr, *q_vec_ptr);</span><br><span class="line">                    q_vec_ptr = q_vec_ptr + mat_offset;</span><br><span class="line">                    k_mat_ptr = k_mat_ptr + mat_offset;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// Scale and quantize the result of the matrix multiplication.</span></span><br><span class="line">                act = (act * scale_mul_qk) &gt;&gt; scale_shf_qk;</span><br><span class="line">                *a_actvec_ptr = act;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 2: B = Softmax(A)</span></span><br><span class="line">    <span class="comment">// This is a three-pass, row-wise softmax for numerical stability and quantization.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; T_SIZE / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; VSIZE; ii++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="type">int</span> row_base_offset = ii * VSIZE + i * VSIZE * K_SIZE;</span><br><span class="line">            <span class="type">int</span> act_row_base_offset = row_base_offset * <span class="number">4</span>;</span><br><span class="line"></span><br><span class="line">            <span class="type">actvec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* a_actvec_ptr = (<span class="type">actvec_t</span> chess_storage(WM)*)attention_scores_ptr + (<span class="type">byte_off_t</span>)(act_row_base_offset);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// --- Pass 1: Find the maximum value in the current row of A ---</span></span><br><span class="line">            <span class="type">actvec_t</span> act_max_vec = *a_actvec_ptr;</span><br><span class="line">            a_actvec_ptr = a_actvec_ptr + actmat_offset;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                act_max_vec = max_(*a_actvec_ptr, act_max_vec);</span><br><span class="line">                a_actvec_ptr = a_actvec_ptr + actmat_offset;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="type">int</span> max_val_scalar = <span class="number">-1000000</span>; <span class="comment">// A very small number</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                max_val_scalar = max_(max_val_scalar, act_ext(act_max_vec, j));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// --- Pass 2: Calculate exponentials and sum ---</span></span><br><span class="line">            <span class="comment">// We compute exp(x_i - max(x)) for numerical stability. pow2 is used as a hardware-friendly approximation of exp.</span></span><br><span class="line">            <span class="type">int</span> sum_exp = <span class="number">0</span>;</span><br><span class="line">            max_val_scalar = -max_val_scalar; <span class="comment">// Prepare for addition (subtraction)</span></span><br><span class="line">            a_actvec_ptr = (<span class="type">actvec_t</span> chess_storage(WM)*)attention_scores_ptr + (<span class="type">byte_off_t</span>)(act_row_base_offset);</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">actvec_t</span> act = *a_actvec_ptr;</span><br><span class="line">                act = act + max_val_scalar;</span><br><span class="line">                act = pow2(act); <span class="comment">// Approximation of exp2</span></span><br><span class="line">                *a_actvec_ptr = act; <span class="comment">// Overwrite A with exponentiated values</span></span><br><span class="line">                sum_exp = sum_exp + vsum(act);</span><br><span class="line">                a_actvec_ptr = a_actvec_ptr + actmat_offset;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// --- Pass 3: Normalize and quantize --- Calculate 1/sum_exp using integer division and scale it.</span></span><br><span class="line">            <span class="type">int</span> inv_sum = <span class="number">65536</span> / sum_exp; <span class="comment">// 65536 represents 1.0 in Q16 format</span></span><br><span class="line">            inv_sum = (inv_sum * scale_mul_softmax); <span class="comment">// Apply user-defined scaling</span></span><br><span class="line"></span><br><span class="line">            a_actvec_ptr = (<span class="type">actvec_t</span> chess_storage(WM)*)attention_scores_ptr + (<span class="type">byte_off_t</span>)(act_row_base_offset);</span><br><span class="line">            <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* b_vec_ptr = (<span class="type">vec_t</span> chess_storage(WM)*)attention_weights_ptr + (<span class="type">byte_off_t</span>)(row_base_offset);</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">actvec_t</span> act = *a_actvec_ptr;</span><br><span class="line">                act = act * inv_sum;</span><br><span class="line">                act = act &gt;&gt; scale_shf_softmax; <span class="comment">// Final shift for quantization</span></span><br><span class="line">                *b_vec_ptr = act; <span class="comment">// Store final 8-bit quantized attention weight</span></span><br><span class="line">                b_vec_ptr = b_vec_ptr + mat_offset;</span><br><span class="line">                a_actvec_ptr = a_actvec_ptr + actmat_offset;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 3: In-place Transpose of the Value (V) matrix. This prepares V for the final matrix multiplication.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; T_SIZE / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = i; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="type">byte_off_t</span> up_offset = (i * (T_SIZE / VSIZE) + j) * VSIZE * VSIZE;</span><br><span class="line">            <span class="type">byte_off_t</span> dw_offset = (j * (T_SIZE / VSIZE) + i) * VSIZE * VSIZE;</span><br><span class="line"></span><br><span class="line">            <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* mat_ptr_up = (<span class="type">mat_t</span> chess_storage(VM)*)v_ptr + up_offset;</span><br><span class="line">            <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* mat_ptr_dw = (<span class="type">mat_t</span> chess_storage(VM)*)v_ptr + dw_offset;</span><br><span class="line"></span><br><span class="line">            <span class="type">mat_t</span> mat_up = *mat_ptr_up;</span><br><span class="line">            <span class="keyword">if</span> (i == j) &#123;</span><br><span class="line">                *mat_ptr_up = transpose(mat_up);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">mat_t</span> mat_dw = *mat_ptr_dw;</span><br><span class="line">                *mat_ptr_up = transpose(mat_dw);</span><br><span class="line">                *mat_ptr_dw = transpose(mat_up);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 4: Final Output -&gt; O = matmul(B, V_transposed)</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; T_SIZE / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; T_SIZE / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; VSIZE; ii++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">int</span> row_offset = ii * VSIZE + i * VSIZE * K_SIZE;</span><br><span class="line">                <span class="type">int</span> col_offset = j * VSIZE * K_SIZE;</span><br><span class="line"></span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* b_vec_ptr = (<span class="type">vec_t</span> chess_storage(WM)*)attention_weights_ptr + (<span class="type">byte_off_t</span>)row_offset;</span><br><span class="line">                <span class="type">mat_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* v_mat_ptr = (<span class="type">mat_t</span> chess_storage(VM)*)v_ptr + (<span class="type">byte_off_t</span>)col_offset;</span><br><span class="line">                <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* o_vec_ptr = (<span class="type">vec_t</span> chess_storage(VM)*)o_ptr + (<span class="type">byte_off_t</span>)row_offset;</span><br><span class="line"></span><br><span class="line">                <span class="type">actvec_t</span> act = <span class="number">0</span>;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> k = <span class="number">0</span>; k &lt; K_SIZE / VSIZE; k++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                    act = mat_vec_mul_add(act, *v_mat_ptr, *b_vec_ptr);</span><br><span class="line">                    b_vec_ptr = b_vec_ptr + mat_offset;</span><br><span class="line">                    v_mat_ptr = v_mat_ptr + mat_offset;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="comment">// Scale and quantize the final output.</span></span><br><span class="line">                act = (act * scale_mul_out) &gt;&gt; scale_shf_out;</span><br><span class="line">                *o_vec_ptr = act;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Functional-Breakdown"><a href="#Functional-Breakdown" class="headerlink" title="Functional Breakdown"></a>Functional Breakdown</h3><ol><li><p><strong>Step 1: Attention Score Calculation (A &#x3D; QK<sup>T</sup>)</strong></p><ul><li>This step computes the dot product between each query vector and all key vectors. The code assumes the Key matrix (<code>k_ptr</code>) is already stored in a pre-transposed, tiled layout to facilitate efficient memory access patterns for the <code>mat_vec_mul_add</code> intrinsic.</li><li>The result, a 32-bit activation matrix <code>A</code>, is immediately scaled and quantized using the provided <code>scale_mul_qk</code> and <code>scale_shf_qk</code> parameters. This corresponds to the <code>1/sqrt(d_k)</code> scaling factor and prepares the data for the subsequent softmax operation. The result is stored in the <code>attention_scores_ptr</code> buffer.</li></ul></li><li><p><strong>Step 2: Quantized Softmax (B &#x3D; Softmax(A))</strong></p><ul><li>A numerically stable, three-pass softmax is applied row-wise to the attention scores matrix <code>A</code>.</li><li><strong>Pass 1:</strong> Finds the maximum value in each row to prevent numerical overflow during exponentiation.</li><li><strong>Pass 2:</strong> Subtracts the row maximum, computes an approximation of the exponential function using the hardware-accelerated <code>pow2</code> instruction, and calculates the sum of these exponentiated values for each row.</li><li><strong>Pass 3:</strong> Normalizes each element by the sum and performs a final scaling and quantization to produce the 8-bit attention weights matrix <code>B</code>, which is stored in the <code>attention_weights_ptr</code> buffer.</li></ul></li><li><p><strong>Step 3: In-place Transposition of Value Matrix (V)</strong></p><ul><li>To optimize the final matrix multiplication, the Value matrix (<code>v_ptr</code>) is transposed in-place. This operation rearranges the <code>VSIZE x VSIZE</code> blocks of the matrix, ensuring that the subsequent multiplication step can access data from <code>V</code> with a contiguous memory access pattern, which is critical for performance.</li></ul></li><li><p><strong>Step 4: Output Calculation (O &#x3D; BV)</strong></p><ul><li>The final output of the attention head is computed by multiplying the 8-bit attention weights matrix <code>B</code> with the now-transposed Value matrix <code>V</code>.</li><li>Similar to Step 1, this is performed using the efficient <code>mat_vec_mul_add</code> intrinsic. The 32-bit accumulated result is then scaled and quantized to the final 8-bit output format using <code>scale_mul_out</code> and <code>scale_shf_out</code> before being written to the output buffer <code>o_ptr</code>.</li></ul></li></ol><h2 id="Add-Norm"><a href="#Add-Norm" class="headerlink" title="Add&amp;Norm"></a><code>Add&amp;Norm</code></h2><p>This function implements a Post-LayerNorm residual connection, a standard operation found after the self-attention and feed-forward sub-layers in a Transformer block. It first performs an element-wise addition of an input tensor and a residual tensor, then applies Layer Normalization to the result. The implementation is carefully designed to maximize performance by using vector primitives for initial calculations and a register-pressure-aware strategy for the final normalization step.</p><p>The Layer Normalization formula is:</p><p>$$<br>\text{output} &#x3D; \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta<br>$$</p><p>where $x$ is the result of <code>input + residual</code>, $\mu$ and $\sigma^2$ are the mean and variance across the feature dimension, and $\gamma$ and $\beta$ are learnable scale and shift parameters.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">add_and_layer_norm</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Data Pointers (in Vector Memory) ---</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* input_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* residual_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* gamma_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* beta_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* output_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(VM)* temp_buffer_ptr,</span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params">    <span class="comment">// --- Dimension &amp; Configuration Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">const</span> <span class="type">int</span> D,</span></span><br><span class="line"><span class="params">    <span class="type">const</span> <span class="type">float</span> epsilon,</span></span><br><span class="line"><span class="params">    <span class="type">const</span> <span class="type">float</span> output_quant_scale</span></span><br><span class="line"><span class="params">)</span> &#123;</span><br><span class="line">    <span class="comment">// Cast void pointers to the correct vector types</span></span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_in   = (<span class="type">vec_t</span> chess_storage(VM)*)input_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_res  = (<span class="type">vec_t</span> chess_storage(VM)*)residual_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_gam  = (<span class="type">vec_t</span> chess_storage(VM)*)gamma_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_bet  = (<span class="type">vec_t</span> chess_storage(VM)*)beta_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_out  = (<span class="type">vec_t</span> chess_storage(VM)*)output_ptr;</span><br><span class="line">    <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(VM)</span>* p_tmp  = (<span class="type">vec_t</span> chess_storage(VM)*)temp_buffer_ptr;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> num_vectors = D / VSIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 1: Residual Addition (Input + Residual) -&gt; Temp Buffer</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="type">vec_t</span> in_vec = *p_in++;</span><br><span class="line">        <span class="type">vec_t</span> res_vec = *p_res++;</span><br><span class="line">        <span class="type">vec_t</span> sum = in_vec + res_vec; <span class="comment">// Element-wise addition of input and residual</span></span><br><span class="line">        *p_tmp++ = sum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 2: Calculate Mean and Variance using vector primitives</span></span><br><span class="line">    <span class="comment">// We use Var(X) = E[X^2] - (E[X])^2 for efficiency.</span></span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> total_sum = <span class="number">0</span>;</span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> total_sum_sq = <span class="number">0</span>;</span><br><span class="line">    p_tmp = (<span class="type">vec_t</span> chess_storage(VM)*)temp_buffer_ptr; <span class="comment">// Reset temp pointer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="type">vec_t</span> tmp_vec = *p_tmp++;</span><br><span class="line">        <span class="type">actvec_t</span> tmp_actvec = (<span class="type">actvec_t</span>) tmp_vec; <span class="comment">// Promote 8-bit vector to 32-bit vector</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// Accumulate sum for mean calculation</span></span><br><span class="line">        total_sum += (<span class="type">long</span> <span class="type">long</span>)vsum(tmp_actvec);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Accumulate sum of squares for variance calculation</span></span><br><span class="line">        <span class="type">actvec_t</span> tmp_actvec2 = tmp_actvec; <span class="comment">// Create a 32-bit accumulator vector</span></span><br><span class="line">        <span class="type">actvec_t</span> tmp_sq_actvec = tmp_actvec * tmp_actvec2; <span class="comment">// Element-wise square</span></span><br><span class="line">        total_sum_sq += (<span class="type">long</span> <span class="type">long</span>)vsum(tmp_sq_actvec);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// --- Perform float calculations for statistics ---</span></span><br><span class="line">    <span class="comment">// Note: Type conversions use the provided soft-float wrappers</span></span><br><span class="line">    <span class="type">float32_t</span> f_sum = wrap_i64_to_f32(total_sum);</span><br><span class="line">    <span class="type">float32_t</span> f_sum_sq = wrap_i64_to_f32(total_sum_sq);</span><br><span class="line">    <span class="type">float32_t</span> f_dim = wrap_i32_to_f32(D);</span><br><span class="line"></span><br><span class="line">    <span class="type">float32_t</span> mean = f_sum / f_dim;</span><br><span class="line">    <span class="type">float32_t</span> e_x_sq = f_sum_sq / f_dim;</span><br><span class="line">    <span class="type">float32_t</span> variance = e_x_sq - (mean * mean);</span><br><span class="line"></span><br><span class="line">    <span class="type">float32_t</span> inv_std_dev = <span class="number">1.0f</span> / sqrtf(variance + epsilon);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Step 3: Apply Normalization, Scale (γ), Shift (β), and Quantize</span></span><br><span class="line">    <span class="comment">// This part is done element-wise.</span></span><br><span class="line">    p_tmp = (<span class="type">vec_t</span> chess_storage(VM)*)temp_buffer_ptr; <span class="comment">// Reset pointers again</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; num_vectors; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="type">vec_t</span> tmp_vec = *p_tmp++;</span><br><span class="line">        <span class="type">vec_t</span> gam_vec = *p_gam++;</span><br><span class="line">        <span class="type">vec_t</span> bet_vec = *p_bet++;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- Create simple, spillable C arrays on the stack ---</span></span><br><span class="line">        <span class="type">int</span> val_scalars[VSIZE];</span><br><span class="line">        <span class="type">int</span> gam_scalars[VSIZE];</span><br><span class="line">        <span class="type">int</span> bet_scalars[VSIZE];</span><br><span class="line">        <span class="type">float32_t</span> norm_f_temp[VSIZE];</span><br><span class="line">        <span class="type">float32_t</span> scaled_f_temp[VSIZE];</span><br><span class="line">        <span class="type">int</span> final_results[VSIZE];</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- STAGE 1: DATA EXTRACTION ---</span></span><br><span class="line">        <span class="comment">// This loop only uses vector intrinsics to read data. Low register pressure.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            val_scalars[j] = act_ext((<span class="type">actvec_t</span>)tmp_vec, j);</span><br><span class="line">            gam_scalars[j] = act_ext((<span class="type">actvec_t</span>)gam_vec, j);</span><br><span class="line">            bet_scalars[j] = act_ext((<span class="type">actvec_t</span>)bet_vec, j);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- STAGE 2: FLOATING-POINT COMPUTATION ---</span></span><br><span class="line">        <span class="comment">// This loop only uses standard C types and function calls.</span></span><br><span class="line">        <span class="comment">// The compiler can freely spill any of these variables to memory.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            <span class="comment">// --- Convert to float ---</span></span><br><span class="line">            <span class="type">float32_t</span> val_f = wrap_i32_to_f32(val_scalars[j]);</span><br><span class="line">            <span class="comment">// --- Apply LayerNorm formula in float ---</span></span><br><span class="line">            norm_f_temp[j] = (val_f - mean) * inv_std_dev;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            <span class="comment">// --- Scale and shift ---</span></span><br><span class="line">            <span class="type">float32_t</span> gam_f = wrap_i32_to_f32(gam_scalars[j]);</span><br><span class="line">            <span class="type">float32_t</span> bet_f = wrap_i32_to_f32(bet_scalars[j]);</span><br><span class="line">            scaled_f_temp[j] = norm_f_temp[j] * gam_f + bet_f;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            <span class="comment">// --- Quantize back to integer ---</span></span><br><span class="line">            final_results[j] = wrap_f32_to_i32(scaled_f_temp[j] * output_quant_scale);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- STAGE 3: VECTOR ASSEMBLY ---</span></span><br><span class="line">        <span class="comment">// This loop only uses vector intrinsics to write data. Low register pressure.</span></span><br><span class="line">        <span class="type">vec_t</span> out_actvec; <span class="comment">// Initialize an empty 32-bit accumulator vector</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; ++j) &#123;</span><br><span class="line">            out_actvec = (<span class="type">vec_t</span>)act_upd(out_actvec, final_results[j], j);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// --- Store the final 8-bit vector ---</span></span><br><span class="line">        <span class="comment">// Convert the 32-bit accumulator vector back to an 8-bit vector and store</span></span><br><span class="line">        *p_out++ = (<span class="type">vec_t</span>)out_actvec;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><ol><li><p><strong>Residual Addition:</strong> The function begins by performing a simple, element-wise vector addition <code>input + residual</code>, storing the intermediate 8-bit result in the provided <code>temp_buffer_ptr</code>.</p></li><li><p><strong>Statistics Calculation:</strong> In a single pass over the temporary buffer, the function calculates the sum and sum-of-squares of the elements required for normalization.</p><ul><li><strong>High-Precision Accumulation:</strong> It uses <code>long long</code> accumulators (<code>total_sum</code>, <code>total_sum_sq</code>) to prevent overflow when summing up values over a large feature dimension <code>D</code>.</li><li><strong>Efficient Variance:</strong> It computes variance using the stable formula $\text{Var}(X) &#x3D; E[X^2] - (E[X])^2$.</li><li><strong>Floating-Point Conversion:</strong> The final statistics (mean, variance, inverse standard deviation) are calculated using 32-bit floating-point arithmetic for higher accuracy.</li></ul></li><li><p><strong>Normalization, Scaling, and Quantization:</strong> This final step applies the normalization formula and is implemented using a three-stage approach to minimize register pressure and allow the compiler to generate efficient code. For each vector of data:</p><ul><li><strong>Stage 1 (Data Extraction):</strong> Vector data (<code>tmp_vec</code>, <code>gam_vec</code>, <code>bet_vec</code>) is read from memory and its elements are extracted into simple scalar C arrays on the stack. This isolates vector memory access.</li><li><strong>Stage 2 (Floating-Point Computation):</strong> All floating-point operations—normalization, scaling ($\gamma$), shifting ($\beta$), and final output scaling—are performed on the scalar arrays. This core computational stage is free of vector intrinsics, making it easier for the compiler to manage registers by spilling variables to the stack if necessary.</li><li><strong>Stage 3 (Vector Assembly):</strong> The final integer results are assembled back into a vector register using <code>act_upd</code> intrinsics and then written to the output memory location.</li></ul></li></ol><h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a><code>Softmax</code></h2><p>This function implements a row-wise, numerically stable, quantized softmax operation. It is optimized for vector processors and is a key subroutine within the <code>attention_head</code> function, but is also provided here as a standalone kernel. It takes a matrix of 32-bit activation scores as input and produces a matrix of 8-bit quantized weights.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">void</span> <span class="title function_">quantized_softmax_rowwise</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Data Pointers ---</span></span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* in_ptr,</span></span><br><span class="line"><span class="params">    <span class="type">void</span> chess_storage(WM)* out_ptr,</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Dimension Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> dim_size,</span></span><br><span class="line"><span class="params">    <span class="comment">// --- Quantization/Scaling Parameters ---</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_mul_softmax,</span></span><br><span class="line"><span class="params">    <span class="type">int</span> scale_shf_softmax</span></span><br><span class="line"><span class="params">)</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">byte_off_t</span> act_tile_offset = <span class="number">4</span> * VSIZE * VSIZE; <span class="comment">// Offset for a tile of 32-bit activations</span></span><br><span class="line">    <span class="type">const</span> <span class="type">byte_off_t</span> out_tile_offset = VSIZE * VSIZE;     <span class="comment">// Offset for a tile of 8-bit outputs</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Iterate over each row of the input matrix.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; dim_size / VSIZE; i++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> ii = <span class="number">0</span>; ii &lt; VSIZE; ii++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">            <span class="comment">// Calculate the byte offset to the beginning of the current row.</span></span><br><span class="line">            <span class="type">int</span> row_start_idx = ii * VSIZE + i * VSIZE * dim_size;</span><br><span class="line">            <span class="type">int</span> act_row_base_offset = (<span class="type">int</span>)row_start_idx * <span class="number">4</span>;</span><br><span class="line">            <span class="type">int</span> out_row_base_offset = (<span class="type">int</span>)row_start_idx;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// === Pass 1: Find the maximum value in the current row ===</span></span><br><span class="line">            <span class="type">actvec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* p_act_row = (<span class="type">actvec_t</span> chess_storage(WM)*)in_ptr + (<span class="type">byte_off_t</span>)act_row_base_offset;</span><br><span class="line">            <span class="type">actvec_t</span> act_max_vec = *p_act_row;</span><br><span class="line"></span><br><span class="line">            p_act_row = p_act_row + (<span class="type">byte_off_t</span>)act_tile_offset; <span class="comment">// Move to the next tile in the same row</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">1</span>; j &lt; dim_size / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                act_max_vec = max_(*p_act_row, act_max_vec);</span><br><span class="line">                p_act_row = p_act_row + (<span class="type">byte_off_t</span>)act_tile_offset;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="comment">// Reduce the vector max to a scalar max</span></span><br><span class="line">            <span class="type">int</span> max_val_scalar = <span class="number">-1000000</span>; <span class="comment">// Initialize with a very small number</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                max_val_scalar = max_(max_val_scalar, act_ext(act_max_vec, j));</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// === Pass 2: Calculate exponentials (using pow2) and sum ===</span></span><br><span class="line">            <span class="comment">// We compute exp(x_i - max(x)) for numerical stability.</span></span><br><span class="line">            <span class="type">int</span> sum_exp = <span class="number">0</span>;</span><br><span class="line">            max_val_scalar = -max_val_scalar; <span class="comment">// Negate for addition (to perform subtraction)</span></span><br><span class="line"></span><br><span class="line">            p_act_row = (<span class="type">actvec_t</span> chess_storage(WM)*)in_ptr + (<span class="type">byte_off_t</span>)act_row_base_offset;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; dim_size / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">actvec_t</span> act = *p_act_row;</span><br><span class="line">                act = act + max_val_scalar;</span><br><span class="line">                act = pow2(act); <span class="comment">// Hardware approximation of exp2</span></span><br><span class="line">                *p_act_row = act; <span class="comment">// Overwrite input buffer with exponentiated values</span></span><br><span class="line">                sum_exp = sum_exp + vsum(act);</span><br><span class="line">                p_act_row = p_act_row + (<span class="type">byte_off_t</span>)act_tile_offset;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// === Pass 3: Normalize and quantize the row ===</span></span><br><span class="line">            <span class="type">int</span> inv_sum = (sum_exp == <span class="number">0</span>) ? <span class="number">0</span> : (<span class="number">65536</span> / sum_exp);</span><br><span class="line">            inv_sum = (inv_sum * scale_mul_softmax); <span class="comment">// Apply user-defined scaling</span></span><br><span class="line"></span><br><span class="line">            p_act_row = (<span class="type">actvec_t</span> chess_storage(WM)*)in_ptr + (<span class="type">byte_off_t</span>)act_row_base_offset;</span><br><span class="line">            <span class="type">vec_t</span> <span class="title function_">chess_storage</span><span class="params">(WM)</span>* p_out_row = (<span class="type">vec_t</span> chess_storage(WM)*)out_ptr + (<span class="type">byte_off_t</span>)out_row_base_offset;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; dim_size / VSIZE; j++) chess_loop_range(<span class="number">1</span>,) &#123;</span><br><span class="line">                <span class="type">actvec_t</span> act = *p_act_row;</span><br><span class="line">                act = act * inv_sum;</span><br><span class="line">                act = act &gt;&gt; scale_shf_softmax; <span class="comment">// Final shift for quantization</span></span><br><span class="line">                *p_out_row = act; <span class="comment">// Store final 8-bit quantized attention weight</span></span><br><span class="line">                p_out_row = p_out_row + (<span class="type">byte_off_t</span>)out_tile_offset;</span><br><span class="line">                p_act_row = p_act_row + (<span class="type">byte_off_t</span>)act_tile_offset;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Algorithmic-Strategy"><a href="#Algorithmic-Strategy" class="headerlink" title="Algorithmic Strategy"></a>Algorithmic Strategy</h3><p>The function employs a three-pass algorithm to compute the softmax for each row of the input matrix.</p><ol><li><p><strong>Pass 1: Find Row Maximum:</strong> To ensure numerical stability, the first pass iterates through each row to find its maximum value. Subtracting this maximum from every element in the row before exponentiation $(\text{i.e., } e^{x_i - \max(x)})$ recenters the data around zero, preventing potential floating-point overflow when dealing with large activation values. This pass uses vector <code>max_</code> operations for efficiency, followed by a scalar reduction to get the final maximum value.</p></li><li><p><strong>Pass 2: Exponentiate and Sum:</strong> The second pass revisits each row. It subtracts the previously found maximum value, computes the exponential, and accumulates the sum of these exponentiated values.</p><ul><li><strong>Hardware Approximation:</strong> The standard <code>exp(x)</code> function is replaced with a hardware-friendly <code>pow2(x)</code> approximation, which is significantly faster on the target processor.</li><li><strong>In-Place Operation:</strong> To conserve memory, the calculated exponentiated values overwrite the original input data in <code>in_ptr</code>.</li></ul></li><li><p><strong>Pass 3: Normalize and Quantize:</strong> The final pass normalizes the exponentiated values and quantizes them to 8-bit integers.</p><ul><li><strong>Multiplicative Inverse:</strong> Instead of performing a costly division by <code>sum_exp</code> for each element, the code calculates the scaled reciprocal (<code>inv_sum</code>) once per row and uses multiplication. The fixed-point value <code>65536</code> represents <code>1.0</code>.</li><li><strong>Quantization:</strong> The result is scaled by <code>inv_sum</code> and the user-provided <code>scale_mul_softmax</code>, then right-shifted by <code>scale_shf_softmax</code> to produce the final 8-bit quantized value, which is written to the <code>out_ptr</code> buffer.</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Computer Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FlexAcc Kernel </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ASIP Designer Processor Modeling</title>
      <link href="/2025/07/03/asip_design_tool/asip_training_processor_modeling/"/>
      <url>/2025/07/03/asip_design_tool/asip_training_processor_modeling/</url>
      
        <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="Why-and-How-to-Design-an-ASIP"><a href="#Why-and-How-to-Design-an-ASIP" class="headerlink" title="Why and How to Design an ASIP?"></a>Why and How to Design an ASIP?</h2><p>Application-Specific Instruction-set Processors (ASIPs) bridge the gap between general-purpose microprocessors and dedicated hardware, offering:</p><ul><li>Maximum performance</li><li>Minimal power consumption</li><li>Programmability</li></ul><p>The core steps in ASIP design include:</p><ul><li><strong>Modeling the Instruction Set Architecture (ISA):</strong> Using the nML language to define the processor’s structure.</li><li><strong>Automatic Software Development Kit (SDK) Generation:</strong> Produces a C compiler and related tools.</li><li><strong>Algorithm-Driven Architectural Exploration:</strong> Utilizes a “Compile-in-the-Loop” approach to optimize the architecture.</li><li><strong>Automatic Synthesizable RTL Generation:</strong> Employs a “Synthesis-in-the-Loop” methodology for hardware implementation.</li></ul><p><img src="/image/img_asip_design/intro_asip_design_overview.png" alt="ASIP Design Overview"></p><h2 id="Processor-Modeling"><a href="#Processor-Modeling" class="headerlink" title="Processor Modeling"></a>Processor Modeling</h2><h3 id="Header-File"><a href="#Header-File" class="headerlink" title="Header File"></a>Header File</h3><p>The core header file, <code>&lt;processor&gt;.h</code>, defines primitive functions and data types within a dedicated namespace: <code>namespace &lt;processor&gt;_primitive &#123;&#125;</code>. It should include:</p><ul><li>Primitive data type definitions</li><li>Primitive function declarations</li><li>Primitive data type conversion rules</li></ul><p>Additional header files, such as <code>&lt;processor&gt;_&lt;function&gt;.h</code>, can be created to extend functionality with more operations or features.</p><h3 id="nML-Language"><a href="#nML-Language" class="headerlink" title="nML Language"></a>nML Language</h3><p>The nML language defines the structural skeleton and instruction-set architecture of the processor. Key components include:</p><ul><li><code>mem</code>: Defines memory spaces.</li><li><code>reg</code>: Specifies register structures.</li><li><code>pipe</code>: Stores values across pipeline stages (persistent).</li><li><code>trn</code>: Holds temporary values within a single stage.</li><li><code>opn</code>: Represents operation nodes.</li><li><code>cst</code>: Declares type conversion rules.</li><li><code>action</code>: Defines instruction behavior.</li><li><code>image</code>: Specifies binary encoding for instructions.</li><li><code>syntax</code>: Defines the assembly code representation.</li></ul><p>Example nML code:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">opn alu_inst (op:opcode, x:c2u, y:c2u, val:c16s) &#123;</span><br><span class="line">  action &#123;</span><br><span class="line">    stage EX1:</span><br><span class="line">    A = R[x];</span><br><span class="line">    B = val;</span><br><span class="line">    switch (op) &#123;</span><br><span class="line">      case add: C = add(A, B) @alu;</span><br><span class="line">      case sub: C = sub(A, B) @alu;</span><br><span class="line">      case and: C = band(A, B) @alu;</span><br><span class="line">      case or:  C = bor(A, B) @alu;</span><br><span class="line">    &#125;</span><br><span class="line">    stage EX2:</span><br><span class="line">    R[y] = C;</span><br><span class="line">  &#125;</span><br><span class="line">  syntax: op &quot; R&quot; y &quot;, R&quot; x &quot;, &quot; val; # Displays as &lt;add R1, R2, val&gt;</span><br><span class="line">  image: &quot;0&quot;::op::x::y::val;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Primitive-Definitions"><a href="#Primitive-Definitions" class="headerlink" title="Primitive Definitions"></a>Primitive Definitions</h3><p>Primitive definitions, written in <code>&lt;processor&gt;.p</code> files, use the Primitive Definition Language (PDG), which is based on C and supports operators, conditions, iterations, and functions with fixed types. PDG definitions are convertible to C++, VHDL, or Verilog.</p><p>Example PDG code:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">word add(word a, word b, astat&amp; s)</span><br><span class="line">&#123;</span><br><span class="line">  int17_t aa = (uint16_t)a; // Zero extension</span><br><span class="line">  int17_t bb = (uint16_t)b;</span><br><span class="line">  int17_t d = aa + bb;</span><br><span class="line">  s = (d[16]^d[15]) :: (d &lt; 0) :: (d == 0);</span><br><span class="line">  return d;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="ASIP-Designer-Tool-Features"><a href="#ASIP-Designer-Tool-Features" class="headerlink" title="ASIP Designer Tool Features"></a>ASIP Designer Tool Features</h2><h3 id="ChessDE"><a href="#ChessDE" class="headerlink" title="ChessDE"></a>ChessDE</h3><p>The ChessDE tool, typically run on Linux via command line, offers:</p><ul><li>Configuration management</li><li>Function-level incremental compilation</li><li>Parallel compilation on multi-core hosts</li></ul><p>For Instruction Set Simulator (ISS) development, it supports:</p><ul><li>Bit-true, cycle-accurate, or instruction-accurate simulation</li><li>Just-in-time compilation</li><li>Performance and speed profiling</li><li>Automatic verification</li></ul><h1 id="Primitives"><a href="#Primitives" class="headerlink" title="Primitives"></a>Primitives</h1><h2 id="Declaring-Primitives"><a href="#Declaring-Primitives" class="headerlink" title="Declaring Primitives"></a>Declaring Primitives</h2><p>Primitive types are defined using C++-style class declarations within a namespace, formatted as <code>class &lt;type&gt; property(format)</code>. Example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> tinycore2_primitive &#123;</span><br><span class="line">  <span class="function"><span class="keyword">class</span> word <span class="title">property</span><span class="params">(<span class="number">16</span> bit <span class="type">signed</span>)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">class</span> pmtype <span class="title">property</span><span class="params">(<span class="number">13</span> bit <span class="type">unsigned</span>)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">class</span> sbyte <span class="title">property</span><span class="params">(<span class="number">8</span> bit <span class="type">signed</span>)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Primitive functions represent user-defined behaviors and are categorized into:</p><ul><li><strong>Named Functions:</strong> Standard function declarations.</li><li><strong>Conversion Constructors:</strong> Handle type conversions, such as numbers or vectors.</li></ul><p>Example conversion:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sint8</span>(sin8)    <span class="comment">// xxxxxxxx -&gt; ssssssssxxxxxxxx</span></span><br><span class="line"><span class="built_in">vword3</span>(vbyte3) <span class="comment">// __ef __10 __ab -&gt; ffef 0010 ffab</span></span><br></pre></td></tr></table></figure><p>Example primitive function declaration:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">namespace</span> mycore_primitive &#123;</span><br><span class="line">  <span class="function"><span class="keyword">class</span> stat <span class="title">property</span><span class="params">(<span class="number">3</span> bit <span class="type">unsigned</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function">word <span class="title">sub</span><span class="params">(word, word)</span></span>;</span><br><span class="line">  <span class="function">word <span class="title">sub</span><span class="params">(word, word, stat&amp;)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">word</span> &#123;</span><br><span class="line">    <span class="built_in">word</span>(sbyte);</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="Primitive-Definitions-1"><a href="#Primitive-Definitions-1" class="headerlink" title="Primitive Definitions"></a>Primitive Definitions</h2><p>Primitive definitions, written in PDG (based on C), exclude control flow primitives and type conversions. PDG supports C-like operations and control flows. Example:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">int16_t</span> INT16_MAX = <span class="number">0x7fff</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int16_t</span> INT16_MIN = <span class="number">-0x8000</span>;</span><br><span class="line"></span><br><span class="line"><span class="function">word <span class="title">sat16</span><span class="params">(<span class="type">int17_t</span> a)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (a &gt; INT16_MAX)</span><br><span class="line">    <span class="keyword">return</span> INT16_MAX;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (a &lt; INT16_MIN)</span><br><span class="line">    <span class="keyword">return</span> INT16_MIN;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">word <span class="title">adds</span><span class="params">(word a, word b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">int17_t</span> sum = (<span class="type">int17_t</span>)a + (<span class="type">int17_t</span>)b;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">sat16</span>(sum);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The PDG tool structure is illustrated below:<br><img src="/image/img_asip_design/PDG_tool.png" alt="PDG Tool Structure"></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>Primitives consist of data types and functions, organized into:</p><ul><li><strong>Declaration:</strong> Defined in <code>&lt;processor&gt;.h</code></li><li><strong>Definition:</strong> Implemented in <code>&lt;processor&gt;.p</code></li></ul><p>These files collectively enable the modeling, simulation, and synthesis of ASIP designs using the ASIP Designer Tool.</p><hr><h1 id="nML"><a href="#nML" class="headerlink" title="nML"></a>nML</h1><p>In this session, we provides a concise and structured overview of the nML (nested Machine Language) processor description language used for retargeting ASIP Designer tools. nML enables a unified model of processor architecture, capturing instruction behavior, resource connectivity, and hierarchical rules in a single description.</p><h2 id="Key-Features"><a href="#Key-Features" class="headerlink" title="Key Features"></a>Key Features</h2><ul><li><strong>Hierarchical</strong>: The ISA is defined as a hierarchy of rules, allowing modular and composable instruction definitions.</li><li><strong>Structural</strong>: Processor resources (registers, memories, transitories) and their interconnections are explicitly modeled, facilitating clear design and analysis.</li><li><strong>RT-Level</strong>: Instruction behavior is described in terms of register-transfer (RT) operations, capturing the flow of data between storage elements.</li></ul><p>nML integrates datapath descriptions, instruction-set encodings, and RT behavior in one language, simplifying retargeting and tool support.</p><hr><h2 id="Structural-Skeleton"><a href="#Structural-Skeleton" class="headerlink" title="Structural Skeleton"></a>Structural Skeleton</h2><h3 id="Registers"><a href="#Registers" class="headerlink" title="Registers"></a>Registers</h3><p>Registers are declared using:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Register file</span></span><br><span class="line"><span class="keyword">reg</span> name[size] &lt;data_type, addr_type&gt;;</span><br><span class="line"><span class="comment">// Single register</span></span><br><span class="line"><span class="keyword">reg</span> name&lt;data_type&gt;;</span><br></pre></td></tr></table></figure><ul><li><p><strong>Aliases</strong>: Slice a register file into smaller files or individual registers:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">reg</span> R[<span class="number">8</span>]&lt;word,b3u&gt;;</span><br><span class="line"><span class="keyword">reg</span> S[<span class="number">4</span>]&lt;word,b2u&gt; <span class="keyword">alias</span> R[<span class="number">0</span>];</span><br><span class="line"><span class="keyword">reg</span> T[<span class="number">2</span>]&lt;word,b1u&gt; <span class="keyword">alias</span> R[<span class="number">6</span>];</span><br><span class="line"><span class="keyword">reg</span> sp&lt;word&gt;      <span class="keyword">alias</span> S[<span class="number">0</span>];</span><br></pre></td></tr></table></figure></li><li><p><strong>Record Aliases</strong>: Combine consecutive fields of a register file into a wider view:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">reg</span> R[<span class="number">8</span>]&lt;word,b3u&gt;;</span><br><span class="line"><span class="keyword">reg</span> L[<span class="number">4</span>]&lt;longword,b2u&gt; <span class="keyword">alias</span> R;  <span class="comment">// 2-word record alias</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Record Structures</strong>: Merge narrow storages into a single wide register:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">reg</span> R[<span class="number">8</span>]&lt;word,b3u&gt;;</span><br><span class="line"><span class="keyword">reg</span> S[<span class="number">8</span>]&lt;<span class="keyword">byte</span>,b3u&gt;;</span><br><span class="line"><span class="keyword">reg</span> T[<span class="number">8</span>]&lt;w24,b3u&gt; &#123; R; S; &#125;;</span><br></pre></td></tr></table></figure></li></ul><table><thead><tr><th>Register Range Aliases</th><th>Register Record Alias</th><th>Register Record Structure</th></tr></thead><tbody><tr><td><img src="/image/img_asip_design/reg_range_alias.png" alt="Register Range Aliases"></td><td><img src="/image/img_asip_design/reg_record_alias.png" alt="Register Record Alias"></td><td><img src="/image/img_asip_design/reg_record_stru.png" alt="Register Record Structure"></td></tr></tbody></table><h3 id="Memories"><a href="#Memories" class="headerlink" title="Memories"></a>Memories</h3><p>Memories are static storage elements whose values persist until written.</p><ul><li><p><strong>Declaration</strong>:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Simple memory declaration</span></span><br><span class="line">mem name[size, step_opt] &lt;data_type, addr_type&gt;;</span><br><span class="line">mem name[from.<span class="variable">.to</span>, step_opt] &lt;data_type, addr_type&gt;;</span><br></pre></td></tr></table></figure><p><strong>Examples</strong>:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mem PM[dmSize]           &lt;pmtype, word&gt; read(pmLdBus);</span><br><span class="line">mem DM[dmSize]           &lt;word, word&gt; read(dmLdBus) write(dmStBus);</span><br></pre></td></tr></table></figure></li><li><p><strong>Explicit Interface</strong>:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mem DM[dmSize]&lt;word, word&gt; access &#123;</span><br><span class="line">  ld_dm: dmLdBus‘<span class="number">1</span>‘ = DM[dmAddr];</span><br><span class="line">  st_dm: DM[dmAddr] = dmStBus;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>Timing can be annotated absolutely (by stage name) or relatively:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Absolute</span></span><br><span class="line">dmLdBus‘EX‘ = DM[dmAddr`ID`]`EX`;</span><br><span class="line"><span class="comment">// Relative</span></span><br><span class="line">dmLdBus‘<span class="number">1</span>‘  = DM[dmAddr]`<span class="number">1</span>`;</span><br></pre></td></tr></table></figure></li><li><p><strong>Memory Record Aliases</strong>: Map consecutive memory fields into wider views.</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Aligned accesses</span></span><br><span class="line">mem bDM[<span class="number">1024</span>,<span class="number">1</span>]&lt;<span class="keyword">byte</span>,addr&gt;;</span><br><span class="line">mem wDM[<span class="number">1024</span>,<span class="number">2</span>]&lt;word,addr&gt; <span class="keyword">alias</span> bDM;</span><br><span class="line">mem lDM[<span class="number">1024</span>,<span class="number">4</span>]&lt;longword,addr&gt; <span class="keyword">alias</span> bDM;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Unaligned accesses</span></span><br><span class="line">mem bDM[<span class="number">1024</span>,<span class="number">1</span>]&lt;<span class="keyword">byte</span>,addr&gt;;</span><br><span class="line">mem wDMu[<span class="number">0</span>.<span class="variable">.1022</span>,<span class="number">1</span>]&lt;word,addr&gt; <span class="keyword">alias</span> bDM align <span class="number">1</span>;</span><br></pre></td></tr></table></figure></li></ul><table><thead><tr><th>Memory Inteface</th><th>Memory Align</th></tr></thead><tbody><tr><td><img src="/image/img_asip_design/mem_interface.png" alt="mem interface"></td><td><img src="/image/img_asip_design/mem_align.png" alt="mem align"></td></tr></tbody></table><h3 id="Transitories-and-Pipeline-Registers"><a href="#Transitories-and-Pipeline-Registers" class="headerlink" title="Transitories and Pipeline Registers"></a>Transitories and Pipeline Registers</h3><ul><li><p><strong>Transitories</strong> lose their value before the end of a clock cycle and must be read in the same stage they are written:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trn Xbus&lt;word&gt;;</span><br><span class="line">trn Ybus&lt;word&gt;;</span><br><span class="line">trn XYbus&lt;longword&gt; &#123; Xbus; Ybus; &#125;;</span><br></pre></td></tr></table></figure></li><li><p><strong>Pipeline Registers</strong> function like transitories but pass values to the next stage:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipe pA&lt;word&gt;;</span><br></pre></td></tr></table></figure></li></ul><h3 id="Enumeration-Types-and-Constants"><a href="#Enumeration-Types-and-Constants" class="headerlink" title="Enumeration Types and Constants"></a>Enumeration Types and Constants</h3><ul><li><p><strong>Enums</strong>:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> alu_op  &#123; ADD <span class="string">&quot;+&quot;</span>, SUB <span class="string">&quot;-&quot;</span>, AND <span class="string">&quot;&amp;&quot;</span>, OR <span class="string">&quot;|&quot;</span> &#125;;</span><br></pre></td></tr></table></figure></li><li><p><strong>Constants</strong>:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def size = <span class="number">2</span>**<span class="number">16</span>;</span><br><span class="line">mem DM[size]&lt;num,addr&gt;;</span><br><span class="line"></span><br><span class="line">cst c_byte&lt;<span class="keyword">byte</span>&gt;;</span><br><span class="line">cst c_3u  &lt;threebitsu&gt;;</span><br></pre></td></tr></table></figure></li></ul><h3 id="Processor-Properties"><a href="#Processor-Properties" class="headerlink" title="Processor Properties"></a>Processor Properties</h3><p>Properties identify elements with special processor-level meaning:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">properties &#123;</span><br><span class="line">  program_memory   : PM;</span><br><span class="line">  program_counter  : PC;</span><br><span class="line">  endianness       : little;</span><br><span class="line">  decode_stage     : ID;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">property</span> unconnected : REGX;</span><br></pre></td></tr></table></figure><hr><h2 id="Instruction-Set-Definition"><a href="#Instruction-Set-Definition" class="headerlink" title="Instruction Set Definition"></a>Instruction Set Definition</h2><h3 id="OR-Rules"><a href="#OR-Rules" class="headerlink" title="OR Rules"></a>OR Rules</h3><p>Define alternatives for instruction parts:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">opn tinycore2 (alu_opn | compare_opn | ... | generate_byte) &#123;</span><br><span class="line">  image: <span class="string">&quot;000000&quot;</span>::alu_opn</span><br><span class="line">       | <span class="string">&quot;000001&quot;</span>::compare_opn</span><br><span class="line">       | ...</span><br><span class="line">       | <span class="string">&quot;100&quot;</span>::generate_byte;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="AND-Rules"><a href="#AND-Rules" class="headerlink" title="AND Rules"></a>AND Rules</h3><p>Compose instruction fields orthogonally. Include:</p><ul><li><strong>Action</strong> (RT behavior)</li><li><strong>Syntax</strong> (assembly representation)</li><li><strong>Image</strong> (binary encoding)</li></ul><p><strong>Example</strong>:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">opn alu_opn (op: alu_op, a: eR, b: eR) &#123;</span><br><span class="line">  action &#123;</span><br><span class="line">    operandA = R[a];</span><br><span class="line">    operandB = R[b];</span><br><span class="line">    aluC    = (op == ADD) ? add(operandA, operandB)</span><br><span class="line">             : (op == SUB) ? sub(operandA, operandB)</span><br><span class="line">             : (op == AND) ? band(operandA, operandB)</span><br><span class="line">             : bor(operandA, operandB);</span><br><span class="line">    R[a]    = aluC;</span><br><span class="line">  &#125;</span><br><span class="line">  syntax: a <span class="string">&quot; = &quot;</span> a op b;</span><br><span class="line">  image : op::a::b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Parallel Composition</strong>:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">opn alu_load (op: alu_opn, ld: load_indirect) &#123;</span><br><span class="line">  action &#123; op; ld; &#125;</span><br><span class="line">  syntax: op <span class="string">&quot; || &quot;</span> ld;</span><br><span class="line">  image : op::ld;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><hr><h2 id="Pipelines-and-Hazards"><a href="#Pipelines-and-Hazards" class="headerlink" title="Pipelines and Hazards"></a>Pipelines and Hazards</h2><h3 id="Describing-Pipelines"><a href="#Describing-Pipelines" class="headerlink" title="Describing Pipelines"></a>Describing Pipelines</h3><ol><li>Declare pipeline registers (<code>pipe</code>).</li><li>Define stages and nested scopes.</li><li>Annotate storage accesses with stage information.</li></ol><p><strong>Example</strong> (multi-accumulate operation):</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> stages &#123; IF, ID, RD, E1, E2 &#125;;</span><br><span class="line"></span><br><span class="line">pipe mA&lt;word&gt;;</span><br><span class="line">pipe mB&lt;word&gt;;</span><br><span class="line"><span class="keyword">reg</span>  ACC&lt;longword&gt; read(ta) write(tb);</span><br><span class="line"></span><br><span class="line">opn macc_opn (op: mult_add_sub, a: eR, b: eR) &#123;</span><br><span class="line">  action &#123;</span><br><span class="line">    stage RD:</span><br><span class="line">      mA = R[a]; mB = R[b];</span><br><span class="line">    stage E1.<span class="variable">.E2</span>:</span><br><span class="line">      tb`E2` = (op == mult_add)</span><br><span class="line">              ? mult_add(ta`E2`, mA`E1`, mB`E1`)</span><br><span class="line">              : mult_sub(ta`E2`, mA`E1`, mB`E1`);</span><br><span class="line">    stage E2:</span><br><span class="line">      ACC = tb;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Pipeline-Hazards"><a href="#Pipeline-Hazards" class="headerlink" title="Pipeline Hazards"></a>Pipeline Hazards</h3><ul><li><p><strong>Control Hazards</strong>: Use <code>cycles(n)</code> or <code>delay_slots(n)</code> to model jump delays.</p></li><li><p><strong>Data &amp; Structural Hazards</strong>:</p><ul><li>Softwall stalls (insert NOPs)</li><li>Hardware stalls</li><li>Bypassing (forwarding)</li></ul></li></ul><p>For detailed hazard mitigation strategies, refer to the <a href="https://en.wikipedia.org/wiki/Hazard_%28computer_architecture%29">Hazard (computer architecture) Wiki</a>.</p><hr><h2 id="Mode-Rules"><a href="#Mode-Rules" class="headerlink" title="Mode Rules"></a>Mode Rules</h2><p>Define how storage locations are addressed.</p><h3 id="Register-Mode"><a href="#Register-Mode" class="headerlink" title="Register Mode"></a>Register Mode</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mode mR (e: eR) &#123;</span><br><span class="line">  value : R[e];</span><br><span class="line">  syntax: e;</span><br><span class="line">  image : e;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Memory-Mode"><a href="#Memory-Mode" class="headerlink" title="Memory Mode"></a>Memory Mode</h3><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mode ind_ld_st (a: eR) &#123;</span><br><span class="line">  value  : DM[dmAddr = R[a]];</span><br><span class="line">  action : R[a] = add(R[a], <span class="number">1</span>);</span><br><span class="line">  syntax : <span class="string">&quot;DM[&quot;</span> a <span class="string">&quot;+&quot;</span>]<span class="string">&quot;;</span></span><br><span class="line"><span class="string">  image  : a;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><hr><h2 id="Image-Attributes"><a href="#Image-Attributes" class="headerlink" title="Image Attributes"></a>Image Attributes</h2><p>Define binary encodings:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">opn arith_instr (alu_instr | as_sh_instr | shift_instr | macc_rnd_nop_instr) &#123;</span><br><span class="line">  image: <span class="string">&quot;00&quot;</span>::alu_instr</span><br><span class="line">       | <span class="string">&quot;01&quot;</span>::as_sh_instr</span><br><span class="line">       | <span class="string">&quot;10&quot;</span>::shift_instr</span><br><span class="line">       | <span class="string">&quot;11&quot;</span>::macc_rnd_nop_instr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Use <code>complete_image</code> when orthogonality of fields must be enforced.</p><h1 id="PAG-Modules"><a href="#PAG-Modules" class="headerlink" title="PAG Modules"></a>PAG Modules</h1><h2 id="Modeling-the-Processor-Control-Unit"><a href="#Modeling-the-Processor-Control-Unit" class="headerlink" title="Modeling the Processor Control Unit"></a>Modeling the Processor Control Unit</h2><p>The Processor Control Unit (PCU) orchestrates instruction fetch, issue, and program-counter updates, managing boot sequence states and supporting control-flow constructs such as jumps, calls, and returns. In our PDG-based implementation, all PCU logic resides in <code>&lt;processor&gt;_pcu.p</code>, which must include:</p><ul><li><p><strong>Storage declarations</strong> for PCU-specific registers and nML-managed state:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pcu_storages &#123;</span><br><span class="line">  <span class="keyword">reg</span> reg_booting&lt;uint2_t&gt;;  <span class="comment">// Tracks reset, boot, run states</span></span><br><span class="line">  trn pc_step&lt;addr&gt;;         <span class="comment">// Pipeline-step tracking (if needed)</span></span><br><span class="line">&#125;</span><br><span class="line">hw_init reg_booting = <span class="number">1</span>;     <span class="comment">// Initialize booting state</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Auxiliary helper functions</strong> for conditional checks or bit manipulations.</p></li><li><p>A <strong><code>user_issue()</code></strong> function that reads the fetched instruction from the PM output bus (<code>pmLdBus</code>) and invokes <code>issue_instr()</code> once the PCU exits boot.</p></li><li><p>A <strong><code>user_next_pc()</code></strong> function that computes the next PC based on control-flow signals, boot state, and sequential increments.</p></li></ul><h2 id="Fetching-and-Issuing-Instruction"><a href="#Fetching-and-Issuing-Instruction" class="headerlink" title="Fetching and Issuing Instruction"></a>Fetching and Issuing Instruction</h2><p>Instruction fetch is fully pipelined in two phases:</p><ol><li><p><strong>Prefetch (PF)</strong>: Drive the next PC address onto the PM address bus:</p><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pm_read`<span class="number">1</span>` = PM[pm_addr `<span class="number">0</span>` = nextpc] `<span class="number">0</span>`;</span><br></pre></td></tr></table></figure></li><li><p><strong>Fetch (IF)</strong>: Sample the instruction when <code>pmLdBus</code> is asserted:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pmtype f_instr = pm_read;</span><br></pre></td></tr></table></figure></li></ol><p>After a successful fetch, <code>user_issue()</code> sends <code>f_instr</code> into the downstream decode stage. On architectures with multi-cycle instructions, <code>issue_sig()</code> remains low for the remaining cycles of the previous instruction, preventing new issues until ready.</p><h2 id="Updating-the-PC"><a href="#Updating-the-PC" class="headerlink" title="Updating the PC"></a>Updating the PC</h2><h3 id="Straight-Line-Execution"><a href="#Straight-Line-Execution" class="headerlink" title="Straight-Line Execution"></a>Straight-Line Execution</h3><p>For ordinary sequential code, the PC simply increments by one. In nML and PDG, this appears as two coupled actions:</p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// tinycore2.n</span></span><br><span class="line"><span class="keyword">reg</span> PC&lt;word&gt; read (pc_r) write (pc_w);</span><br><span class="line"></span><br><span class="line"><span class="comment">// tinycore2_pcu.p</span></span><br><span class="line"><span class="keyword">void</span> tinycore2::user_next_pc() &#123;</span><br><span class="line">  pc_r = PC;</span><br><span class="line">  word nextpc;</span><br><span class="line">  <span class="keyword">if</span> (is_control_flow_instruction(f_instr))</span><br><span class="line">    nextpc = compute_branch_target();</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    nextpc = pc_r + <span class="number">1</span>;</span><br><span class="line">  PC = pc_w = nextpc;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Conditional-and-Unconditional-Jumps"><a href="#Conditional-and-Unconditional-Jumps" class="headerlink" title="Conditional and Unconditional Jumps"></a>Conditional and Unconditional Jumps</h3><p>Relative and absolute jumps are declared in the nML header:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">jumpi</span><span class="params">(sbyte)</span> <span class="title">property</span><span class="params">(relative jump)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">jumpci</span><span class="params">(<span class="type">bool</span>, sbyte)</span> <span class="title">property</span><span class="params">(relative jump)</span></span>;</span><br></pre></td></tr></table></figure><p>The PCU models them by checking <code>jump_pc_offs_sig</code> (relative) and <code>jump_pc_trgt_sig</code> (absolute), routing <code>pc_r + offset</code> or <code>pc_trgt</code> accordingly.</p><h3 id="Subroutine-Calls-and-Returns"><a href="#Subroutine-Calls-and-Returns" class="headerlink" title="Subroutine Calls and Returns"></a>Subroutine Calls and Returns</h3><p>Subroutine calls (<code>bsr</code>) and returns (<code>rst</code>) use a link register to save return addresses, respecting architectural delay slots:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">word <span class="title">bsr</span><span class="params">(word)</span> <span class="title">property</span><span class="params">(absolute call)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">rst</span><span class="params">(word)</span> <span class="title">property</span><span class="params">(ret)</span></span>;</span><br></pre></td></tr></table></figure><p>The model captures this in <code>user_next_pc()</code> by setting <code>lnk_pf = pc_r + 1</code> before control-flow resolution and reading it back on <code>rst</code>.</p><h3 id="Boot-Sequence"><a href="#Boot-Sequence" class="headerlink" title="Boot Sequence"></a>Boot Sequence</h3><p>The PCU tracks three states in <code>reg_booting</code>: 2 (reset), 1 (first boot cycle), and 0 (run). During reset, fetch is disabled; in the first boot cycle, only fetch is enabled; thereafter, normal issue&#x2F;fetch resumes.</p><p><img src="/image/img_asip_design/pcu_booting.png" alt="Boot State Diagram"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">pcu_storages &#123;</span><br><span class="line">  reg reg_booting&lt;<span class="type">uint2_t</span>&gt;;</span><br><span class="line">&#125;</span><br><span class="line">hw_init reg_booting = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">tinycore2::user_issue</span><span class="params">()</span> &#123;</span><br><span class="line">  pmtype f_instr = pmLdBus;</span><br><span class="line">  <span class="keyword">if</span> (reg_booting == <span class="number">0</span>)</span><br><span class="line">    issue_instr(pc_r = PC, <span class="number">1</span>, f_instr);</span><br><span class="line">  reg_booting &gt;&gt;= <span class="number">1</span>;  <span class="comment">// Shift through reset→boot→run</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">tinycore2::user_next_pc</span><span class="params">()</span> &#123;</span><br><span class="line">  lnk_pf = (pc_r = PC) + <span class="number">1</span>;</span><br><span class="line">  <span class="type">bool</span> allow_fetch = (reg_booting != <span class="number">2</span>);</span><br><span class="line">  word nextpc;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (reg_booting == <span class="number">1</span>)</span><br><span class="line">    allow_fetch = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (jump_pc_offs_sig)</span><br><span class="line">    nextpc = pc_r + pc_offs;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (jump_pc_trgt_sig)</span><br><span class="line">    nextpc = pc_trgt;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span> (loop_test())</span><br><span class="line">    nextpc = ls_r = LS;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    nextpc = pc_r + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  PC = pc_w = nextpc;</span><br><span class="line">  <span class="keyword">if</span> (allow_fetch)</span><br><span class="line">    pmLdBus`<span class="number">1</span>` = PM[pmAddr`<span class="number">0</span>` = nextpc]`<span class="number">1</span>`;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Manipulating-the-Pipeline"><a href="#Manipulating-the-Pipeline" class="headerlink" title="Manipulating the Pipeline"></a>Manipulating the Pipeline</h2><ul><li><p><strong>Hardware stalls</strong>: Temporarily pause issue and fetch via stall rules:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hw_stall <span class="number">1.</span>.<span class="number">2</span> cycles &#123;</span><br><span class="line">  stage E3: R[#] = ...;</span><br><span class="line">&#125; -&gt; &#123;</span><br><span class="line">  stage E1: ... = R[#];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong>Instruction kills</strong>: Abort an in-flight instruction on a condition:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (&lt;condition&gt;)</span><br><span class="line">  kill_instr(instr_id);</span><br></pre></td></tr></table></figure></li><li><p><strong>Pipeline stalls</strong>: Prevent issue of new instructions while a long-latency unit is busy:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (div_busy)</span><br><span class="line">  stall_instr(E1);</span><br></pre></td></tr></table></figure></li></ul><h2 id="Modeling-Multi-Cycle-Functional-Units"><a href="#Modeling-Multi-Cycle-Functional-Units" class="headerlink" title="Modeling Multi-Cycle Functional Units"></a>Modeling Multi-Cycle Functional Units</h2><p>Multi-Cycle Functional Units (MCFUs) implement operations spanning multiple clock cycles (e.g., division). Each MCFU is a standalone PDG module with:</p><ul><li><strong>Local state</strong>: operand registers, iteration counters (e.g., <code>Cnt&lt;uint5_t&gt;</code>), and busy signals.</li><li><strong>Primitive declaration</strong> in the header with <code>property(multicycle_N)</code>.</li><li><strong>nML action</strong> specifying read&#x2F;write ports and initial invocation.</li><li><strong>PDG <code>process()</code></strong> function that drives the iterative algorithm and updates <code>div_busy</code>.</li></ul><p><strong>Example: Non-Restoring Division</strong></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Primitive</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">divide</span><span class="params">(word a, word b, word&amp; q, word&amp; r)</span> <span class="title function_">property</span><span class="params">(multicycle_16)</span>;</span><br></pre></td></tr></table></figure><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// nML action</span><br><span class="line">opn divide (a : c_3, b : c_3) &#123;</span><br><span class="line">  action &#123; stage EX: divide(ta=R[a], tb=R[b], Q=qw, M=mw); &#125;</span><br><span class="line">  image : a, b, cycles(<span class="number">1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// PDG module</span></span><br><span class="line">multicycle_fu div &#123;</span><br><span class="line">  reg B&lt;uint16&gt;;</span><br><span class="line">  reg Cnt&lt;uint5&gt;;</span><br><span class="line">  <span class="type">void</span> <span class="title function_">process</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (vd_divide_ta_tb_qw_mw_EX_sig) &#123;</span><br><span class="line">      Q = qw = ta;</span><br><span class="line">      M = mw = tb;</span><br><span class="line">      Cnt = <span class="number">16</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (Cnt &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">auto</span> [mq_new] = divide_step((mr=M, qr=Q), B);</span><br><span class="line">      M = mw = mq_new[<span class="number">31</span>:<span class="number">16</span>];</span><br><span class="line">      Q = qw = mq_new[<span class="number">15</span>:<span class="number">0</span>];</span><br><span class="line">      Cnt -= <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    div_busy = (Cnt != <span class="number">0</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Modeling-IO-Interfaces"><a href="#Modeling-IO-Interfaces" class="headerlink" title="Modeling IO Interfaces"></a>Modeling IO Interfaces</h2><p>IO interfaces bridge abstract nML memory definitions to hardware-mapped memories. Each interface is a PDG module named <code>io_interface &lt;unit&gt; (MemorySpec)</code>, containing:</p><ol><li><strong>External interface</strong> <code>eMem</code> with inports and outports for <code>ld</code>&#x2F;<code>st</code> operations.</li><li><strong>Local storage</strong> registers (e.g., <code>st_ff</code>, <code>addr_ff</code>, <code>data_ff</code>) to buffer and delay signals.</li><li><strong><code>process_request()</code></strong> and <strong><code>process_result()</code></strong> routines to deconflict structural hazards and implement bypass logic.</li><li><strong><code>dbg_access_*</code></strong> helpers for simulator debugging and memory inspection.</li></ol><p><strong>Example: Load&#x2F;Store Bypass</strong></p><figure class="highlight vhdl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">io_interface bypass_dm (DM) &#123;</span><br><span class="line">  mem eDM[<span class="number">2</span>**<span class="number">16</span>]&lt;word,addr&gt; <span class="keyword">access</span> &#123;</span><br><span class="line">    e_ld: e_db`<span class="number">1</span>` = eDM[e_ab];</span><br><span class="line">    e_st: eDM[e_ab] = e_wb;</span><br><span class="line">  &#125;;</span><br><span class="line">  reg st_ff&lt;uint1&gt;;</span><br><span class="line">  reg addr_ff&lt;addr&gt;;</span><br><span class="line">  reg data_ff&lt;word&gt;;</span><br><span class="line">  reg valid_ff&lt;uint1&gt;;</span><br><span class="line">  reg match_ff&lt;uint1&gt;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">process</span> process_result() &#123;</span><br><span class="line">    db = match_ff ? data_ff : e_db;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">process</span> process_request() &#123;</span><br><span class="line">    // <span class="keyword">Default</span> signals</span><br><span class="line">    e_ld = e_st = <span class="number">0</span>;</span><br><span class="line">    e_ab = ab; e_wb = wb;</span><br><span class="line"></span><br><span class="line">    // Load path</span><br><span class="line">    <span class="keyword">if</span> (ld_dm) &#123;</span><br><span class="line">      bool hit = (ab == addr_ff) &amp;&amp; (valid_ff || st_ff);</span><br><span class="line">      match_ff = hit;</span><br><span class="line">      e_ld = !hit;</span><br><span class="line">      <span class="keyword">if</span> (st_ff) &#123; data_ff = wb; valid_ff = <span class="number">1</span>; &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      // Drain <span class="keyword">buffer</span> <span class="keyword">or</span> pass-through store</span><br><span class="line">      <span class="keyword">if</span> (valid_ff) &#123;</span><br><span class="line">        e_st = <span class="number">1</span>; e_ab = addr_ff; e_wb = data_ff; valid_ff = <span class="number">0</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (st_ff) &#123;</span><br><span class="line">        e_st = <span class="number">1</span>; // normal store</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Update delay registers</span><br><span class="line">    <span class="keyword">if</span> (st_dm) addr_ff = ab;</span><br><span class="line">    st_ff = st_dm;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  void dbg_access_DM(<span class="built_in">unsigned</span> a, word&amp; v, bool read) &#123;</span><br><span class="line">    <span class="keyword">if</span> (valid_ff &amp;&amp; addr_ff == a)</span><br><span class="line">      v = data_ff;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">      dbg_access_eDM(a, v, read);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Summary-MCFUs-vs-IO-Interfaces"><a href="#Summary-MCFUs-vs-IO-Interfaces" class="headerlink" title="Summary: MCFUs vs. IO Interfaces"></a>Summary: MCFUs vs. IO Interfaces</h2><table><thead><tr><th></th><th><strong>MCFU</strong></th><th><strong>IO Interface</strong></th></tr></thead><tbody><tr><td><strong>Purpose</strong></td><td>Execute compiler primitives over multiple cycles</td><td>Bridge nML memory spec to physical memory</td></tr><tr><td><strong>State</strong></td><td>Local registers, counters, busy flags</td><td>Buffers for data, addresses, valid&#x2F;match flags</td></tr><tr><td><strong>Control activation</strong></td><td>Driven by primitive invocation signals</td><td>Driven by load&#x2F;store handshakes</td></tr><tr><td><strong>Compiler view</strong></td><td>Abstract primitive call with fixed latency (<code>multicycle_N</code>)</td><td>Abstract memory interface with timing and port constraints</td></tr><tr><td><strong>Stall integration</strong></td><td>Implicit stall until <code>busy</code> is cleared</td><td>Explicit <code>in_wait_cycle()</code> checks and hazard bypass logic</td></tr></tbody></table><h1 id="The-Compiler-Processor-Header-File"><a href="#The-Compiler-Processor-Header-File" class="headerlink" title="The Compiler Processor Header File"></a>The Compiler Processor Header File</h1><h2 id="Part-1"><a href="#Part-1" class="headerlink" title="Part 1"></a>Part 1</h2><h3 id="Overview-of-the-Chess-Compiler"><a href="#Overview-of-the-Chess-Compiler" class="headerlink" title="Overview of the Chess Compiler"></a>Overview of the Chess Compiler</h3><p><img src="/image/img_asip_design/chess_compile_overview.png" alt="Chess compiler overview"></p><p>The Chess compiler front end supports two pipelines—Chess and LLVM—and provides language bindings for C, C++, and OpenCL. Both pipelines translate the source code into a unified intermediate representation: a Control-Data Flow Graph (CDFG). This graph captures both control structures (branches, loops) and data dependencies, enabling advanced optimizations.</p><h4 id="Front-End"><a href="#Front-End" class="headerlink" title="Front End"></a>Front End</h4><ol><li><strong>Static Single Assignment (SSA):</strong> All variables are converted into SSA form to simplify data-flow analysis and enable precise value tracking.</li><li><strong>Induction-Variable Analysis:</strong> Detects loop-carried dependencies, transforms loop counters into affine functions, and optimizes pointer arithmetic within loops.</li><li><strong>Expression Flattening &amp; Chain Building:</strong> Breaks down complex pointer and arithmetic expressions into sequences of primitive operations, facilitating pattern matching and resource scheduling.</li></ol><h4 id="Back-End"><a href="#Back-End" class="headerlink" title="Back End"></a>Back End</h4><ol><li><strong>Pattern-Based Code Selection:</strong> Bundles subgraphs of the CDFG into hardware-specific patterns (Instruction Selection Graphs, ISGs) and maps them to the target processor’s instruction set.</li><li><strong>Register Allocation:</strong> Assigns live values to physical registers, spilling to memory only when necessary. Utilizes graph-coloring and priority-based heuristics to minimize spills.</li><li><strong>Scheduling:</strong> Orders instructions to respect data dependencies and maximize parallelism. Supports both list scheduling and modulo scheduling for loops, targeting low-latency and high-throughput architectures.</li></ol><hr><h2 id="Part-2"><a href="#Part-2" class="headerlink" title="Part 2"></a>Part 2</h2><h3 id="Mapping-of-Types"><a href="#Mapping-of-Types" class="headerlink" title="Mapping of Types"></a>Mapping of Types</h3><p>The header file <code>&lt;processor&gt;_chess.h</code> defines how C&#x2F;C++ types and operators map to the processor’s primitive types and functions. It also introduces Chess-specific directives (chess_properties) to guide resource usage.</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Single-type mapping</span></span><br><span class="line">chess_properties &#123;</span><br><span class="line">  representation <span class="type">int</span>   : word;</span><br><span class="line">  representation <span class="type">void</span>* : addr;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Multi-type mapping</span></span><br><span class="line">chess_properties &#123;</span><br><span class="line">  representation <span class="type">int</span>, <span class="type">frac_t</span> : word;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>To customize storage for composite types, you can define a struct with the <code>property(keep_in_registers)</code> attribute:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> long_r <span class="title function_">property</span><span class="params">(keep_in_registers)</span> &#123;</span><br><span class="line">  <span class="type">unsigned</span> lo;</span><br><span class="line">  <span class="type">unsigned</span> hi;</span><br><span class="line">  long_r(<span class="type">unsigned</span> l, <span class="type">unsigned</span> h)</span><br><span class="line">    : lo(l), hi(h) &#123;&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Bind the long C type to the long_r struct</span></span><br><span class="line">chess_properties &#123;</span><br><span class="line">  representation <span class="type">long</span> : long_r;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Type Promotion:</strong> The compiler can allocate a narrower C type (e.g., <code>short</code>) into a wider hardware type (e.g., <code>word</code>) to reduce register pressure or align with the target datapath.</p><h3 id="Definition-of-Operators"><a href="#Definition-of-Operators" class="headerlink" title="Definition of Operators"></a>Definition of Operators</h3><p>Chess allows mapping C built-in operators to either primitive hardware functions or inline helper functions, enabling tight control over status registers and side effects.</p><ol><li><p><strong>Promotion to Primitive Functions</strong><br>Define a C++ operator by forwarding to a low-level primitive:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// In &lt;processor&gt;.h</span></span><br><span class="line">namespace mycore_primitive &#123;</span><br><span class="line">  word <span class="title function_">sub</span><span class="params">(word a, word b, stat &amp;s)</span>; <span class="comment">// subtract with status update</span></span><br><span class="line">  <span class="type">bool</span> <span class="title function_">lt</span><span class="params">(stat s)</span>;                   <span class="comment">// test status for less-than</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// In &lt;processor&gt;_chess.h</span></span><br><span class="line">namespace mycore_primitive &#123;</span><br><span class="line">  <span class="keyword">inline</span> <span class="type">bool</span> <span class="title function_">less_than</span><span class="params">(word a, word b)</span> &#123;</span><br><span class="line">    stat s;</span><br><span class="line">    sub(a, b, s);</span><br><span class="line">    <span class="keyword">return</span> lt(s);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Map the C operator&lt; on ints to the primitive helper</span></span><br><span class="line">promotion <span class="type">bool</span> operator&lt;(<span class="type">int</span>, <span class="type">int</span>) = mycore_primitive::less_than(word, word);</span><br></pre></td></tr></table></figure></li><li><p><strong>Promotion to Inline Functions</strong><br>For more complex or composite operations, map operators to inline C++ functions:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">inline</span> <span class="type">frac_t</span> operator+(<span class="type">frac_t</span> a, <span class="type">frac_t</span> b) &#123;</span><br><span class="line">  <span class="comment">// perform fractional addition using hardware-supported primitives</span></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">promotion <span class="type">frac_t</span> operator+(<span class="type">frac_t</span>, <span class="type">frac_t</span>) = inline_add_frac(<span class="type">frac_t</span>, <span class="type">frac_t</span>);</span><br></pre></td></tr></table></figure></li><li><p><strong>Pointer Arithmetic</strong><br>Chess directly supports pointer increments and differences by mapping <code>ptr + n</code> and <code>ptr1 - ptr2</code> onto address-unit primitives, preserving bounds information when available.</p></li><li><p><strong>Application-Specific Operators</strong><br>Extend the language with custom types—vectors, complex numbers, or fixed-point fractions—and define their arithmetic:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Complex multiplication</span></span><br><span class="line"><span class="keyword">inline</span> <span class="type">complex_t</span> operator*(<span class="type">complex_t</span> x, <span class="type">complex_t</span> y) &#123;</span><br><span class="line">  <span class="keyword">return</span> complex_mul_prim(x.real, x.imag, y.real, y.imag);</span><br><span class="line">&#125;</span><br><span class="line">promotion <span class="type">complex_t</span> operator*(<span class="type">complex_t</span>, <span class="type">complex_t</span>) = complex_mul_prim;</span><br></pre></td></tr></table></figure></li></ol><h3 id="Intrinsic-Functions"><a href="#Intrinsic-Functions" class="headerlink" title="Intrinsic Functions"></a>Intrinsic Functions</h3><p>Intrinsic functions bypass C++ overload resolution and map directly onto one or more nML primitives. This mechanism supports heterogeneous operations—such as multiplying two 32-bit integers to produce a 64-bit result—and leverages specialized DSP or SIMD units:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Multiply two 32-bit ints into a 64-bit long</span></span><br><span class="line">intrinsic <span class="type">long</span> <span class="title function_">mul_int32_to_int64</span><span class="params">(<span class="type">int</span> a, <span class="type">int</span> b)</span></span><br><span class="line">  = nml_mul_32x32_to_64(a, b);</span><br></pre></td></tr></table></figure><p>These mappings ensure that high-level code retains the performance characteristics of the target architecture while maintaining readability and type safety.</p><hr><h2 id="Part-3"><a href="#Part-3" class="headerlink" title="Part 3"></a>Part 3</h2><h3 id="Support-of-Subroutines"><a href="#Support-of-Subroutines" class="headerlink" title="Support of Subroutines"></a>Support of Subroutines</h3><p>A subroutine in ASIP design corresponds to a C function at the hardware level. It is implemented using dedicated control flow instructions—<code>bsr</code> for subroutine call and <code>rts</code> for return. These instructions manage control flow, preserve register context, and handle memory interface operations. The compiler guarantees consistency through enforced calling conventions, enabling seamless software-hardware integration.</p><p>Defined in the primitive header file:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// call</span></span><br><span class="line">word <span class="title function_">bsr</span><span class="params">(word)</span> <span class="title function_">property</span><span class="params">(absolute call)</span>;</span><br><span class="line"><span class="comment">// return</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">rts</span><span class="params">(word)</span> <span class="title function_">property</span><span class="params">(ret)</span>;</span><br></pre></td></tr></table></figure><p>The <code>bsr</code> instruction saves the return address (often in a link register), while <code>rts</code> retrieves it to continue execution. The <code>link_register</code> is specified in the compiler header:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chess_properties &#123; link_register: LR; &#125;</span><br></pre></td></tr></table></figure><hr><h4 id="Data-Memory"><a href="#Data-Memory" class="headerlink" title="Data Memory"></a>Data Memory</h4><p>Memory regions are organized to support different storage classes:</p><ul><li><strong>DM</strong>: Default memory for general data.</li><li><strong>DMs</strong>: Static memory for persistent&#x2F;static variables.</li><li><strong>DMl</strong>: Local memory for stack frames and automatic (local) variables.</li></ul><p>This categorization helps in optimizing allocation and access according to variable scope and lifetime.</p><h4 id="Software-Stack"><a href="#Software-Stack" class="headerlink" title="Software Stack"></a>Software Stack</h4><p>Function-related stack manipulation is supported in ASIP design. A dedicated memory and register pair manage stack behavior, including direction (up&#x2F;down) and SP-relative addressing.</p><p>Stack configuration example:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">chess_properties &#123;</span><br><span class="line">   default_memories : DM, VDM;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// First software stack:</span></span><br><span class="line">   local_memory : DM; </span><br><span class="line">   stack_pointer : SP;</span><br><span class="line"></span><br><span class="line">chess_stack:</span><br><span class="line">   <span class="comment">// Second software stack:</span></span><br><span class="line">   local_memory : VDM; </span><br><span class="line">   stack_pointer : VSP;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This allows support for multiple stacks, such as a scalar and vector stack. Each stack frame includes:</p><ul><li><strong>Locals area</strong></li><li><strong>Spill area</strong> (for register spilling and context save)</li><li><strong>Argument build area</strong></li></ul><p>These areas are sized per function and accessed using SP-relative addressing.</p><h4 id="Context-Saving"><a href="#Context-Saving" class="headerlink" title="Context Saving"></a>Context Saving</h4><p>Function calls may overwrite registers. To prevent data loss, ASIP supports two context-saving strategies:</p><table><thead><tr><th>Caller saved</th><th>Callee saved</th></tr></thead><tbody><tr><td>Prior to call, caller saves all regs in use in caller, to its spill area</td><td>After call, callee saves specified regs, if used by callee, to its spill area</td></tr><tr><td><img src="/image/img_asip_design/context_save_caller.png" alt="caller"></td><td><img src="/image/img_asip_design/context_save_callee.png" alt="callee"></td></tr></tbody></table><ul><li><p><strong>Caller-saved</strong>: Default for argument registers and enables inter-procedural optimizations.</p></li><li><p><strong>Callee-saved</strong>: Common in interrupt service routines, can be explicitly enabled via:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chess_properties &#123; callee_saved : R; &#125;</span><br></pre></td></tr></table></figure></li></ul><p>You can also define persistent values across calls:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value_across_call R4 : <span class="number">10</span>;</span><br></pre></td></tr></table></figure><h4 id="Multi-Treading-Support"><a href="#Multi-Treading-Support" class="headerlink" title="Multi-Treading Support"></a>Multi-Treading Support</h4><p>ASIP offers both software and hardware mechanisms for thread-level context switching.</p><ul><li><strong>Software-based context save</strong>: The interrupt service routine (ISR) saves all used registers, with the cost proportional to the register count.</li><li><strong>Hardware-based support</strong>: Through <em>shadow registers</em>, which provide rapid context switching with minimal latency.</li></ul><p><img src="/image/img_asip_design/multi_sp.png" alt="Multi-Theading Support"></p><p>Advanced hardware schemes include:</p><ol><li><strong>Context shift ring (<code>context_shift_trn</code>)</strong></li><li><strong>Central register selector (<code>context_select_reg</code>)</strong></li><li><strong>Interleaved context issue (<code>context_issue_trn</code>)</strong></li></ol><p>This enables efficient multitasking with low overhead and supports deterministic behavior.</p><hr><h2 id="Part-4"><a href="#Part-4" class="headerlink" title="Part 4"></a>Part 4</h2><h3 id="Miscellaneous-topics"><a href="#Miscellaneous-topics" class="headerlink" title="Miscellaneous topics"></a>Miscellaneous topics</h3><p>To be updated.</p><p>Currently, the compiler supports a powerful mechanism for <em>property verification</em> through <strong>one-liners</strong>—compact function definitions used to verify compiler behavior.</p><p>One-liner tests cover:</p><ul><li>Constant generation</li><li>Stack manipulation and spill behavior</li><li>Register moves and argument passing</li><li>Operator correctness</li><li>Instruction-level parallelism</li><li>Multi-threading and context switching</li></ul><p>They are automatically generated and stored in:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lib/oneliners/oneliners.prx</span><br></pre></td></tr></table></figure><p>These form a lightweight, systematic test suite for validating compiler support against the C language features mapped to the ASIP model.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASIP design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Overview of ASIP design tool</title>
      <link href="/2025/07/02/asip_design_tool/Introduction_to_asip_design/"/>
      <url>/2025/07/02/asip_design_tool/Introduction_to_asip_design/</url>
      
        <content type="html"><![CDATA[<h1 id="Overview-of-ASIP-Designer"><a href="#Overview-of-ASIP-Designer" class="headerlink" title="Overview of ASIP Designer"></a>Overview of ASIP Designer</h1><h2 id="What-Is-ASIP-Designer"><a href="#What-Is-ASIP-Designer" class="headerlink" title="What Is ASIP Designer"></a>What Is ASIP Designer</h2><p><strong>ASIP Designer</strong> is Synopsys’s tool for developing Application-Specific Instruction-set Processors (ASIPs). It lets you:</p><ul><li>Define custom processor architectures using <strong>nML</strong>, a high-level description language for ISA, registers, and memory</li><li>Generate software toolchains tailored to your ISA</li><li>Simulate performance and refine your design</li></ul><h2 id="Example-Project-tctcore"><a href="#Example-Project-tctcore" class="headerlink" title="Example Project: tctcore"></a>Example Project: <code>tctcore</code></h2><p>Before starting, set up the environment and load the required modules:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> asip_design/</span><br><span class="line"><span class="built_in">source</span> chess_env_LNa64.sh</span><br><span class="line">module load base</span><br><span class="line">module load asip_designer</span><br><span class="line">module load vcs</span><br></pre></td></tr></table></figure><p>In this example, we use the educational <code>tctcore</code> project to demonstrate the workflow.</p><h3 id="Project-Structure"><a href="#Project-Structure" class="headerlink" title="Project Structure"></a>Project Structure</h3><p><img src="/image/img_asip_design/tctcore_project_structure.png" alt="tctcore Project Structure"></p><h3 id="Generating-the-Simulation-Model"><a href="#Generating-the-Simulation-Model" class="headerlink" title="Generating the Simulation Model"></a>Generating the Simulation Model</h3><p>Run the model generation script:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">make_model</span><br></pre></td></tr></table></figure><p>Then compile the C&#x2F;C++ algorithm (e.g., <code>irrdirect</code>) and simulate it:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;tctcore_chess.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Low-pass filter parameters:</span></span><br><span class="line"><span class="comment">//   Sample frequency (Hz)              : 44 000</span></span><br><span class="line"><span class="comment">//   Cut-off frequency (Hz)            : 5 000</span></span><br><span class="line"><span class="comment">//   Damping factor                     : 1.5</span></span><br><span class="line"><span class="comment">//   Data width (signed)                : 16 bits</span></span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">int</span> a = <span class="number">1341</span>, b = <span class="number">5591</span>, g = <span class="number">16607</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> C[<span class="number">5</span>] = &#123; a, <span class="number">2</span>*a, a, g, -b &#125;;</span><br><span class="line"><span class="type">int</span> xd[<span class="number">2</span>] = &#123;<span class="number">0</span>&#125;, yd[<span class="number">2</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">low_pass</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">    <span class="type">long</span> acc = fmult(x, C[<span class="number">0</span>])</span><br><span class="line">             + fmult(xd[<span class="number">0</span>], C[<span class="number">1</span>])</span><br><span class="line">             + fmult(xd[<span class="number">1</span>], C[<span class="number">2</span>])</span><br><span class="line">             + fmult(yd[<span class="number">0</span>], C[<span class="number">3</span>])</span><br><span class="line">             + fmult(yd[<span class="number">1</span>], C[<span class="number">4</span>]);</span><br><span class="line">    <span class="type">int</span> y = <span class="number">2</span> * (<span class="type">int</span>)round(acc);</span><br><span class="line">    xd[<span class="number">1</span>] = xd[<span class="number">0</span>]; xd[<span class="number">0</span>] = x;</span><br><span class="line">    yd[<span class="number">1</span>] = yd[<span class="number">0</span>]; yd[<span class="number">0</span>] = y;</span><br><span class="line">    <span class="keyword">return</span> y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">low_pass_v2</span><span class="params">(<span class="type">int</span> x)</span> &#123;</span><br><span class="line">    <span class="type">long</span> acc = fmult(x, C[<span class="number">0</span>]);</span><br><span class="line">    acc = fmacc(xd[<span class="number">0</span>], C[<span class="number">1</span>], acc);</span><br><span class="line">    acc = fmacc(xd[<span class="number">1</span>], C[<span class="number">2</span>], acc);</span><br><span class="line">    acc = fmacc(yd[<span class="number">0</span>], C[<span class="number">3</span>], acc);</span><br><span class="line">    acc = fmacc(yd[<span class="number">1</span>], C[<span class="number">4</span>], acc);</span><br><span class="line">    <span class="type">int</span> y = <span class="number">2</span> * (<span class="type">int</span>)round(acc);</span><br><span class="line">    xd[<span class="number">1</span>] = xd[<span class="number">0</span>]; xd[<span class="number">0</span>] = x;</span><br><span class="line">    yd[<span class="number">1</span>] = yd[<span class="number">0</span>]; yd[<span class="number">0</span>] = y;</span><br><span class="line">    <span class="keyword">return</span> y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">volatile</span> <span class="type">int</span> <span class="title function_">chess_storage</span><span class="params">(DM:<span class="number">256</span>)</span> input_port;</span><br><span class="line"><span class="keyword">volatile</span> <span class="type">int</span> <span class="title function_">chess_storage</span><span class="params">(DM:<span class="number">257</span>)</span> output_port;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">64</span>; i++)</span><br><span class="line">        output_port = low_pass(input_port);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">64</span>; i++)</span><br><span class="line">        output_port = low_pass_v2(input_port);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The code above illustrates two implementations:</p><ol><li><strong><code>low_pass</code></strong> uses basic multiply and add operations.</li><li><strong><code>low_pass_v2</code></strong> leverages the intrinsic <code>fmacc</code> for fused multiply-accumulate.</li></ol><p>Further details on nML structure will follow in subsequent sessions.</p><h2 id="HDL-ISS-and-Libraries"><a href="#HDL-ISS-and-Libraries" class="headerlink" title="HDL, ISS, and Libraries"></a>HDL, ISS, and Libraries</h2><ul><li><strong>HDL</strong>: Generates synthesizable Verilog&#x2F;VHDL from the design.</li><li><strong>ISS</strong>: Provides an instruction-set simulator for software validation.</li><li><strong>Libraries</strong>: Include standard and custom components (e.g., arithmetic units, register files).</li></ul><h2 id="Executing-a-tctcore-Task"><a href="#Executing-a-tctcore-Task" class="headerlink" title="Executing a tctcore Task"></a>Executing a <code>tctcore</code> Task</h2><p>To run prx files and generate infrastructure code, use <code>chessmk</code> and a Makefile. For example:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chessmk lib/tctcore.prx -r</span><br><span class="line">chessmk lib/tctcore.prx</span><br><span class="line">chessmk iss/caiss.prx</span><br><span class="line">chessmk hdl/tctcore_vlog.prx</span><br></pre></td></tr></table></figure><p>This process creates directories for debugging, type&#x2F;register visualization, and centralized maintenance.</p><h3 id="Running-C-C-Simulation"><a href="#Running-C-C-Simulation" class="headerlink" title="Running C&#x2F;C++ Simulation"></a>Running C&#x2F;C++ Simulation</h3><p>Create a dedicated Makefile (e.g., <code>make_tctcore</code>) then:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compile algorithm</span></span><br><span class="line">chesscc -c iirdirect/iirdirect.c -P lib/tctcore.prx</span><br><span class="line"><span class="comment"># Link to generate executable</span></span><br><span class="line">chesscc -o hdl/tctcore_vlog_go/iirdirectx iirdirect.o -P lib/tctcore.prx</span><br><span class="line"></span><br><span class="line"><span class="comment"># Elaborate and simulate</span></span><br><span class="line"><span class="built_in">cd</span> hdl/tctcore_vlog_go</span><br><span class="line">make elab</span><br><span class="line">make <span class="built_in">test</span> TEST=iirdirectx</span><br><span class="line">make sim</span><br><span class="line"><span class="built_in">cd</span> ../..</span><br></pre></td></tr></table></figure><p>At this point, the basic simulation and verification are complete. Continuous integration into the CI&#x2F;CD pipeline can be configured as needed.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Architecture </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASIP design </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Biography</title>
      <link href="/2025/02/11/aboutMe/"/>
      <url>/2025/02/11/aboutMe/</url>
      
        <content type="html"><![CDATA[<h1 id="Biography"><a href="#Biography" class="headerlink" title="Biography"></a>Biography</h1><p>I am currently a junior at the School of Electronic Engineering and Computer Science at Peking University, where my research interests lie at the intersection of hardware architecture design and machine learning, with a particular focus on hardware-software co-design. This field excites me because it bridges the gap between efficient hardware systems and intelligent algorithms, offering transformative potential for modern computing.</p><p>Building on a strong foundation in hardware design from my undergraduate studies, along with research experience in AI chip development, I am eager to deepen my expertise and contribute to groundbreaking innovations in the field. I plan to continue my graduate studies in the fall of 2026, and I am currently seeking a fall research internship to further develop my skills and knowledge.</p><p>Feel free to reach out to me anytime. You can find my CV <a href="https://worldline22.github.io/pdf/CV.pdf">here</a>.</p>]]></content>
      
      
      <categories>
          
          <category> Introduction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>写在前面（Preface）</title>
      <link href="/2024/07/29/preinfo/"/>
      <url>/2024/07/29/preinfo/</url>
      
        <content type="html"><![CDATA[<ol><li>这是我的个人博客，本网站将不定期更新自己科研、生活上的点点滴滴，希望你能喜欢。</li><li>网页分为“黑&#x2F;白”两种模式可供选择，右下角设置键可以修改模式。</li><li>本网页目前评论功能只支持拥有github账号的访客使用，评论时请使用自己的github账号登录，注意评论文明，感谢你的浏览(❤ ω ❤)。</li></ol><hr><ol><li><p>This is my personal blog. It will be updated from time to time with bits and pieces of my research and daily life. I hope you enjoy it.</p></li><li><p>The website offers both dark and light modes, which you can switch using the settings button at the bottom right corner.</p></li><li><p>Currently, the comment feature only supports visitors with a GitHub account. Please log in with your GitHub account to comment. Be respectful—thank you for visiting! (❤ ω ❤)</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Introduction </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tips </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
